{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer#Your code here; import some packages/modules you plan to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv(\"Bank_complaints.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)\n",
    "\n",
    "#Alternative custom script:\n",
    "# random.seed(123)\n",
    "# test_index = random.sample(range(1,10000), 1500)\n",
    "# test = one_hot_results[test_index]\n",
    "# train = np.delete(one_hot_results, test_index, 0)\n",
    "# label_test = product_onehot[test_index]\n",
    "# label_train = np.delete(product_onehot, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.9527 - acc: 0.1555 - val_loss: 1.9378 - val_acc: 0.1720\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.9222 - acc: 0.1979 - val_loss: 1.9178 - val_acc: 0.2130\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.9005 - acc: 0.2281 - val_loss: 1.9004 - val_acc: 0.2340\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8789 - acc: 0.2540 - val_loss: 1.8817 - val_acc: 0.2520\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.8547 - acc: 0.2749 - val_loss: 1.8597 - val_acc: 0.2690\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.8267 - acc: 0.3001 - val_loss: 1.8330 - val_acc: 0.2900\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7937 - acc: 0.3257 - val_loss: 1.8007 - val_acc: 0.3140\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7553 - acc: 0.3523 - val_loss: 1.7626 - val_acc: 0.3360\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7117 - acc: 0.3753 - val_loss: 1.7197 - val_acc: 0.3570\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6647 - acc: 0.3992 - val_loss: 1.6735 - val_acc: 0.3840\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6159 - acc: 0.4216 - val_loss: 1.6256 - val_acc: 0.4080\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5663 - acc: 0.4483 - val_loss: 1.5773 - val_acc: 0.4310\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5169 - acc: 0.4772 - val_loss: 1.5291 - val_acc: 0.4520\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4676 - acc: 0.4988 - val_loss: 1.4813 - val_acc: 0.4810\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.4192 - acc: 0.5231 - val_loss: 1.4338 - val_acc: 0.5160\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3724 - acc: 0.5469 - val_loss: 1.3892 - val_acc: 0.5290\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3270 - acc: 0.5663 - val_loss: 1.3447 - val_acc: 0.5530\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2831 - acc: 0.5865 - val_loss: 1.3025 - val_acc: 0.5700\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.2413 - acc: 0.6009 - val_loss: 1.2632 - val_acc: 0.5870\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2018 - acc: 0.6180 - val_loss: 1.2259 - val_acc: 0.5950\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1637 - acc: 0.6316 - val_loss: 1.1903 - val_acc: 0.6040\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1275 - acc: 0.6461 - val_loss: 1.1560 - val_acc: 0.6040\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0936 - acc: 0.6551 - val_loss: 1.1236 - val_acc: 0.6160\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0608 - acc: 0.6641 - val_loss: 1.0937 - val_acc: 0.6210\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.0304 - acc: 0.6705 - val_loss: 1.0656 - val_acc: 0.6290\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.0011 - acc: 0.6813 - val_loss: 1.0401 - val_acc: 0.6370\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.9739 - acc: 0.6893 - val_loss: 1.0156 - val_acc: 0.6460\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9486 - acc: 0.6976 - val_loss: 0.9910 - val_acc: 0.6480\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9242 - acc: 0.7027 - val_loss: 0.9696 - val_acc: 0.6590\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9010 - acc: 0.7080 - val_loss: 0.9491 - val_acc: 0.6670\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8795 - acc: 0.7151 - val_loss: 0.9299 - val_acc: 0.6690\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8596 - acc: 0.7207 - val_loss: 0.9119 - val_acc: 0.6720\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8407 - acc: 0.7256 - val_loss: 0.8950 - val_acc: 0.6780\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8224 - acc: 0.7309 - val_loss: 0.8793 - val_acc: 0.6820\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8058 - acc: 0.7341 - val_loss: 0.8639 - val_acc: 0.6830\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7893 - acc: 0.7417 - val_loss: 0.8496 - val_acc: 0.6930\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7748 - acc: 0.7436 - val_loss: 0.8376 - val_acc: 0.6950\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7606 - acc: 0.7443 - val_loss: 0.8239 - val_acc: 0.7020\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7471 - acc: 0.7503 - val_loss: 0.8163 - val_acc: 0.6980\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7344 - acc: 0.7515 - val_loss: 0.8023 - val_acc: 0.7000\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7223 - acc: 0.7577 - val_loss: 0.7921 - val_acc: 0.7090\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.7110 - acc: 0.7608 - val_loss: 0.7827 - val_acc: 0.7070\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.7002 - acc: 0.7628 - val_loss: 0.7777 - val_acc: 0.7110\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6907 - acc: 0.7684 - val_loss: 0.7652 - val_acc: 0.7100\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6808 - acc: 0.7691 - val_loss: 0.7571 - val_acc: 0.7130\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6712 - acc: 0.7721 - val_loss: 0.7522 - val_acc: 0.7170\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6625 - acc: 0.7761 - val_loss: 0.7437 - val_acc: 0.7150\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6536 - acc: 0.7785 - val_loss: 0.7366 - val_acc: 0.7190\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6456 - acc: 0.7796 - val_loss: 0.7323 - val_acc: 0.7260\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6379 - acc: 0.7812 - val_loss: 0.7238 - val_acc: 0.7260\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.6306 - acc: 0.7844 - val_loss: 0.7181 - val_acc: 0.7250\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.6235 - acc: 0.7859 - val_loss: 0.7143 - val_acc: 0.7300\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6163 - acc: 0.7876 - val_loss: 0.7098 - val_acc: 0.7290\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6098 - acc: 0.7896 - val_loss: 0.7061 - val_acc: 0.7350\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.6031 - acc: 0.7885 - val_loss: 0.7005 - val_acc: 0.7350\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5970 - acc: 0.7917 - val_loss: 0.7003 - val_acc: 0.7370\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.5916 - acc: 0.7957 - val_loss: 0.6925 - val_acc: 0.7430\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5846 - acc: 0.7964 - val_loss: 0.6883 - val_acc: 0.7470\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5792 - acc: 0.7991 - val_loss: 0.6835 - val_acc: 0.7400\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5740 - acc: 0.8025 - val_loss: 0.6814 - val_acc: 0.7480\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5686 - acc: 0.8027 - val_loss: 0.6794 - val_acc: 0.7450\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5633 - acc: 0.8019 - val_loss: 0.6813 - val_acc: 0.7400\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5586 - acc: 0.8049 - val_loss: 0.6729 - val_acc: 0.7470\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5528 - acc: 0.8075 - val_loss: 0.6695 - val_acc: 0.7430\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5482 - acc: 0.8091 - val_loss: 0.6668 - val_acc: 0.7510\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5433 - acc: 0.8129 - val_loss: 0.6675 - val_acc: 0.7530\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5388 - acc: 0.8133 - val_loss: 0.6615 - val_acc: 0.7430\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5346 - acc: 0.8144 - val_loss: 0.6639 - val_acc: 0.7520\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5298 - acc: 0.8164 - val_loss: 0.6573 - val_acc: 0.7540\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5256 - acc: 0.8175 - val_loss: 0.6546 - val_acc: 0.7470\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5209 - acc: 0.8205 - val_loss: 0.6519 - val_acc: 0.7530\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5164 - acc: 0.8205 - val_loss: 0.6519 - val_acc: 0.7530\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.5130 - acc: 0.8239 - val_loss: 0.6537 - val_acc: 0.7540\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.5084 - acc: 0.8213 - val_loss: 0.6460 - val_acc: 0.7520\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5047 - acc: 0.8265 - val_loss: 0.6465 - val_acc: 0.7550\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5010 - acc: 0.8283 - val_loss: 0.6462 - val_acc: 0.7550\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.4971 - acc: 0.8269 - val_loss: 0.6447 - val_acc: 0.7450\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.4935 - acc: 0.8297 - val_loss: 0.6451 - val_acc: 0.7570\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4894 - acc: 0.8304 - val_loss: 0.6389 - val_acc: 0.7540\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.4857 - acc: 0.8316 - val_loss: 0.6368 - val_acc: 0.7570\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.4821 - acc: 0.8332 - val_loss: 0.6381 - val_acc: 0.7540\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4786 - acc: 0.8347 - val_loss: 0.6350 - val_acc: 0.7570\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4752 - acc: 0.8359 - val_loss: 0.6335 - val_acc: 0.7560\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4717 - acc: 0.8369 - val_loss: 0.6320 - val_acc: 0.7560\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4682 - acc: 0.8388 - val_loss: 0.6320 - val_acc: 0.7470\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4647 - acc: 0.8412 - val_loss: 0.6346 - val_acc: 0.7620\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4613 - acc: 0.8399 - val_loss: 0.6300 - val_acc: 0.7510\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4584 - acc: 0.8423 - val_loss: 0.6283 - val_acc: 0.7550\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4549 - acc: 0.8436 - val_loss: 0.6286 - val_acc: 0.7510\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4519 - acc: 0.8465 - val_loss: 0.6325 - val_acc: 0.7570\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4490 - acc: 0.8475 - val_loss: 0.6257 - val_acc: 0.7550\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4452 - acc: 0.8509 - val_loss: 0.6249 - val_acc: 0.7550\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4422 - acc: 0.8507 - val_loss: 0.6248 - val_acc: 0.7540\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4389 - acc: 0.8513 - val_loss: 0.6257 - val_acc: 0.7550\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4362 - acc: 0.8524 - val_loss: 0.6235 - val_acc: 0.7550\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4334 - acc: 0.8548 - val_loss: 0.6243 - val_acc: 0.7550\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4304 - acc: 0.8552 - val_loss: 0.6241 - val_acc: 0.7570\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4276 - acc: 0.8567 - val_loss: 0.6216 - val_acc: 0.7610\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4244 - acc: 0.8563 - val_loss: 0.6227 - val_acc: 0.7600\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4215 - acc: 0.8593 - val_loss: 0.6262 - val_acc: 0.7590\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4186 - acc: 0.8597 - val_loss: 0.6211 - val_acc: 0.7630\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4162 - acc: 0.8611 - val_loss: 0.6210 - val_acc: 0.7570\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.4130 - acc: 0.8612 - val_loss: 0.6182 - val_acc: 0.7600\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4105 - acc: 0.8633 - val_loss: 0.6188 - val_acc: 0.7610\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4075 - acc: 0.8643 - val_loss: 0.6190 - val_acc: 0.7640\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.4050 - acc: 0.8649 - val_loss: 0.6206 - val_acc: 0.7620\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4022 - acc: 0.8652 - val_loss: 0.6213 - val_acc: 0.7610\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3997 - acc: 0.8679 - val_loss: 0.6190 - val_acc: 0.7610\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3971 - acc: 0.8679 - val_loss: 0.6180 - val_acc: 0.7680\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3945 - acc: 0.8695 - val_loss: 0.6178 - val_acc: 0.7560\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3918 - acc: 0.8703 - val_loss: 0.6194 - val_acc: 0.7590\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.3892 - acc: 0.8705 - val_loss: 0.6210 - val_acc: 0.7600\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.3873 - acc: 0.8727 - val_loss: 0.6179 - val_acc: 0.7590\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3838 - acc: 0.8725 - val_loss: 0.6176 - val_acc: 0.7610\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.3821 - acc: 0.8731 - val_loss: 0.6185 - val_acc: 0.7610\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.3799 - acc: 0.8731 - val_loss: 0.6172 - val_acc: 0.7610\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.3769 - acc: 0.8747 - val_loss: 0.6182 - val_acc: 0.7610\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3747 - acc: 0.8763 - val_loss: 0.6173 - val_acc: 0.7640\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.3718 - acc: 0.8777 - val_loss: 0.6197 - val_acc: 0.7620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.3696 - acc: 0.8792 - val_loss: 0.6190 - val_acc: 0.7600\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 55us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 58us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3669822705030441, 0.8816]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6777662576039633, 0.7519999995231629]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.9728 - acc: 0.1431 - val_loss: 1.9505 - val_acc: 0.1640\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9409 - acc: 0.1717 - val_loss: 1.9291 - val_acc: 0.1820\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9185 - acc: 0.1996 - val_loss: 1.9108 - val_acc: 0.1930\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.8973 - acc: 0.2263 - val_loss: 1.8919 - val_acc: 0.2180\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8748 - acc: 0.2549 - val_loss: 1.8710 - val_acc: 0.2430\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.8503 - acc: 0.2773 - val_loss: 1.8464 - val_acc: 0.2780\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8221 - acc: 0.3044 - val_loss: 1.8178 - val_acc: 0.3070\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.7900 - acc: 0.3384 - val_loss: 1.7858 - val_acc: 0.3360\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.7536 - acc: 0.3699 - val_loss: 1.7496 - val_acc: 0.3650\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.7125 - acc: 0.4048 - val_loss: 1.7077 - val_acc: 0.4100\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6666 - acc: 0.4473 - val_loss: 1.6625 - val_acc: 0.4440\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6165 - acc: 0.4784 - val_loss: 1.6128 - val_acc: 0.4740\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.5631 - acc: 0.5117 - val_loss: 1.5609 - val_acc: 0.5170\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.5094 - acc: 0.544 - 0s 58us/step - loss: 1.5067 - acc: 0.5455 - val_loss: 1.5076 - val_acc: 0.5260\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.4496 - acc: 0.5628 - val_loss: 1.4520 - val_acc: 0.5530\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3920 - acc: 0.5844 - val_loss: 1.3961 - val_acc: 0.5710\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3354 - acc: 0.6037 - val_loss: 1.3424 - val_acc: 0.5840\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2810 - acc: 0.6176 - val_loss: 1.2916 - val_acc: 0.6020\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2291 - acc: 0.6297 - val_loss: 1.2438 - val_acc: 0.6110\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1808 - acc: 0.6459 - val_loss: 1.2003 - val_acc: 0.6210\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1360 - acc: 0.6548 - val_loss: 1.1589 - val_acc: 0.6290\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0947 - acc: 0.6601 - val_loss: 1.1195 - val_acc: 0.6330\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0570 - acc: 0.6700 - val_loss: 1.0857 - val_acc: 0.6450\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0222 - acc: 0.6779 - val_loss: 1.0540 - val_acc: 0.6420\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9911 - acc: 0.6821 - val_loss: 1.0243 - val_acc: 0.6460\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9622 - acc: 0.6911 - val_loss: 1.0000 - val_acc: 0.6580\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9359 - acc: 0.6989 - val_loss: 0.9754 - val_acc: 0.6620\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9114 - acc: 0.7033 - val_loss: 0.9541 - val_acc: 0.6640\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8891 - acc: 0.7067 - val_loss: 0.9362 - val_acc: 0.6720\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8686 - acc: 0.7091 - val_loss: 0.9160 - val_acc: 0.6700\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8498 - acc: 0.7172 - val_loss: 0.8992 - val_acc: 0.6750\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8319 - acc: 0.7204 - val_loss: 0.8832 - val_acc: 0.6770\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8158 - acc: 0.7225 - val_loss: 0.8691 - val_acc: 0.6830\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8005 - acc: 0.7297 - val_loss: 0.8572 - val_acc: 0.6910\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7860 - acc: 0.7308 - val_loss: 0.8470 - val_acc: 0.6850\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7725 - acc: 0.7343 - val_loss: 0.8323 - val_acc: 0.6930\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7601 - acc: 0.7360 - val_loss: 0.8241 - val_acc: 0.6970\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7479 - acc: 0.7425 - val_loss: 0.8133 - val_acc: 0.6990\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7361 - acc: 0.7447 - val_loss: 0.8044 - val_acc: 0.7090\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7262 - acc: 0.7493 - val_loss: 0.7950 - val_acc: 0.7100\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.7157 - acc: 0.7537 - val_loss: 0.7879 - val_acc: 0.7100\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7059 - acc: 0.7551 - val_loss: 0.7798 - val_acc: 0.7190\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6964 - acc: 0.7592 - val_loss: 0.7706 - val_acc: 0.7150\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6876 - acc: 0.7624 - val_loss: 0.7655 - val_acc: 0.7150\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6791 - acc: 0.7641 - val_loss: 0.7571 - val_acc: 0.7190\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6709 - acc: 0.7665 - val_loss: 0.7553 - val_acc: 0.7170\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6634 - acc: 0.7701 - val_loss: 0.7456 - val_acc: 0.7190\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.6555 - acc: 0.7723 - val_loss: 0.7399 - val_acc: 0.7160\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6481 - acc: 0.7776 - val_loss: 0.7401 - val_acc: 0.7160\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6412 - acc: 0.7783 - val_loss: 0.7290 - val_acc: 0.7220\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6341 - acc: 0.7820 - val_loss: 0.7247 - val_acc: 0.7190\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.6275 - acc: 0.7833 - val_loss: 0.7216 - val_acc: 0.7270\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6213 - acc: 0.7852 - val_loss: 0.7174 - val_acc: 0.7290\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.6149 - acc: 0.7861 - val_loss: 0.7124 - val_acc: 0.7260\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.6087 - acc: 0.7885 - val_loss: 0.7070 - val_acc: 0.7290\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6029 - acc: 0.7895 - val_loss: 0.7037 - val_acc: 0.7270\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.5968 - acc: 0.7920 - val_loss: 0.7041 - val_acc: 0.7320\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5913 - acc: 0.7943 - val_loss: 0.6956 - val_acc: 0.7290\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5862 - acc: 0.7961 - val_loss: 0.6947 - val_acc: 0.7320\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.5805 - acc: 0.7963 - val_loss: 0.6912 - val_acc: 0.7310\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 45us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 65us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5761956606388092, 0.7996]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.747904699643453, 0.7253333328564961]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 2.5979 - acc: 0.1409 - val_loss: 2.5992 - val_acc: 0.1380\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.5685 - acc: 0.1779 - val_loss: 2.5774 - val_acc: 0.1620\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.5454 - acc: 0.2067 - val_loss: 2.5577 - val_acc: 0.1870\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.5225 - acc: 0.2155 - val_loss: 2.5372 - val_acc: 0.1970\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.4981 - acc: 0.2259 - val_loss: 2.5151 - val_acc: 0.2020\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.4714 - acc: 0.2385 - val_loss: 2.4895 - val_acc: 0.2100\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 2.4410 - acc: 0.2596 - val_loss: 2.4605 - val_acc: 0.2340\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.4063 - acc: 0.2883 - val_loss: 2.4257 - val_acc: 0.2660\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 2.3672 - acc: 0.3197 - val_loss: 2.3872 - val_acc: 0.2980\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.3238 - acc: 0.3527 - val_loss: 2.3439 - val_acc: 0.3330\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.2767 - acc: 0.3887 - val_loss: 2.2982 - val_acc: 0.3700\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.2276 - acc: 0.4169 - val_loss: 2.2512 - val_acc: 0.3930\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.1780 - acc: 0.4451 - val_loss: 2.2038 - val_acc: 0.4190\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 2.1298 - acc: 0.475 - 0s 48us/step - loss: 2.1280 - acc: 0.4780 - val_loss: 2.1549 - val_acc: 0.4550\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.0784 - acc: 0.5040 - val_loss: 2.1074 - val_acc: 0.4910\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.0291 - acc: 0.5353 - val_loss: 2.0603 - val_acc: 0.5080\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.9806 - acc: 0.5593 - val_loss: 2.0141 - val_acc: 0.5260\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.9336 - acc: 0.5821 - val_loss: 1.9706 - val_acc: 0.5390\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8887 - acc: 0.6003 - val_loss: 1.9285 - val_acc: 0.5690\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8463 - acc: 0.6171 - val_loss: 1.8887 - val_acc: 0.5760\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8058 - acc: 0.6285 - val_loss: 1.8511 - val_acc: 0.5830\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.7675 - acc: 0.6419 - val_loss: 1.8141 - val_acc: 0.6030\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.7316 - acc: 0.6537 - val_loss: 1.7827 - val_acc: 0.6060\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6979 - acc: 0.6603 - val_loss: 1.7489 - val_acc: 0.6210\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6656 - acc: 0.6692 - val_loss: 1.7203 - val_acc: 0.6270\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6357 - acc: 0.6752 - val_loss: 1.6925 - val_acc: 0.6320\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6071 - acc: 0.6837 - val_loss: 1.6633 - val_acc: 0.6460\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5805 - acc: 0.6893 - val_loss: 1.6384 - val_acc: 0.6450\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5551 - acc: 0.6948 - val_loss: 1.6146 - val_acc: 0.6570\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5312 - acc: 0.7011 - val_loss: 1.5930 - val_acc: 0.6660\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5089 - acc: 0.7041 - val_loss: 1.5715 - val_acc: 0.6600\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.4878 - acc: 0.7108 - val_loss: 1.5509 - val_acc: 0.6680\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4677 - acc: 0.7151 - val_loss: 1.5322 - val_acc: 0.6770\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.4487 - acc: 0.7181 - val_loss: 1.5149 - val_acc: 0.6800\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4305 - acc: 0.7217 - val_loss: 1.4980 - val_acc: 0.6780\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.4132 - acc: 0.7267 - val_loss: 1.4821 - val_acc: 0.6830\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3969 - acc: 0.7297 - val_loss: 1.4669 - val_acc: 0.6840\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3813 - acc: 0.7325 - val_loss: 1.4523 - val_acc: 0.6870\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3666 - acc: 0.7364 - val_loss: 1.4395 - val_acc: 0.6900\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.3525 - acc: 0.7372 - val_loss: 1.4258 - val_acc: 0.6920\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3387 - acc: 0.7415 - val_loss: 1.4126 - val_acc: 0.6920\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3255 - acc: 0.7455 - val_loss: 1.3988 - val_acc: 0.6980\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3129 - acc: 0.7471 - val_loss: 1.3873 - val_acc: 0.6970\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3008 - acc: 0.7508 - val_loss: 1.3783 - val_acc: 0.6970\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2890 - acc: 0.7541 - val_loss: 1.3653 - val_acc: 0.6990\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2780 - acc: 0.7569 - val_loss: 1.3569 - val_acc: 0.7060\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2673 - acc: 0.7596 - val_loss: 1.3470 - val_acc: 0.7060\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2566 - acc: 0.7635 - val_loss: 1.3385 - val_acc: 0.6990\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2469 - acc: 0.7668 - val_loss: 1.3322 - val_acc: 0.7070\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2371 - acc: 0.7653 - val_loss: 1.3211 - val_acc: 0.7100\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2275 - acc: 0.7685 - val_loss: 1.3137 - val_acc: 0.7140\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2185 - acc: 0.7700 - val_loss: 1.3040 - val_acc: 0.7130\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2096 - acc: 0.7735 - val_loss: 1.2966 - val_acc: 0.7170\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.2011 - acc: 0.7749 - val_loss: 1.2901 - val_acc: 0.7200\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1928 - acc: 0.7768 - val_loss: 1.2822 - val_acc: 0.7180\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1844 - acc: 0.7809 - val_loss: 1.2748 - val_acc: 0.7270\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1766 - acc: 0.7817 - val_loss: 1.2692 - val_acc: 0.7280\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1689 - acc: 0.7837 - val_loss: 1.2645 - val_acc: 0.7200\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1613 - acc: 0.7847 - val_loss: 1.2569 - val_acc: 0.7230\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1541 - acc: 0.7851 - val_loss: 1.2510 - val_acc: 0.7260\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1471 - acc: 0.7895 - val_loss: 1.2456 - val_acc: 0.7270\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1402 - acc: 0.7907 - val_loss: 1.2390 - val_acc: 0.7320\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1330 - acc: 0.7933 - val_loss: 1.2352 - val_acc: 0.7270\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1261 - acc: 0.7949 - val_loss: 1.2300 - val_acc: 0.7340\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1196 - acc: 0.7921 - val_loss: 1.2233 - val_acc: 0.7310\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1129 - acc: 0.7961 - val_loss: 1.2192 - val_acc: 0.7320\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1065 - acc: 0.8005 - val_loss: 1.2140 - val_acc: 0.7370\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1006 - acc: 0.8001 - val_loss: 1.2148 - val_acc: 0.7240\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0943 - acc: 0.8043 - val_loss: 1.2042 - val_acc: 0.7370\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0881 - acc: 0.8041 - val_loss: 1.2016 - val_acc: 0.7350\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0821 - acc: 0.8048 - val_loss: 1.1969 - val_acc: 0.7380\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0763 - acc: 0.8069 - val_loss: 1.1928 - val_acc: 0.7400\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 1.0707 - acc: 0.8071 - val_loss: 1.1874 - val_acc: 0.7430\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0646 - acc: 0.8117 - val_loss: 1.1835 - val_acc: 0.7390\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0591 - acc: 0.8128 - val_loss: 1.1798 - val_acc: 0.7400\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0537 - acc: 0.8137 - val_loss: 1.1753 - val_acc: 0.7410\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0487 - acc: 0.8149 - val_loss: 1.1714 - val_acc: 0.7400\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0431 - acc: 0.8155 - val_loss: 1.1676 - val_acc: 0.7490\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0376 - acc: 0.8189 - val_loss: 1.1654 - val_acc: 0.7490\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0324 - acc: 0.8185 - val_loss: 1.1610 - val_acc: 0.7470\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0272 - acc: 0.8219 - val_loss: 1.1570 - val_acc: 0.7480\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0221 - acc: 0.8227 - val_loss: 1.1538 - val_acc: 0.7480\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0171 - acc: 0.8233 - val_loss: 1.1522 - val_acc: 0.7400\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0123 - acc: 0.8259 - val_loss: 1.1477 - val_acc: 0.7480\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0072 - acc: 0.8247 - val_loss: 1.1479 - val_acc: 0.7470\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.0036 - acc: 0.828 - 0s 54us/step - loss: 1.0024 - acc: 0.8291 - val_loss: 1.1403 - val_acc: 0.7490\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9981 - acc: 0.8296 - val_loss: 1.1380 - val_acc: 0.7510\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9930 - acc: 0.8316 - val_loss: 1.1363 - val_acc: 0.7500\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9886 - acc: 0.8317 - val_loss: 1.1315 - val_acc: 0.7510\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9839 - acc: 0.8327 - val_loss: 1.1311 - val_acc: 0.7530\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9795 - acc: 0.8343 - val_loss: 1.1255 - val_acc: 0.7480\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9749 - acc: 0.8359 - val_loss: 1.1279 - val_acc: 0.7540\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9702 - acc: 0.8353 - val_loss: 1.1199 - val_acc: 0.7480\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9657 - acc: 0.8373 - val_loss: 1.1183 - val_acc: 0.7510\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9614 - acc: 0.8395 - val_loss: 1.1162 - val_acc: 0.7510\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9558 - acc: 0.839 - 0s 57us/step - loss: 0.9571 - acc: 0.8384 - val_loss: 1.1138 - val_acc: 0.7540\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9529 - acc: 0.8416 - val_loss: 1.1122 - val_acc: 0.7510\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9478 - acc: 0.8435 - val_loss: 1.1161 - val_acc: 0.7440\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9443 - acc: 0.8435 - val_loss: 1.1055 - val_acc: 0.7470\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9401 - acc: 0.8448 - val_loss: 1.1052 - val_acc: 0.7530\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9361 - acc: 0.8445 - val_loss: 1.1006 - val_acc: 0.7450\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9317 - acc: 0.8463 - val_loss: 1.0989 - val_acc: 0.7480\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9278 - acc: 0.8493 - val_loss: 1.0990 - val_acc: 0.7520\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9237 - acc: 0.8475 - val_loss: 1.0943 - val_acc: 0.7540\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9200 - acc: 0.8496 - val_loss: 1.0934 - val_acc: 0.7550\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9159 - acc: 0.8497 - val_loss: 1.0930 - val_acc: 0.7540\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9119 - acc: 0.8501 - val_loss: 1.0875 - val_acc: 0.7510\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9083 - acc: 0.8520 - val_loss: 1.0862 - val_acc: 0.7520\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9041 - acc: 0.8521 - val_loss: 1.0820 - val_acc: 0.7530\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9005 - acc: 0.8529 - val_loss: 1.0815 - val_acc: 0.7510\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8966 - acc: 0.8531 - val_loss: 1.0792 - val_acc: 0.7500\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8927 - acc: 0.8565 - val_loss: 1.0754 - val_acc: 0.7480\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8889 - acc: 0.8571 - val_loss: 1.0738 - val_acc: 0.7510\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8855 - acc: 0.8561 - val_loss: 1.0723 - val_acc: 0.7520\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8819 - acc: 0.8579 - val_loss: 1.0684 - val_acc: 0.7520\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8780 - acc: 0.8579 - val_loss: 1.0672 - val_acc: 0.7500\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8747 - acc: 0.8588 - val_loss: 1.0652 - val_acc: 0.7580\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8710 - acc: 0.8588 - val_loss: 1.0644 - val_acc: 0.7570\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8674 - acc: 0.8604 - val_loss: 1.0631 - val_acc: 0.7520\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8639 - acc: 0.8612 - val_loss: 1.0600 - val_acc: 0.7530\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 15.9777 - acc: 0.1621 - val_loss: 15.5696 - val_acc: 0.1610\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 15.2117 - acc: 0.1912 - val_loss: 14.8219 - val_acc: 0.1790\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 14.4714 - acc: 0.2176 - val_loss: 14.0959 - val_acc: 0.2010\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 13.7507 - acc: 0.2405 - val_loss: 13.3888 - val_acc: 0.2200\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 13.0484 - acc: 0.2595 - val_loss: 12.6998 - val_acc: 0.2350\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 12.3636 - acc: 0.2776 - val_loss: 12.0283 - val_acc: 0.2570\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 11.6974 - acc: 0.3019 - val_loss: 11.3759 - val_acc: 0.2930\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 11.0500 - acc: 0.3316 - val_loss: 10.7443 - val_acc: 0.3060\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 10.4239 - acc: 0.3677 - val_loss: 10.1345 - val_acc: 0.3420\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 9.8209 - acc: 0.4008 - val_loss: 9.5450 - val_acc: 0.3740\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 9.2411 - acc: 0.4375 - val_loss: 8.9787 - val_acc: 0.4070\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 8.6842 - acc: 0.4729 - val_loss: 8.4368 - val_acc: 0.4380\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 8.1505 - acc: 0.5051 - val_loss: 7.9158 - val_acc: 0.4650\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 7.6392 - acc: 0.5336 - val_loss: 7.4178 - val_acc: 0.5100\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 7.1520 - acc: 0.5577 - val_loss: 6.9445 - val_acc: 0.5280\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 6.6889 - acc: 0.5812 - val_loss: 6.4938 - val_acc: 0.5440\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 6.2496 - acc: 0.5981 - val_loss: 6.0673 - val_acc: 0.5670\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 5.8326 - acc: 0.6195 - val_loss: 5.6635 - val_acc: 0.5830\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 5.4381 - acc: 0.6329 - val_loss: 5.2804 - val_acc: 0.6070\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 5.0666 - acc: 0.6431 - val_loss: 4.9200 - val_acc: 0.6230\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 4.7179 - acc: 0.6537 - val_loss: 4.5836 - val_acc: 0.6290\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 4.3918 - acc: 0.6600 - val_loss: 4.2700 - val_acc: 0.6320\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 4.0873 - acc: 0.6707 - val_loss: 3.9756 - val_acc: 0.6450\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 3.8055 - acc: 0.6809 - val_loss: 3.7051 - val_acc: 0.6480\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 3.5456 - acc: 0.6843 - val_loss: 3.4573 - val_acc: 0.6630\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 3.3081 - acc: 0.6880 - val_loss: 3.2309 - val_acc: 0.6760\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 3.0925 - acc: 0.6892 - val_loss: 3.0230 - val_acc: 0.6730\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 2.8968 - acc: 0.6957 - val_loss: 2.8371 - val_acc: 0.6760\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.7233 - acc: 0.6935 - val_loss: 2.6733 - val_acc: 0.6760\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.5699 - acc: 0.6965 - val_loss: 2.5298 - val_acc: 0.6830\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.4367 - acc: 0.6971 - val_loss: 2.4052 - val_acc: 0.6840\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.3236 - acc: 0.6999 - val_loss: 2.3031 - val_acc: 0.6820\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.2295 - acc: 0.6984 - val_loss: 2.2163 - val_acc: 0.6930\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.1538 - acc: 0.7015 - val_loss: 2.1483 - val_acc: 0.6870\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.0955 - acc: 0.7011 - val_loss: 2.0967 - val_acc: 0.6820\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.0514 - acc: 0.7035 - val_loss: 2.0595 - val_acc: 0.6880\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.0182 - acc: 0.7011 - val_loss: 2.0280 - val_acc: 0.6910\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9912 - acc: 0.7016 - val_loss: 2.0023 - val_acc: 0.6940\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9682 - acc: 0.7047 - val_loss: 1.9802 - val_acc: 0.6910\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.9468 - acc: 0.7029 - val_loss: 1.9585 - val_acc: 0.6880\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9277 - acc: 0.7056 - val_loss: 1.9387 - val_acc: 0.6950\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.9095 - acc: 0.7057 - val_loss: 1.9205 - val_acc: 0.6910\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.8922 - acc: 0.7057 - val_loss: 1.9042 - val_acc: 0.6960\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.8762 - acc: 0.7053 - val_loss: 1.8906 - val_acc: 0.6950\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8610 - acc: 0.7047 - val_loss: 1.8724 - val_acc: 0.6960\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.8456 - acc: 0.7083 - val_loss: 1.8588 - val_acc: 0.6970\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8322 - acc: 0.7075 - val_loss: 1.8442 - val_acc: 0.7000\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8182 - acc: 0.7073 - val_loss: 1.8326 - val_acc: 0.6990\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8054 - acc: 0.7113 - val_loss: 1.8179 - val_acc: 0.6920\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.7930 - acc: 0.7092 - val_loss: 1.8063 - val_acc: 0.7080\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.7810 - acc: 0.7085 - val_loss: 1.7922 - val_acc: 0.7050\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.7691 - acc: 0.7111 - val_loss: 1.7801 - val_acc: 0.7050\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.7574 - acc: 0.7083 - val_loss: 1.7772 - val_acc: 0.6920\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.7468 - acc: 0.7099 - val_loss: 1.7583 - val_acc: 0.7020\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.7360 - acc: 0.7101 - val_loss: 1.7479 - val_acc: 0.7070\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.7250 - acc: 0.7109 - val_loss: 1.7397 - val_acc: 0.7080\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.7153 - acc: 0.7103 - val_loss: 1.7277 - val_acc: 0.7020\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.7052 - acc: 0.7129 - val_loss: 1.7162 - val_acc: 0.7090\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6953 - acc: 0.7109 - val_loss: 1.7074 - val_acc: 0.7070\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6862 - acc: 0.7116 - val_loss: 1.6988 - val_acc: 0.7080\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6762 - acc: 0.7115 - val_loss: 1.6885 - val_acc: 0.7120\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6674 - acc: 0.7113 - val_loss: 1.6785 - val_acc: 0.7130\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.6581 - acc: 0.7128 - val_loss: 1.6708 - val_acc: 0.7090\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6495 - acc: 0.7128 - val_loss: 1.6642 - val_acc: 0.7090\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6409 - acc: 0.7125 - val_loss: 1.6559 - val_acc: 0.7020\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6326 - acc: 0.7133 - val_loss: 1.6438 - val_acc: 0.7130\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6241 - acc: 0.7151 - val_loss: 1.6352 - val_acc: 0.7090\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.6160 - acc: 0.7151 - val_loss: 1.6256 - val_acc: 0.7120\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.6074 - acc: 0.7149 - val_loss: 1.6218 - val_acc: 0.7080\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6001 - acc: 0.7155 - val_loss: 1.6089 - val_acc: 0.7100\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5914 - acc: 0.7159 - val_loss: 1.6015 - val_acc: 0.7120\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5841 - acc: 0.7176 - val_loss: 1.5945 - val_acc: 0.7140\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5766 - acc: 0.7147 - val_loss: 1.5900 - val_acc: 0.7110\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5690 - acc: 0.7180 - val_loss: 1.5859 - val_acc: 0.7090\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.5619 - acc: 0.7167 - val_loss: 1.5755 - val_acc: 0.7160\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5542 - acc: 0.7168 - val_loss: 1.5653 - val_acc: 0.7160\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5474 - acc: 0.7160 - val_loss: 1.5626 - val_acc: 0.7100\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5400 - acc: 0.7176 - val_loss: 1.5534 - val_acc: 0.7180\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5335 - acc: 0.7165 - val_loss: 1.5438 - val_acc: 0.7140\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5258 - acc: 0.7177 - val_loss: 1.5379 - val_acc: 0.7180\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5189 - acc: 0.7192 - val_loss: 1.5351 - val_acc: 0.7170\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5121 - acc: 0.7187 - val_loss: 1.5276 - val_acc: 0.7170\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5061 - acc: 0.7196 - val_loss: 1.5160 - val_acc: 0.7180\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4987 - acc: 0.7201 - val_loss: 1.5149 - val_acc: 0.7160\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4923 - acc: 0.7193 - val_loss: 1.5078 - val_acc: 0.7200\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.4865 - acc: 0.7189 - val_loss: 1.4995 - val_acc: 0.7170\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.4794 - acc: 0.7216 - val_loss: 1.4906 - val_acc: 0.7180\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.4729 - acc: 0.7195 - val_loss: 1.4831 - val_acc: 0.7200\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.4672 - acc: 0.7204 - val_loss: 1.4771 - val_acc: 0.7180\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.4608 - acc: 0.7207 - val_loss: 1.4733 - val_acc: 0.7210\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4549 - acc: 0.7215 - val_loss: 1.4662 - val_acc: 0.7210\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.4485 - acc: 0.7223 - val_loss: 1.4654 - val_acc: 0.7190\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.4428 - acc: 0.7228 - val_loss: 1.4530 - val_acc: 0.7200\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.4370 - acc: 0.7220 - val_loss: 1.4519 - val_acc: 0.7200\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.4312 - acc: 0.7221 - val_loss: 1.4439 - val_acc: 0.7170\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4251 - acc: 0.7212 - val_loss: 1.4433 - val_acc: 0.7220\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4201 - acc: 0.7235 - val_loss: 1.4417 - val_acc: 0.7220\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4151 - acc: 0.7235 - val_loss: 1.4247 - val_acc: 0.7200\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4083 - acc: 0.7232 - val_loss: 1.4201 - val_acc: 0.7140\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.4030 - acc: 0.7257 - val_loss: 1.4180 - val_acc: 0.7220\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3979 - acc: 0.7249 - val_loss: 1.4146 - val_acc: 0.7230\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.3931 - acc: 0.7232 - val_loss: 1.4046 - val_acc: 0.7240\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3868 - acc: 0.7237 - val_loss: 1.3995 - val_acc: 0.7210\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3821 - acc: 0.7252 - val_loss: 1.3959 - val_acc: 0.7210\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3771 - acc: 0.7253 - val_loss: 1.3946 - val_acc: 0.7280\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3720 - acc: 0.7240 - val_loss: 1.3846 - val_acc: 0.7280\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.3674 - acc: 0.7259 - val_loss: 1.3777 - val_acc: 0.7270\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3615 - acc: 0.7249 - val_loss: 1.3796 - val_acc: 0.7250\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3568 - acc: 0.7276 - val_loss: 1.3672 - val_acc: 0.7260\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3519 - acc: 0.7267 - val_loss: 1.3629 - val_acc: 0.7240\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3469 - acc: 0.7265 - val_loss: 1.3644 - val_acc: 0.7190\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3424 - acc: 0.7263 - val_loss: 1.3533 - val_acc: 0.7300\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3367 - acc: 0.7261 - val_loss: 1.3504 - val_acc: 0.7270\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.3320 - acc: 0.7271 - val_loss: 1.3456 - val_acc: 0.7250\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3279 - acc: 0.7277 - val_loss: 1.3391 - val_acc: 0.7290\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3229 - acc: 0.7271 - val_loss: 1.3364 - val_acc: 0.7240\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3179 - acc: 0.7299 - val_loss: 1.3303 - val_acc: 0.7310\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3137 - acc: 0.7297 - val_loss: 1.3291 - val_acc: 0.7290\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3092 - acc: 0.7305 - val_loss: 1.3264 - val_acc: 0.7240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.3051 - acc: 0.7296 - val_loss: 1.3195 - val_acc: 0.7230\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8FFW2wPHfSZOEfQsgS8BEQNnXCCKobCpuKDwdRR0XVJ6Ojjo6o/JGR57OjI4rOjrOOCougzCOiiAPcUGCimEJu4DIFkgIa9jJnpz3R1XaTuispNPdyfl+Pnzoqrpdfaq6U6fq3lu3RFUxxhhjACKCHYAxxpjQYUnBGGOMlyUFY4wxXpYUjDHGeFlSMMYY42VJwRhjjJclhXKIiEdEjotIp+osG+pE5F8iMsV9PVxE1lekbBU+p9bss1AnIptE5Lwyln8nIrfUYEg1TkT+KCJvn8L73xCR/6nGkIrW+4WI3FDd662KWpcU3ANM0b9CEcnyma70TlfVAlVtrKo7q7NsVYjI2SKyUkSOiciPIjI6EJ9TkqomqmrP6lhXyQNPoPeZ+ZmqnqWq30K1HBxHi0hKKctGiUiiiBwVkS1V/YxQpKq3q+qfT2Ud/va9ql6kqtNPKbhqUuuSgnuAaayqjYGdwBU+807a6SJSr+ajrLK/AXOApsClwK7ghmNKIyIRIlLr/r4q6ATwBvBwZd8Yyn+PIuIJdgw1oc79aN0s/W8RmSEix4AbRWSIiCwRkcMisltEXhaRSLd8PRFREYlzp//lLv/MPWNPEpH4ypZ1l18iIj+JyBER+auILC7n8j0f2KGObaq6sZxt3SwiY3ymo0TkoIj0cQ9aH4rIHne7E0WkeynrKXZWKCIDRWS1u00zgGifZTEiMk9E9ovIIRH5VEQ6uMv+AgwB/u5euU31s8+au/ttv4ikiMhkERF32e0iskhEXnRj3iYiF5Wx/Y+6ZY6JyHoRGVti+X+7V1zHROQHEenrzj9dRD5xYzggIi+584ud4YlIFxFRn+nvRORJEUnCOTB2cmPe6H7GVhG5vUQM4919eVREtojIRSIyQUSWlij3sIh86GcbLxSRVT7TiSLyvc/0EhG53H2dJk5V4OXAQ8AN7vewwmeV8SLyvRvvfBFpWdr+LY2qLlHVfwHbyytbtA9F5FYR2Ql84c4fKj//Ta4WkfN93tPZ3dfHxKl2ea3oeyn5W/Xdbj+fXebfgPs7fNXdDyeA86R4tepncnLNxI3uslfczz0qIstF5Fx3vt99Lz5X0G5cfxCRHSKyT0TeFpGmJfbXTe7694vIIxX7ZipIVWvtPyAFGF1i3h+BXOAKnKTYADgbGAzUA84AfgLuccvXAxSIc6f/BRwAEoBI4N/Av6pQtg1wDLjSXfYAkAfcUsb2vAQcBPpWcPufAN7xmb4S+MF9HQHcAjQB6gOvAMk+Zf8FTHFfjwZS3NfRQBpwrxv3dW7cRWVbA+Pc/doU+Bj40Ge93/luo5999r77nibud7EFuNlddrv7WRMBD/BrILWM7f8F0M7d1uuB48Bp7rIJQCowEBDgTKCjG88PwHNAI3c7hvr8dt72WX8XQEtsWwrQ3d039XB+Z2e4nzESyAL6uOXPBQ4Do9wYOwJnuZ95GOjqs+51wJV+trERkA20AKKAPcBud37RsuZu2TRguL9t8Yl/M9AVaAh8C/yxlH3r/U2Usf/HAFvKKdPF/f6nuZ/ZwN0PGcDF7n4Zg/N3FOO+ZxnwF3d7z8f5O3q7tLhK224q9jdwCOdEJgLnt+/9uyjxGZfjXLl3cKd/CbR0fwMPu8uiy9n3t7ivJ+Ecg+Ld2GYD00rsr7+7MQ8Acnx/K6f6r85dKbi+U9VPVbVQVbNUdbmqLlXVfFXdBrwOXFDG+z9U1WRVzQOmA/2qUPZyYLWqznaXvYjzw/fLPQMZCtwI/J+I9HHnX1LyrNLH+8BVIlLfnb7enYe77W+r6jFVzQamAANFpFEZ24IbgwJ/VdU8VZ0JeM9UVXW/qs5y9+tR4M+UvS99tzES50D+iBvXNpz98kufYltV9S1VLQDeAWJFpJW/9anqB6q6293W93EO2Anu4tuBp1V1hTp+UtVUnANAK+BhVT3hbsfiisTvektVN7r7Jt/9nW1zP+NrYAFQ1Nh7G/BPVV3gxpiqqptUNQv4D853jYj0w0lu8/xs4wmc/X8eMAhYCSS523EusEFVD1ci/jdVdbOqZroxlPXbrk6Pq2qmu+03AXNU9XN3v8wH1gBjROQMoC/OgTlXVb8B/q8qH1jBv4FZqprkls3xtx4R6Qa8BVyjqrvcdb+nqgdVNR94BucEqUsFQ7sBeE5Vt6vqMeB/gOuleHXkFFXNVtWVwHqcfVIt6mpSSPWdEJFuIvJ/7mXkUZwzbL8HGtcen9eZQOMqlG3vG4c6pwFpZaznPuBlVZ0H3A184SaGc4Gv/L1BVX8EtgKXiUhjnET0Pnh7/TwjTvXKUZwzcih7u4viTnPjLbKj6IWINBKnh8ZOd71fV2CdRdrgXAHs8Jm3A+jgM11yf0Ip+19EbhGRNW7VwGGgm08sHXH2TUkdcc40CyoYc0klf1uXi8hScartDgMXVSAGcBJeUceIG4F/uycP/iwChuOcNS8CEnES8QXudGVU5rddnXz32+nAhKLvzd1v5+D89toDGW7y8PfeCqvg30CZ6xaR5jjtfJNV1bfa7iFxqiaP4FxtNKLifwftOflvIArnKhwAVQ3Y91RXk0LJoWH/gVNl0EVVmwJ/wLncD6TdQGzRhIgIxQ9+JdXDaVNAVWfjXJJ+hXPAmFrG+2bgVJWMw7kySXHn34TTWD0SaMbPZzHlbXexuF2+3UkfwrnsHeTuy5ElypY1LO8+oADnoOC77ko3qLtnlK8Bd+FUOzQHfuTn7UsFOvt5aypwuvhvVDyBU8VRpK2fMr5tDA2AD4GncKqtmuPUmZcXA6r6nbuOoTjf33v+yrlKJoVFlJ8UQmp45BInGak41SXNff41UtVncX5/MT5Xv+Ak1yLFviNxGq5jSvnYivwNlLqf3N/ITGC+qr7pM38ETnXwfwHNcar2jvust7x9n87JfwO5wP5y3lct6mpSKKkJcAQ44TY0/XcNfOZcYICIXOH+cO/D50zAj/8AU0Skt3sZ+SPOD6UBTt1iaWYAl+DUU77vM78JTl1kBs4f0Z8qGPd3QISI3CNOI/E1OPWavuvNBA6JSAxOgvW1F6eO/STumfCHwJ9FpLE4jfK/wanHrazGOH98+3Fy7u04VwpF3gAeEpH+4ugqIh1xql4y3BgaikgD98AMsBq4QEQ6umeI5TXwReOc4e0HCtxGxlE+y98EbheREW7jYqyInOWz/D2cxHZCVZeU8TnfAT2B/sAKYC3OAS4Bp13An71AnHsyUlUiIvVL/BN3W+rjtKsUlYmsxHrfA8aJ04jucd8/QkTaq+pWnPaVx8XpODEMuMznvT8CTUTkYvczH3fj8KeqfwNFnubn9sCS683HqQ6OxKmW8q2SKm/fzwAeEJE4EWnixjVDVQsrGV+VWFJwPAjcjNNg9Q+cBuGAUtW9wLXACzg/ys44dcN+6y1xGtbexblUPYhzdXA7zg/o/4p6J/j5nDQgGefy+wOfRdNwzkjSceokvz/53X7Xl4Nz1XEHzmXxeOATnyIv4Jx1Zbjr/KzEKqbyc9XAC34+4lc4yW47zlnuO+52V4qqrgVexmmU3I2TEJb6LJ+Bs0//DRzFadxu4dYBX47TWJyK0635avdt84FZOAelZTjfRVkxHMZJarNwvrOrcU4GipZ/j7MfX8Y5KVlI8bPed4FelH2VgFvvvBZY67ZlqBvfFlXNKOVt/8ZJWAdFZFlZ6y9DJ5yGc99/p/Nzg/ocnBOALE7+HZTKvZodBzyGk1B34vyNFh2vJuBcFWXgHPT/jft3o6qHcDogvINzhXmQ4lVivqr0N+BjAm5nAfm5B9K1OG0/X+E02qfg/L52+7yvvH3/T7fMt8A2nOPSfZWMrcqk+FWbCRb3UjQduFrdG4xM3eY2eO4Deqlqud076yoR+QinavTJYMdSG9iVQhCJyBgRaSYi0ThnRfk4Z3jGgNOhYLElhOJEZJCIxLvVVJfiXNnNDnZctUXI3j1YRwzD6aYahXP5elVp3d5M3SIiaTj3ZFwZ7FhCUHvgI5z7ANKAO9zqQlMNrPrIGGOMl1UfGWOM8Qq76qNWrVppXFxcsMMwxpiwsmLFigOqWla3dyAMk0JcXBzJycnBDsMYY8KKiOwov5RVHxljjPFhScEYY4yXJQVjjDFelhSMMcZ4WVIwxhjjZUnBGGOMlyUFY4wJAUmpSTz17VMkpSYFNY6wu0/BGGMCJSk1icSURIbHDQfwvh7ScUiV1+PvvSWXJ6UmMerdUeQW5BLpieTJEU+SeiSV05udTteYrsQ1jyOueRxNoptUx2aWyZKCMcZAsQOzJ8KDIOQX5uOJ8DCx30Ru6nsT4CSKszuczdo9a9lyaAuXnXkZl3a5lKJn5viuJ8oTxdQxU8nIzKBRVCO2H9pOhETwyvJXyC/IJ9ITyXMXPcc7q98hK995wmhBfgG/+/J3fmP86yV/5Z5B9wR0P4TdgHgJCQlqdzQbU3dV5Gz+UNYhZv04i5k/zORYzjHaNmlLo8hGTOg1gZYNWnrfc1ars1iRvoLVe1Yzb8s8FqUsQlHEfXKm+jw5M4IIRISCUh7f7REPUZ4oFCU7P7tS2xQVEUWBFqCqIKCq3jgGdRjE5WdeTvqxdDzi4fre11f6ygVARFaoakK55SwpGGNOhb+qkiPZR9h5ZCc92/RkadrSk5YnpSbx9favifREsuPwDjo27ci+E/tYtXcVzaObcyLvBMdyjtGuSTsUpUuLLnRq1onD2Yf507d/Iq8wD0Eo1ELvgbthZENa1G9BiwYt2HRgE3mFeQHdbkGKJY16EfUYETeC9KPpbMzYSNGxVUs8klkQPBEeCgsLKaQQQahfr773iiKmYQz3z7+fnPwcCikkQiKoF1HPe+US5YliwU0LKp0YKpoUrPrImFqs6IAd0zCGjMyMkw7M/g7WC1MWckaLM9h0YBPr9q3jtEankZ2fTfsm7dl9bDfr9q2jcVRjjucd52DmQbYf3k6hFlIvoh439L6B5N3JbNy/kQItoHFkY7Lysyh0Hy/crH4zIojgYPbBat/WzLxMcvJz6NyyM/cOvpes/Cz+kfyPUs/si0RGRBIhEd6qopv73szQjkPZemgrK9JXsGD7Au8y3yolQcgtyPUe9AsLC0lMSfTui4n9JtK/Xf+TDvDRnmimjpnKRxs+4qvtX1GoheQW5JKRmcHk8yYD0LtNb6YkTvEuzytwEpyi5BbkkpiSWKWrhYqwpGBMAJTX0FjZstn52RzMOsiR7CMczTlK2tE0Fqcu5outXyAiTOw3kf9O+G+mr53OS0tfYu+JvXRq2om1e9eSr/ne9URGRPL7837PjiM7eGfNOxSqc6AaGTeS1KOp/JTx00lntqUpeaacV5jH22veLrb8RN6JYmUOZx/2Vs2UXBdQZtVNREQEBYUF3jKRnsiTDswAPVr1IKZBDDENY4jyRPk94/Z9T35h/s+fVwjxzeO5ud/N3vWVVl0F8O6ad5m2ehr5hfneqqVCLYRC6NSsE5MGTqJ3m95+E3PvNr35due33raHonUCDOk4hCnDp3iX+yakkmWrm1UfGVMJe4/v5WjOUbq07OJtWPSVlJpU7EAR5Yni3XHvckaLMwC8VQUNIhtwIvcEX277koe+fMhbHRIhTr110d9llCcKwNsI6ateRD0KtdB7Fl50kB7QbgAD2g5g/tb5pB1N87sdJQ/o/pZD8QNzWQfuklUiRSKIwBPhIb8w31u2aF5BYYHf6pGiA2BeQZ53eYREeLc1gghGnzGaKcOnAMUPzCUPnr5VMkUH5PLeU9mqGd+rsfvn3+89yFdkPZXppQRV7w0F1qZg6qgTuSf467K/knY0jYs7X0yjyEYs3bWU4XHDOSf2HJbtWsbcn+bSrVU3xnUfR8PIhqgqqUdT2XdiH5l5mRzLOUb6sXSS0pLYcXgH/dv158yYM/l448d8sfULFKVNwzaM6ToGgB8P/MieY3uI8kSx7fA270G6KuKbx9Mtphtfbv+SgsICIiSCnm16cm7suTSNbsrK3StZtGMRBVpQ7EBZJNoTzctjXiYjy3/ddLQnmofOfYjUo6lMXzfde4ZbMrkUnYX7Hpj9Hbh9e+eUVVWyaveqYomytIN1yQNg0fLyDrhFB8+dR3byz5X/pEAL8IiHJ0c86a2SKak6D7j+1hmo6p2qsqRgaiVV5YutXzB702xaNWxFbNNYmtdvTr2IeqQdTeOp755iz/E9NKjXoNjZtSA0imrE8dzj3nkNIxsytONQ1u5dy94Te8v97LaN2nIg64D3YN0wsiGFWkhmXqb37Lnk2Xdp1R4PDnmQ3m16czj7MA988QC5+bl+z4qLYvdXVVLWWXihFvo9+PoeXEsezH3n+Xa/LO/A7a+doqJtGJVRkfeX7A5alQbZ2iokkoKIjAFeAjzAG6r6dInlLwIj3MmGQBtVbV7WOi0phK+DWQdJTEmke6vudGvVjaz8LD758RNmb5rNkewj5BfmFzvrbRjZkKbRTWlevzltGrWhef3mvL/ufZanL6dRZKNiDZhFhnYcyjMXPkNC+wR+9X+/4q1VbxXvISL18ER4yCvIQ0Q4vfnpDO04lNYNW7Nh/wYWbF/w81m4WxXiEQ/X9LiGg1kHvQ1/5R2Qiw6ugPfMtYhHPNwx4A46NevkPbj6Nir6W7e/6pqiHiu+Z+G+Z/3+zpSf+vYpHlv42Eln0qF8hltZtWlbqlPQk4KIeICfgAuBNGA5MEFVN5RS/tdAf1WdWNZ6LSmEBlVl1Z5V5OTn0CS6CftP7OfjjR8zd/Nc2jZuy9Xdr+bCzheSnZ/NvhP7+Hjjx8z4YYa3/3ZMgxhyCnI4nnuc9k3aE9s0lnoR9fCIx1k/SmZeJkdzjnIo6xAHsw6iKPHN45k8bDI397uZCIlg7qa5LNqxiAHtBuCJ8JByOIURcSOK3SWanZ/t9+BadFAcHjf8pHIlD/Bl1XEXrde3W2HJO1XL614InHTDk+/BvrSbqfydoZdXzWJn0nVTKCSFIcAUVb3YnZ4MoKpPlVL+e+BxVf2yrPVaUqhZmXmZzNo4i7mb59KhSQcGthvIoexD/G3531i/f32xsvXr1efCMy4k7Wgaq/asKrasYWRDbux9IxN6T2DboW18t/M7IiMiub739Zx3+nlESPFhuEqe7eUX5vP5ls9ZvWc1rRu19lsV4q+xsGTDr79yiSmJ3rNnOPkA71tPXdTI+V89/stvlUtZjYVFVSql1XuX1kW0svXelR1iwdQNoZAUrgbGqOrt7vQvgcGqetI92iJyOrAEiFU9uVOxiEwCJgF06tRp4I4dFXrUqClHbkEuG/dvpHvr7s6dmKos27WMDzd8yN4TezmQeYDFqYs5mnOU0xqdxuHsw+QU5ABwVsxZ/O7c3xHbNJajOUeJrhfNyPiRNI5qDMC2Q9tYmraUptFN2XVsF7uO7WJM5zGVHgemrEZM37N1f1cAvtUmZR1c/X1eybNwf2fXVT242tm6CYZQSArXABeXSAqDVPXXfso+jJMQTlpWkl0pVI9DWYcY9+9xLNqxiGbRzRjTZQybD25m5e6VRHuiadu4LTENY+jdpje39LuF808/n8U7F3PhexeSV5hHtCe6zIOZv+qM8g64pdXDe7s7luhp46+Kp7TqlbJirMhZeHWfXdvZuqlpoZAUKlx9JCKrgLtV9fvy1mtJ4dSlHknlkumX8FPGT0wZPoWtB7cyb8s8WjdszV0Jd3Fjnxv9jsZYWiOlr5LVNf66O5ZWNVO0vKyeNr5924u6O5bW/7yss3A7Wzd1TSgMc7Ec6Coi8cAu4Drg+pKFROQsoAUQ3EHEazFV5ZMfP+E/G/7Dhv0b+PHAj0TXi+bzGz9nRPyI8lfgGh43nChPlPdAGtMwhqe+feqkaphiDbbqHMyLBvhSlJz8HO6Zd4/3DL9eRD0KCwq9ywsKC7hjwB0A5fZtL3kgT0xJJL8wnwItKHM4gMSURHILcsstZ0xdE7CkoKr5InIP8DlOl9S3VHW9iDwBJKvqHLfoBGCmhtsNE2GgUAv5dNOnTFk0hdV7VtOucTv6te3H6DNGc1v/2+jeuvtJ7ynrlv7ElMSTBu0qWe3je3YvCNH1Tr55qeRwAP4SQFHVz019b6pUNUvJxFXacAAVLWdMXWM3r9VCOfk5vLf2PZ5Pep4fD/xI5xad+cMFf+D63tdTL6L084CyxpOvSI+domqfqnabrK569oqux+r1TV0S9DaFQLGkULZCLeTS6Zfy+dbP6d+2Pw8OeZBf9PwFyenJ5XZ39L2BqrRxbsrq21/ypiw7IBsTOkKhTcEEwYtJL/L51s+ZevFU7h18LyLi90lQ/m6Mqsg4N0VVLUM6DmHBTQtOatgtr8dPkSEdh1gyMCYEWVKoRVbtXsXkBZO5qttV3oQAxRtVixp5fYdQKCxwegYp6tycFf/zCJRlddksOrBXtt7fGBO6rPqoljiRe4KBrw/keO5x1ty5hpiGMd5lvlcKZY2IeSpPdTLGhDarPqpjHvryIX7K+ImvbvqqWEIAvFU9Zd1MBtUzdLAxJrxZUqgFPt/yOX9L/hsPnPMAI+NHFlvm26Dr+6g/fwnAkoExxqqPwtzBrIP0+lsvWjZoSfKkZOrXq+9dZnftGmOKVLT6KKK8AiZ0ncg9wQ0f38CBzAO8N+69YgkB/N+1a4wxZbHqozC16+guxs4cy+o9q/n7ZX+nf7v+J5Wxu3aNMZVlSSEMbT24lQvevoAjOUf4dMKnXNr1Ur/lfBuYrQHZGFMRlhTCTH5hPjfOupETeSdYPHExfU7rc1KZkncLWzIwxlSUJYUw85fv/sKStCW8P/79kxJCyWGrrXHZGFNZlhTCyMrdK5myaArX9ryWCb0nFFvmb9hqGxLaGFNZ1vsoTGTmZfLLWb+kTaM2/O2yv520vKinke+w1da4bIypLLtSCBMPfP4AG/Zv4Isbv6Blg5YnLfftaVTRx1EaY0xJlhTCwEcbPuIfK/7Bw0Mf5sLOF/otYz2NjDHVwZJCiNt5ZCe3f3o7gzoM4skRT5603HoaGWOqkyWFEPfbL35LbkEu749/n0hPJFD+E8yMMaaqLCmEsKVpS/nPhv/w+AWP07llZ6D0YbCtp5ExpjpYUghRqsrvvvwdpzU6jQeHPAg4CWFK4hRyCnIo1EIiNML7VDTraWSMqQ6WFELUpz99yrc7v+W1y16jSXQT7xVCTn6O95GZ0Z5opo6ZSkZmhjUuG2OqhSWFEKSqTF4wmbNizuK2/rcBP9+HUEhhsUdmWiIwxlQnSwohaEnaEjbs38DbV77tbVwuOeKpJQRjTCBYUghBM3+YSbQnmnHdx3nn2X0IxpiaYEkhxBQUFvDBhg+47MzLaBrd1O5DMMbUKEsKISYxJZE9x/cwodcEe5ymMabGBXRAPBEZIyKbRGSLiDxSSplfiMgGEVkvIu8HMp5wMPOHmTSOasxlXS+zx2kaY2pcwK4URMQDvApcCKQBy0Vkjqpu8CnTFZgMDFXVQyLSJlDxhIPcglw+2vgRV3W7igaRDexxmsaYGhfI6qNBwBZV3QYgIjOBK4ENPmXuAF5V1UMAqrovgPGEvC+2fsGh7ENc1/M6wBqXjTE1L5BJoQOQ6jOdBgwuUeZMABFZDHiAKao6v+SKRGQSMAmgU6dOAQk2FLy56k1aNWxVbCRUa1w2xtSkQLYpiJ95WmK6HtAVGA5MAN4QkeYnvUn1dVVNUNWE1q1bV3ugoWDboW3M/nE2kwZMIsoTFexwjDF1VCCTQhrQ0Wc6Fkj3U2a2quap6nZgE06SqHNeWfYKnggPvzr7V8EOxRhThwUyKSwHuopIvIhEAdcBc0qU+QQYASAirXCqk7YFMKaQdCznGG+uepNrelxDh6Ydgh2OMaYOC1hSUNV84B7gc2Aj8IGqrheRJ0RkrFvscyBDRDYAC4HfqWpGoGIKVW+vfpujOUe5b/B93nlJqUk89e1TJKUmBTEyY0xdI6olq/lDW0JCgiYnJwc7jGpTqIV0e6UbMQ1jSLrNSQB205oxprqJyApVTSivXEBvXjPlW5K2hM0HN/OrhJ/bEuymNWNMsFhSCLJZG2cRGRHJ2LPGeucV3bTmEY/dtGaMqVE29lEQqSqzfpzFyPiRNKvfzDvfblozxgSLJYUgWr9/PVsPbeW35/4WwEZENcYEnSWFIPrkx08AuPKsK61x2RgTEqxNIYhm/TiLc2LPoV2Tdta4bIwJCZYUgmTnkZ2s3L2Scd2cp6tZ47IxJhRY9VGQFFUdXdXtKsAal40xocGSQpDM3jSb7q26c2bMmd551rhsjAk2qz4KgkNZh1iUsogrz7oy2KEYY0wxlhSC4LMtn1GgBVzZzZKCMSa0WFIIgjmb5tCmURsGdRgE2OB3xpjQYW0KNSy3IJfPtnzGNT2uIUIi7P4EY0xIsSuFGvbNjm84mnPUO9aR3Z9gjAkllhRq2JxNc6hfrz6jzxgN2P0JxpjQYtVHNUhVmb1pNheecSENIxsCdn+CMSa0WFKoQev2rWPnkZ08dv5jxebb/QnGmFBh1Uc1aMG2BQBc3Pli63FkjAlJdqVQg75O+ZquLbuSdjTNehwZY0KSXSnUkPzCfBalLGJk/EjrcWSMCVl2pVBDVqSv4FjuMUbGj6Rj045EeaK8VwrW48gYEyosKdSQr7d/DcCIuBG0btTaehwZY0KSJYUasmD7Avqc1ofWjVoD1uPIGBOarE2hBmTnZ7M4dTEj40ZaryNjTEizK4UasCRtCdn52bRv0t56HRljQlpArxREZIyIbBKRLSLyiJ/lt4jIfhFZ7f67PZDxBMvX278mQiI4nnvceh0ZY0JawJKCiHiAV4FLgB7ABBHp4afov1W1n/vvjUDFE0wLUxYysN1AxnQZY+McGWNCWiCrjwYBW1R1G4CIzASuBDYE8DNDTk5+Dst3LeeeQffYOEfGmJAXyKTQAUj1mU4DBvsp918icj7wE/AbVU0tWUAVJToxAAAeTUlEQVREJgGTADp16hSAUANn5e6V5BTkMLTjUMB6HRljQlsg2xTEzzwtMf0pEKeqfYCvgHf8rUhVX1fVBFVNaN26dTWHGViLUxcDcG7Hc4MciTHGlC+QSSEN6OgzHQuk+xZQ1QxVzXEn/wkMDGA8QbE4dTGdW3TmtManBTsUY4wpVyCTwnKgq4jEi0gUcB0wx7eAiLTzmRwLbAxgPDVOVVm8czFDOw0NdijGGFMhAWtTUNV8EbkH+BzwAG+p6noReQJIVtU5wL0iMhbIBw4CtwQqnmDYemgr+zP3e9sTjDEm1AX05jVVnQfMKzHvDz6vJwOTAxlDMC3e6bQnWFIwxoQLG+YigBanLqZ5/eZ0b9092KEYY0yFWFIIoMWpixkSO4QIsd1sjAkPdrQKkINZB9mwf4NVHRljwoolhQBZkrYEsPsTjDHhxZJCgCzbtYwIieDsDmfbcNnGmLBRod5HItIZSFPVHBEZDvQB3lXVw4EMLpwtT19O91bdWbd3nQ2XbYwJGxW9UvgIKBCRLsCbQDzwfsCiCnOqyvJdyxnUYRCJKYk2XLYxJmxU9D6FQvdmtHHAVFX9q4isCmRg4WzHkR3sz9zP2e3Ppl/bfkR5orxXCjZctjEmlFU0KeSJyATgZuAKd15kYEIKf8t3LQfg7A5nk9A+wYbLNsaEjYomhVuBO4E/qep2EYkH/hW4sMLb8vTlRHmi6HNaH8CGyzbGhI8KJQVV3QDcCyAiLYAmqvp0IAMLZ8vTl9P3tL5EeaKCHYoxxlRKhRqaRSRRRJqKSEtgDTBNRF4IbGjhqaCwgBXpKxjUYVCwQzHGmEqraO+jZqp6FBgPTFPVgcDowIUVvjZlbOJY7jHObn92sEMxxphKq2hSqOc+++AXwNwAxhP2fBuZjTEm3FQ0KTyB81yEraq6XETOADYHLqzwtTx9OY2jGnMw66DdxWyMCTuiWvKxyaEtISFBk5OTgx1GqQa/MZjcglw2HdhkdzEbY0KGiKxQ1YTyylW0oTlWRGaJyD4R2SsiH4lI7KmHWbvkFuSyZs8amkQ1sbuYjTFhqaLVR9Nwnq/cHugAfOrOMz7W71tPTkEOF55xIVGeKDzisbuYjTFhpaI3r7VWVd8k8LaI3B+IgMLZit0rAJjQewKjzxhtdzEbY8JORZPCARG5EZjhTk8AMgITUvhKTk+mWXQzOrfoTJeWXSwZGGPCTkWrjybidEfdA+wGrsYZ+sL4WLF7BQPbD0REgh2KMcZUSYWSgqruVNWxqtpaVduo6lU4N7IZV25BLmv3riWhXbmN+8YYE7JO5clrD1RbFLXAD/t+ILcgl4HtBwY7FGOMqbJTSQpWR+IjOd25dyKhvV0pGGPC16kkhfC66y3AktOTaVG/BfHN44MdijHGVFmZvY9E5Bj+D/4CNAhIRGGqqJF5SdoS64pqjAlbZV4pqGoTVW3q518TVS23O6uIjBGRTSKyRUQeKaPc1SKiIhKWdS85+Tms27uOto3aMurdUTy28DFGvTvKxj0yxoSdU6k+KpOIeIBXgUuAHsAEEenhp1wTnAf4LA1ULIG2bt868grzKNRCG97CGBPWApYUgEHAFlXdpqq5wEzgSj/lngSeAbIDGEtAFTUyj+8+3oa3MMaEtYre0VwVHYBUn+k0YLBvARHpD3RU1bki8tvSViQik4BJAJ06dQpAqKdmSdoSWjdszfju42nfpL21KRhjwlYgk4K/LqveRmsRiQBeBG4pb0Wq+jrwOjhDZ1dTfNXm+9TvGdJxCCLCkI5DLBkYY8JWIKuP0oCOPtOxQLrPdBOgF5AoIinAOcCccGtsPpB5gM0HNzMk1hKBMSb8BTIpLAe6iki8iEQB1+EMvw2Aqh5R1VaqGqeqccASYKyqhu4TdPxYkrYEwJKCMaZWCFhSUNV84B6cx3huBD5Q1fUi8oSIjA3U59a0pNQkPOKxO5mNMbVCINsUUNV5wLwS8/5QStnhgYwlUJLSkujbti+NohoFOxRjjDllgaw+qvXyC/NZtmuZVR0ZY2oNSwqn4Id9P3Ai7wStG7bmqW+fsjuYjTFhL6DVR7VdURJ4evHT5BXkEeWJYsFNC6xLqjEmbNmVwilISkuicWRj8grybGgLY0ytYEnhFHyf+j0D2g+woS2MMbWGVR9VUUZmBlsPbeWOAXfw9KinbWgLY0ytYEmhilbsXgHA2R3OtqEtjDG1hlUfVdHyXcsBGNBuQJAjMcaY6mNJoYqSdyfTtWVXmtdvHuxQjDGm2lhSqKLk9GTO7nB2sMMwxphqZUmhCvYc30Pa0TQS2tl4R8aY2sWSQhUUPWnNBsEzxtQ2lhSqIDk9mQiJILcg14a3MMbUKtYltQqS05M5vdnpXDHjCnILcm14C2NMrWFXCpWkqiSnJ9OifgtyC3JteAtjTK1iSaGSdh3bxd4TexkeN9yGtzDG1DpWfVRJRTetXdPzGq7ucbUNb2GMqVUsKVTS8vTleMRD39P60iCygSUDY0ytYtVHlfTNjm8Y2H4gDSIbBDsUY4ypdpYUKiEzL5Nlu5Yx/PThwQ7FGGMCwpJCJSSlJpFXmMcFcRcEOxRjjAkISwqVsGjHIiIkgmGdhgU7FGOMCQhLCpWQmJLIgHYDaBrdNNihGGNMQFhSqKCsvCyW7lpq7QnGmFrNkkIFLUlbQm5BrrUnGGNqNUsKFZSYkkiERBDlibJB8IwxtVZAb14TkTHAS4AHeENVny6x/E7gbqAAOA5MUtUNgYypqhbtWESXll24auZVNgieMabWCtiVgoh4gFeBS4AewAQR6VGi2Puq2ltV+wHPAC8EKp5TkZ2fzZK0JbRp2MYGwTPG1GqBrD4aBGxR1W2qmgvMBK70LaCqR30mGwEawHiqLDElkZyCHK7qdpUNgmeMqdUCWX3UAUj1mU4DBpcsJCJ3Aw8AUcBIfysSkUnAJIBOnTpVe6DlmbNpDtGeaDLzMpk6ZioZmRk2CJ4xplYS1cCcnIvINcDFqnq7O/1LYJCq/rqU8te75W8ua70JCQmanJxc7fGWRlU57bnTyMjKQBBrSzDGhCURWaGq5T5DOJDVR2lAR5/pWCC9jPIzgasCGE+VrN6zmv2Z+1FVa0swxtR6gUwKy4GuIhIvIlHAdcAc3wIi0tVn8jJgcwDjqZI5m5yQo+tFW1uCMabWC1ibgqrmi8g9wOc4XVLfUtX1IvIEkKyqc4B7RGQ0kAccAsqsOgqGOT/N4dyO5/Lchc/ZA3WMMbVeQO9TUNV5wLwS8/7g8/q+QH7+qUo7msbK3St5atRTDOk4xJKBMabWszuayzD3p7kAjD1rbJAjMcaYmmFJoQwfrP+Azi06071V92CHYowxNcKSQik2HdjEwpSFTOw/EREJdjjGGFMjLCmU4vUVr1Mvoh4T+08MdijGGFNjLCn4kZWXxdtr3mZct3G0bdw22OEYY0yNsaTgx4cbPuRg1kHuTLgz2KEYY0yNsqTgx99X/J0zY85kRNyIYIdijDE1ypJCCev2ruP71O+5c+CdLElbYg/UMcbUKQG9eS0cvbHyDaI8UXRv1Z1R746yB+oYY+oUu1LwkZ2fzXtr32N89/Gs2rPKHqhjjKlz7ErBx6yNsziUfYjb+t9Go8hGRHmivFcKNgieMaYusKTg441VbxDXPI6R8SOJkAgW3LTABsEzxtQplhRcWw9u5evtX/PkiCeJEKdWzQbBM8bUNdam4Jq2ehoREsEt/W4JdijGGBM0dqWA88jNf639Fxd3vpjYprEkpSZZtZGplfLy8khLSyM7OzvYoZgAqV+/PrGxsURGRlbp/ZYUgJW7V7LjyA6mDJ9CUmqSdUU1tVZaWhpNmjQhLi7OBnqshVSVjIwM0tLSiI+Pr9I6rPoI+Hjjx3jEwxVnXkFiSqJ1RTW1VnZ2NjExMZYQaikRISYm5pSuBOt8UlBVPtr4EcPjhhPTMIbhccOJ8kTZ85hNrWUJoXY71e+3zlcfbTywkU0Zm7h38L2A0+PIuqIaY+qqOn+l8PHGjwGIbRrrHedoSMchTD5vsiUEY6pZRkYG/fr1o1+/frRt25YOHTp4p3Nzcyu0jltvvZVNmzaVWebVV19l+vTp1RFytXv00UeZOnXqSfNvvvlmWrduTb9+/YIQ1c/q/JXCxxs/pneb3lz34XXWuGxMgMXExLB69WoApkyZQuPGjfntb39brIyqoqpERPg/Z502bVq5n3P33XeferA1bOLEidx9991MmjQpqHHU6aSw7dA2Vu1ZxSVdLmHD/g3FGpctKZja7v7597N6z+pqXWe/tv2YOubks+DybNmyhauuuophw4axdOlS5s6dy//+7/+ycuVKsrKyuPbaa/nDH/4AwLBhw3jllVfo1asXrVq14s477+Szzz6jYcOGzJ49mzZt2vDoo4/SqlUr7r//foYNG8awYcP4+uuvOXLkCNOmTePcc8/lxIkT3HTTTWzZsoUePXqwefNm3njjjZPO1B9//HHmzZtHVlYWw4YN47XXXkNE+Omnn7jzzjvJyMjA4/Hw8ccfExcXx5///GdmzJhBREQEl19+OX/6058qtA8uuOACtmzZUul9V93qdPXR3J/mAnBrv1utcdmYINuwYQO33XYbq1atokOHDjz99NMkJyezZs0avvzySzZs2HDSe44cOcIFF1zAmjVrGDJkCG+99Zbfdasqy5Yt49lnn+WJJ54A4K9//Stt27ZlzZo1PPLII6xatcrve++77z6WL1/OunXrOHLkCPPnzwdgwoQJ/OY3v2HNmjV8//33tGnThk8//ZTPPvuMZcuWsWbNGh588MFq2js1p05fKczfMp8zY87kmp7XENs01hqXTZ1SlTP6QOrcuTNnn322d3rGjBm8+eab5Ofnk56ezoYNG+jRo0ex9zRo0IBLLrkEgIEDB/Ltt9/6Xff48eO9ZVJSUgD47rvvePjhhwHo27cvPXv29PveBQsW8Oyzz5Kdnc2BAwcYOHAg55xzDgcOHOCKK64AnBvGAL766ismTpxIgwYNAGjZsmVVdkVQ1dmkkJ2fTWJKIrcPuB2wcY6MCbZGjRp5X2/evJmXXnqJZcuW0bx5c2688Ua/fe+joqK8rz0eD/n5+X7XHR0dfVIZVS03pszMTO655x5WrlxJhw4dePTRR71x+Ov6qaph3+U3oNVHIjJGRDaJyBYRecTP8gdEZIOIrBWRBSJyeiDj8fXtjm/Jys9iTJcxNfWRxpgKOnr0KE2aNKFp06bs3r2bzz//vNo/Y9iwYXzwwQcArFu3zm/1VFZWFhEREbRq1Ypjx47x0UcfAdCiRQtatWrFp59+Cjg3BWZmZnLRRRfx5ptvkpWVBcDBgwerPe5AC1hSEBEP8CpwCdADmCAiPUoUWwUkqGof4EPgmUDFU9L8LfOpF1GP5buW2+M2jQkxAwYMoEePHvTq1Ys77riDoUOHVvtn/PrXv2bXrl306dOH559/nl69etGsWbNiZWJiYrj55pvp1asX48aNY/Dgwd5l06dP5/nnn6dPnz4MGzaM/fv3c/nllzNmzBgSEhLo168fL774ot/PnjJlCrGxscTGxhIXFwfANddcw3nnnceGDRuIjY3l7bffrvZtrgipyCVUlVYsMgSYoqoXu9OTAVT1qVLK9wdeUdUyv/2EhARNTk4+5fjOeOkMdhzZgSDWDdXUGRs3bqR79+7BDiMk5Ofnk5+fT/369dm8eTMXXXQRmzdvpl698K9V9/c9i8gKVU0o772B3PoOQKrPdBowuJSyALcBn/lbICKTgEkAnTp1OuXAUo+ksv3wdgShkELrhmpMHXT8+HFGjRpFfn4+qso//vGPWpEQTlUg94C/1ha/lyUiciOQAFzgb7mqvg68Ds6VwqkG9sXWLwCIrhdNXkGedUM1pg5q3rw5K1asCHYYISeQSSEN6OgzHQuklywkIqOB3wMXqGpOAOPxmr91Ph2adOCDqz9g0Y5F1g3VGGNcgUwKy4GuIhIP7AKuA673LeC2I/wDGKOq+wIYi1dBYQFfbfuK8d3Gc26nczm307k18bHGGBMWAtb7SFXzgXuAz4GNwAequl5EnhCRsW6xZ4HGwH9EZLWIzAlUPEXW7F3D4ezDjIwfGeiPMsaYsBPQVhVVnQfMKzHvDz6vRwfy8/1ZuH0hACPiR9T0RxtjTMirc2MfLUxZSMemHXln9Tt2f4IxNWz48OEn3Yg2depUfvWrX5X5vsaNGwOQnp7O1VdfXeq6y+uuPnXqVDIzM73Tl156KYcPH65I6DUqMTGRyy+//KT5r7zyCl26dEFEOHDgQEA+u04lhfzCfBZuX0j6sXQeW/gYo94dZYnBmHIkpSZ5nzVyqiZMmMDMmTOLzZs5cyYTJkyo0Pvbt2/Phx9+WOXPL5kU5s2bR/Pmzau8vpo2dOhQvvrqK04/PXCDP9SppLBy90oy8zNRVXsGszEVkJSaxKh3R1XbSdTVV1/N3LlzyclxOhqmpKSQnp7OsGHDvPcNDBgwgN69ezN79uyT3p+SkkKvXr0AZwiK6667jj59+nDttdd6h5YAuOuuu0hISKBnz548/vjjALz88sukp6czYsQIRoxwqo/j4uK8Z9wvvPACvXr1olevXt6H4KSkpNC9e3fuuOMOevbsyUUXXVTsc4p8+umnDB48mP79+zN69Gj27t0LOPdC3HrrrfTu3Zs+ffp4h8mYP38+AwYMoG/fvowaNarC+69///7eO6ADpU7dqVHUnhBVL8ruTzCmAhJTEsktyK22Z43ExMQwaNAg5s+fz5VXXsnMmTO59tprERHq16/PrFmzaNq0KQcOHOCcc85h7NixpQ4w99prr9GwYUPWrl3L2rVrGTBggHfZn/70J1q2bElBQQGjRo1i7dq13HvvvbzwwgssXLiQVq1aFVvXihUrmDZtGkuXLkVVGTx4MBdccAEtWrRg8+bNzJgxg3/+85/84he/4KOPPuLGG28s9v5hw4axZMkSRIQ33niDZ555hueff54nn3ySZs2asW7dOgAOHTrE/v37ueOOO/jmm2+Ij48PufGR6tSVQuKORHq07sHXN33NkyOetKEtjCnH8Ljh1f6sEd8qJN+qI1Xlf/7nf+jTpw+jR49m165d3jNuf7755hvvwblPnz706dPHu+yDDz5gwIAB9O/fn/Xr1/sd7M7Xd999x7hx42jUqBGNGzdm/Pjx3mG44+PjvQ/e8R1621daWhoXX3wxvXv35tlnn2X9+vWAM5S271PgWrRowZIlSzj//POJj48HQm947TpzpZBXkMe3O77lln632DDZxlTQkI5DWHDTgmp91shVV13FAw884H2qWtEZ/vTp09m/fz8rVqwgMjKSuLg4v8Nl+/J3FbF9+3aee+45li9fTosWLbjlllvKXU9ZY8AVDbsNztDb/qqPfv3rX/PAAw8wduxYEhMTmTJline9JWMM9eG168yVwrTV0ziRd4J2TdoFOxRjwsqQjkOYfN7kajuRaty4McOHD2fixInFGpiPHDlCmzZtiIyMZOHChezYsaPM9Zx//vlMnz4dgB9++IG1a9cCzrDbjRo1olmzZuzdu5fPPvt5SLUmTZpw7Ngxv+v65JNPyMzM5MSJE8yaNYvzzjuvwtt05MgROnToAMA777zjnX/RRRfxyiuveKcPHTrEkCFDWLRoEdu3bwdCb3jtOpEUklKTuGfePQD88Zs/Wo8jY4JswoQJrFmzhuuuu84774YbbiA5OZmEhASmT59Ot27dylzHXXfdxfHjx+nTpw/PPPMMgwYNApynqPXv35+ePXsyceLEYsNuT5o0iUsuucTb0FxkwIAB3HLLLQwaNIjBgwdz++23079//wpvz5QpU7xDX/u2Vzz66KMcOnSIXr160bdvXxYuXEjr1q15/fXXGT9+PH379uXaa6/1u84FCxZ4h9eOjY0lKSmJl19+mdjYWNLS0ujTpw+33357hWOsqIANnR0oVRk6+6lvn+KxhY9RoAV4xMOTI55k8nmTAxShMaHLhs6uG05l6Ow6caUQiMYyY4ypjepEQ3MgGsuMMaY2qhNJAbAeR8a4Qr33izk1p9okUCeqj4wxjvr165ORkXHKBw4TmlSVjIwM6tevX+V11JkrBWMM3p4r+/fvD3YoJkDq169PbGxsld9vScGYOiQyMtJ7J60x/lj1kTHGGC9LCsYYY7wsKRhjjPEKuzuaRWQ/UPagKCdrBQTmMUU1z7YlNNm2hK7atD2nsi2nq2rr8gqFXVKoChFJrsjt3eHAtiU02baErtq0PTWxLVZ9ZIwxxsuSgjHGGK+6khReD3YA1ci2JTTZtoSu2rQ9Ad+WOtGmYIwxpmLqypWCMcaYCrCkYIwxxqtWJwURGSMim0Rki4g8Eux4KkNEOorIQhHZKCLrReQ+d35LEflSRDa7/7cIdqwVJSIeEVklInPd6XgRWepuy79FJCrYMVaUiDQXkQ9F5Ef3OxoSrt+NiPzG/Y39ICIzRKR+uHw3IvKWiOwTkR985vn9HsTxsns8WCsiA4IX+clK2ZZn3d/YWhGZJSLNfZZNdrdlk4hcXF1x1NqkICIe4FXgEqAHMEFEegQ3qkrJBx5U1e7AOcDdbvyPAAtUtSuwwJ0OF/cBG32m/wK86G7LIeC2oERVNS8B81W1G9AXZ7vC7rsRkQ7AvUCCqvYCPMB1hM938zYwpsS80r6HS4Cu7r9JwGs1FGNFvc3J2/Il0EtV+wA/AZMB3GPBdUBP9z1/c495p6zWJgVgELBFVbepai4wE7gyyDFVmKruVtWV7utjOAedDjjb8I5b7B3gquBEWDkiEgtcBrzhTgswEvjQLRJO29IUOB94E0BVc1X1MGH63eCMltxAROoBDYHdhMl3o6rfAAdLzC7te7gSeFcdS4DmItKuZiItn79tUdUvVDXfnVwCFI2JfSUwU1VzVHU7sAXnmHfKanNS6ACk+kynufPCjojEAf2BpcBpqrobnMQBtAleZJUyFXgIKHSnY4DDPj/4cPp+zgD2A9Pc6rA3RKQRYfjdqOou4DlgJ04yOAKsIHy/Gyj9ewj3Y8JE4DP3dcC2pTYnBX/PGwy7/rci0hj4CLhfVY8GO56qEJHLgX2qusJ3tp+i4fL91AMGAK+pan/gBGFQVeSPW99+JRAPtAca4VSzlBQu301ZwvY3JyK/x6lSnl40y0+xatmW2pwU0oCOPtOxQHqQYqkSEYnESQjTVfVjd/beokte9/99wYqvEoYCY0UkBacabyTOlUNzt8oCwuv7SQPSVHWpO/0hTpIIx+9mNLBdVferah7wMXAu4fvdQOnfQ1geE0TkZuBy4Ab9+caygG1LbU4Ky4Gubi+KKJxGmTlBjqnC3Dr3N4GNqvqCz6I5wM3u65uB2TUdW2Wp6mRVjVXVOJzv4WtVvQFYCFztFguLbQFQ1T1Aqoic5c4aBWwgDL8bnGqjc0SkofubK9qWsPxuXKV9D3OAm9xeSOcAR4qqmUKViIwBHgbGqmqmz6I5wHUiEi0i8TiN58uq5UNVtdb+Ay7FabHfCvw+2PFUMvZhOJeDa4HV7r9LceriFwCb3f9bBjvWSm7XcGCu+/oM94e8BfgPEB3s+CqxHf2AZPf7+QRoEa7fDfC/wI/AD8B7QHS4fDfADJy2kDycs+fbSvsecKpcXnWPB+twelwFfRvK2ZYtOG0HRceAv/uU/727LZuAS6orDhvmwhhjjFdtrj4yxhhTSZYUjDHGeFlSMMYY42VJwRhjjJclBWOMMV6WFIxxiUiBiKz2+VdtdymLSJzv6JfGhKp65Rcxps7IUtV+wQ7CmGCyKwVjyiEiKSLyFxFZ5v7r4s4/XUQWuGPdLxCRTu7809yx79e4/851V+URkX+6zy74QkQauOXvFZEN7npmBmkzjQEsKRjjq0GJ6qNrfZYdVdVBwCs44zbhvn5XnbHupwMvu/NfBhapal+cMZHWu/O7Aq+qak/gMPBf7vxHgP7ueu4M1MYZUxF2R7MxLhE5rqqN/cxPAUaq6jZ3kMI9qhojIgeAdqqa587fraqtRGQ/EKuqOT7riAO+VOfBL4jIw0Ckqv5RROYDx3GGy/hEVY8HeFONKZVdKRhTMVrK69LK+JPj87qAn9v0LsMZk2cgsMJndFJjapwlBWMq5lqf/5Pc19/jjPoKcAPwnft6AXAXeJ9L3bS0lYpIBNBRVRfiPISoOXDS1YoxNcXOSIz5WQMRWe0zPV9Vi7qlRovIUpwTqQnuvHuBt0TkdzhPYrvVnX8f8LqI3IZzRXAXzuiX/niAf4lIM5xRPF9U59GexgSFtSkYUw63TSFBVQ8EOxZjAs2qj4wxxnjZlYIxxhgvu1IwxhjjZUnBGGOMlyUFY4wxXpYUjDHGeFlSMMYY4/X/hXRX7JqLfEYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 16.0708 - acc: 0.1621 - val_loss: 15.6637 - val_acc: 0.1660\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 15.3031 - acc: 0.1965 - val_loss: 14.9173 - val_acc: 0.1860\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 14.5653 - acc: 0.2124 - val_loss: 14.1949 - val_acc: 0.1960\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 13.8503 - acc: 0.2167 - val_loss: 13.4937 - val_acc: 0.2060\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 13.1559 - acc: 0.2303 - val_loss: 12.8120 - val_acc: 0.2090\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 12.4803 - acc: 0.2391 - val_loss: 12.1484 - val_acc: 0.2190\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 11.8241 - acc: 0.2603 - val_loss: 11.5048 - val_acc: 0.2270\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 11.1880 - acc: 0.2751 - val_loss: 10.8801 - val_acc: 0.2570\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 10.5715 - acc: 0.2911 - val_loss: 10.2746 - val_acc: 0.2820\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 9.9742 - acc: 0.3183 - val_loss: 9.6887 - val_acc: 0.3150\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 9.3970 - acc: 0.3503 - val_loss: 9.1235 - val_acc: 0.3370\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 8.8405 - acc: 0.3803 - val_loss: 8.5791 - val_acc: 0.3670\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 8.3048 - acc: 0.4145 - val_loss: 8.0552 - val_acc: 0.4110\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 7.7900 - acc: 0.4551 - val_loss: 7.5540 - val_acc: 0.4340\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 7.2973 - acc: 0.4784 - val_loss: 7.0737 - val_acc: 0.4740\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 6.8267 - acc: 0.5129 - val_loss: 6.6149 - val_acc: 0.5040\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 6.3784 - acc: 0.5381 - val_loss: 6.1798 - val_acc: 0.5310\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 5.9532 - acc: 0.5584 - val_loss: 5.7671 - val_acc: 0.5700\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 5.5516 - acc: 0.5819 - val_loss: 5.3788 - val_acc: 0.5710\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 5.1732 - acc: 0.6012 - val_loss: 5.0130 - val_acc: 0.5800\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 4.8184 - acc: 0.6131 - val_loss: 4.6706 - val_acc: 0.6020\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 4.4861 - acc: 0.6295 - val_loss: 4.3500 - val_acc: 0.6330\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 4.1764 - acc: 0.6443 - val_loss: 4.0522 - val_acc: 0.6280\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 3.8884 - acc: 0.6547 - val_loss: 3.7760 - val_acc: 0.6310\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 3.6229 - acc: 0.6643 - val_loss: 3.5232 - val_acc: 0.6380\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 3.3796 - acc: 0.6663 - val_loss: 3.2909 - val_acc: 0.6570\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 3.1574 - acc: 0.6759 - val_loss: 3.0789 - val_acc: 0.6590\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 2.9574 - acc: 0.6784 - val_loss: 2.8930 - val_acc: 0.6510\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 2.7792 - acc: 0.6801 - val_loss: 2.7244 - val_acc: 0.6600\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.6223 - acc: 0.6833 - val_loss: 2.5789 - val_acc: 0.6770\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.4866 - acc: 0.6875 - val_loss: 2.4527 - val_acc: 0.6830\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 2.3718 - acc: 0.6887 - val_loss: 2.3484 - val_acc: 0.6730\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.2763 - acc: 0.6881 - val_loss: 2.2596 - val_acc: 0.6780\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.1994 - acc: 0.6892 - val_loss: 2.1916 - val_acc: 0.6840\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 2.1395 - acc: 0.6907 - val_loss: 2.1388 - val_acc: 0.6910\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.0946 - acc: 0.6912 - val_loss: 2.1018 - val_acc: 0.6830\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.0613 - acc: 0.6913 - val_loss: 2.0720 - val_acc: 0.6860\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.0345 - acc: 0.6909 - val_loss: 2.0447 - val_acc: 0.6830\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.0110 - acc: 0.6905 - val_loss: 2.0240 - val_acc: 0.6770\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9895 - acc: 0.6931 - val_loss: 2.0046 - val_acc: 0.6750\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9703 - acc: 0.6908 - val_loss: 1.9829 - val_acc: 0.6990\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.9517 - acc: 0.6948 - val_loss: 1.9643 - val_acc: 0.6930\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9345 - acc: 0.6940 - val_loss: 1.9473 - val_acc: 0.6930\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.9178 - acc: 0.6957 - val_loss: 1.9308 - val_acc: 0.6900\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.9019 - acc: 0.6936 - val_loss: 1.9184 - val_acc: 0.6880\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8872 - acc: 0.6944 - val_loss: 1.8984 - val_acc: 0.6960\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.8725 - acc: 0.6956 - val_loss: 1.8846 - val_acc: 0.6960\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8586 - acc: 0.6961 - val_loss: 1.8728 - val_acc: 0.6980\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8448 - acc: 0.6971 - val_loss: 1.8590 - val_acc: 0.7000\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8321 - acc: 0.6972 - val_loss: 1.8455 - val_acc: 0.6920\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8193 - acc: 0.6968 - val_loss: 1.8316 - val_acc: 0.7010\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8066 - acc: 0.6971 - val_loss: 1.8209 - val_acc: 0.6980\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.7948 - acc: 0.6992 - val_loss: 1.8092 - val_acc: 0.7010\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.7829 - acc: 0.7007 - val_loss: 1.7972 - val_acc: 0.7000\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.7713 - acc: 0.7025 - val_loss: 1.7847 - val_acc: 0.7010\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.7605 - acc: 0.7012 - val_loss: 1.7722 - val_acc: 0.6960\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.7492 - acc: 0.7036 - val_loss: 1.7616 - val_acc: 0.7010\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.7384 - acc: 0.7037 - val_loss: 1.7521 - val_acc: 0.7070\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.7280 - acc: 0.7033 - val_loss: 1.7427 - val_acc: 0.7040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.7176 - acc: 0.7029 - val_loss: 1.7308 - val_acc: 0.7080\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.7075 - acc: 0.7045 - val_loss: 1.7204 - val_acc: 0.7050\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6979 - acc: 0.7044 - val_loss: 1.7109 - val_acc: 0.7130\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6881 - acc: 0.7056 - val_loss: 1.7012 - val_acc: 0.7070\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6783 - acc: 0.7079 - val_loss: 1.6920 - val_acc: 0.7080\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6693 - acc: 0.7072 - val_loss: 1.6806 - val_acc: 0.7110\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6600 - acc: 0.7076 - val_loss: 1.6762 - val_acc: 0.7120\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6511 - acc: 0.7076 - val_loss: 1.6631 - val_acc: 0.7090\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6425 - acc: 0.7085 - val_loss: 1.6544 - val_acc: 0.7040\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6334 - acc: 0.7088 - val_loss: 1.6479 - val_acc: 0.7080\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.6251 - acc: 0.7101 - val_loss: 1.6387 - val_acc: 0.7050\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6165 - acc: 0.7092 - val_loss: 1.6286 - val_acc: 0.7090\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6081 - acc: 0.7092 - val_loss: 1.6217 - val_acc: 0.7140\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6002 - acc: 0.7117 - val_loss: 1.6118 - val_acc: 0.7140\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5919 - acc: 0.7107 - val_loss: 1.6059 - val_acc: 0.7110\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5839 - acc: 0.7127 - val_loss: 1.5980 - val_acc: 0.7090\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5762 - acc: 0.7113 - val_loss: 1.5874 - val_acc: 0.7140\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5676 - acc: 0.7125 - val_loss: 1.5834 - val_acc: 0.7110\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5603 - acc: 0.7124 - val_loss: 1.5759 - val_acc: 0.7100\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5532 - acc: 0.7132 - val_loss: 1.5637 - val_acc: 0.7170\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5456 - acc: 0.7144 - val_loss: 1.5590 - val_acc: 0.7140\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5383 - acc: 0.7163 - val_loss: 1.5525 - val_acc: 0.7170\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5307 - acc: 0.7147 - val_loss: 1.5429 - val_acc: 0.7100\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5231 - acc: 0.7172 - val_loss: 1.5348 - val_acc: 0.7130\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5157 - acc: 0.7156 - val_loss: 1.5316 - val_acc: 0.7130\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5095 - acc: 0.7171 - val_loss: 1.5269 - val_acc: 0.7050\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.5030 - acc: 0.7180 - val_loss: 1.5147 - val_acc: 0.7120\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4957 - acc: 0.7187 - val_loss: 1.5092 - val_acc: 0.7130\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4890 - acc: 0.7193 - val_loss: 1.5029 - val_acc: 0.7120\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4821 - acc: 0.7189 - val_loss: 1.4980 - val_acc: 0.7130\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.4755 - acc: 0.7205 - val_loss: 1.4902 - val_acc: 0.7110\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4695 - acc: 0.7184 - val_loss: 1.4822 - val_acc: 0.7130\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4630 - acc: 0.7197 - val_loss: 1.4777 - val_acc: 0.7100\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4570 - acc: 0.7211 - val_loss: 1.4689 - val_acc: 0.7180\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4504 - acc: 0.7227 - val_loss: 1.4670 - val_acc: 0.7150\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.4447 - acc: 0.7200 - val_loss: 1.4587 - val_acc: 0.7130\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4384 - acc: 0.7224 - val_loss: 1.4527 - val_acc: 0.7140\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4328 - acc: 0.7193 - val_loss: 1.4477 - val_acc: 0.7210\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4268 - acc: 0.7216 - val_loss: 1.4404 - val_acc: 0.7150\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4210 - acc: 0.7232 - val_loss: 1.4367 - val_acc: 0.7140\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.4150 - acc: 0.7240 - val_loss: 1.4292 - val_acc: 0.7150\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4091 - acc: 0.7239 - val_loss: 1.4230 - val_acc: 0.7140\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4037 - acc: 0.7240 - val_loss: 1.4199 - val_acc: 0.7150\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3978 - acc: 0.7247 - val_loss: 1.4138 - val_acc: 0.7190\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3927 - acc: 0.7229 - val_loss: 1.4078 - val_acc: 0.7210\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3871 - acc: 0.7236 - val_loss: 1.4030 - val_acc: 0.7170\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3813 - acc: 0.7261 - val_loss: 1.3976 - val_acc: 0.7130\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3764 - acc: 0.7265 - val_loss: 1.3903 - val_acc: 0.7150\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3714 - acc: 0.7249 - val_loss: 1.3854 - val_acc: 0.7150\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3657 - acc: 0.7269 - val_loss: 1.3799 - val_acc: 0.7160\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3604 - acc: 0.7273 - val_loss: 1.3754 - val_acc: 0.7170\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3551 - acc: 0.7261 - val_loss: 1.3697 - val_acc: 0.7180\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3503 - acc: 0.7280 - val_loss: 1.3650 - val_acc: 0.7130\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3452 - acc: 0.7267 - val_loss: 1.3654 - val_acc: 0.7110\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3405 - acc: 0.7277 - val_loss: 1.3565 - val_acc: 0.7180\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3354 - acc: 0.7279 - val_loss: 1.3504 - val_acc: 0.7160\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.3306 - acc: 0.7277 - val_loss: 1.3457 - val_acc: 0.7180\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3258 - acc: 0.7272 - val_loss: 1.3425 - val_acc: 0.7220\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3206 - acc: 0.7281 - val_loss: 1.3373 - val_acc: 0.7170\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3171 - acc: 0.7283 - val_loss: 1.3349 - val_acc: 0.7210\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.3116 - acc: 0.7291 - val_loss: 1.3292 - val_acc: 0.7150\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3072 - acc: 0.7304 - val_loss: 1.3249 - val_acc: 0.7100\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.3030 - acc: 0.7287 - val_loss: 1.3182 - val_acc: 0.7160\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.2973 - acc: 0.7312 - val_loss: 1.3143 - val_acc: 0.7220\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.2934 - acc: 0.7315 - val_loss: 1.3111 - val_acc: 0.7180\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.2896 - acc: 0.7301 - val_loss: 1.3077 - val_acc: 0.7140\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.2852 - acc: 0.7316 - val_loss: 1.3016 - val_acc: 0.7220\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2806 - acc: 0.7329 - val_loss: 1.2985 - val_acc: 0.7190\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.2765 - acc: 0.7320 - val_loss: 1.2929 - val_acc: 0.7140\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2721 - acc: 0.7324 - val_loss: 1.2895 - val_acc: 0.7160\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2680 - acc: 0.7323 - val_loss: 1.2883 - val_acc: 0.7240\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2639 - acc: 0.7336 - val_loss: 1.2822 - val_acc: 0.7230\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2604 - acc: 0.7329 - val_loss: 1.2775 - val_acc: 0.7220\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2566 - acc: 0.7335 - val_loss: 1.2736 - val_acc: 0.7190\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2517 - acc: 0.7335 - val_loss: 1.2736 - val_acc: 0.7230\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2487 - acc: 0.7332 - val_loss: 1.2709 - val_acc: 0.7230\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2442 - acc: 0.7352 - val_loss: 1.2630 - val_acc: 0.7180\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2407 - acc: 0.7336 - val_loss: 1.2588 - val_acc: 0.7160\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2372 - acc: 0.7339 - val_loss: 1.2534 - val_acc: 0.7210\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2328 - acc: 0.7368 - val_loss: 1.2530 - val_acc: 0.7250\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2298 - acc: 0.7344 - val_loss: 1.2481 - val_acc: 0.7190\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2253 - acc: 0.7357 - val_loss: 1.2466 - val_acc: 0.7190\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2230 - acc: 0.7353 - val_loss: 1.2418 - val_acc: 0.7220\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.2183 - acc: 0.7369 - val_loss: 1.2402 - val_acc: 0.7190\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2157 - acc: 0.7355 - val_loss: 1.2344 - val_acc: 0.7250\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2119 - acc: 0.7376 - val_loss: 1.2337 - val_acc: 0.7190\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2087 - acc: 0.7381 - val_loss: 1.2269 - val_acc: 0.7200\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2056 - acc: 0.7375 - val_loss: 1.2231 - val_acc: 0.7170\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2016 - acc: 0.7369 - val_loss: 1.2224 - val_acc: 0.7180\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1987 - acc: 0.7384 - val_loss: 1.2191 - val_acc: 0.7190\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1959 - acc: 0.7384 - val_loss: 1.2151 - val_acc: 0.7240\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1928 - acc: 0.7373 - val_loss: 1.2122 - val_acc: 0.7250\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1893 - acc: 0.7371 - val_loss: 1.2103 - val_acc: 0.7200\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1862 - acc: 0.7387 - val_loss: 1.2099 - val_acc: 0.7270\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1840 - acc: 0.7377 - val_loss: 1.2043 - val_acc: 0.7190\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1804 - acc: 0.7399 - val_loss: 1.2055 - val_acc: 0.7210\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1777 - acc: 0.7383 - val_loss: 1.2016 - val_acc: 0.7220\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1744 - acc: 0.7380 - val_loss: 1.1956 - val_acc: 0.7220\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1723 - acc: 0.7389 - val_loss: 1.1922 - val_acc: 0.7260\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1690 - acc: 0.7399 - val_loss: 1.1890 - val_acc: 0.7250\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1658 - acc: 0.7396 - val_loss: 1.1884 - val_acc: 0.7220\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1636 - acc: 0.7405 - val_loss: 1.1858 - val_acc: 0.7240\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.1605 - acc: 0.7403 - val_loss: 1.1832 - val_acc: 0.7240\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1584 - acc: 0.7391 - val_loss: 1.1814 - val_acc: 0.7220\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1565 - acc: 0.7421 - val_loss: 1.1807 - val_acc: 0.7280\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.1531 - acc: 0.7421 - val_loss: 1.1738 - val_acc: 0.7190\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.1509 - acc: 0.7416 - val_loss: 1.1720 - val_acc: 0.7220\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.1479 - acc: 0.7421 - val_loss: 1.1727 - val_acc: 0.7170\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1461 - acc: 0.7413 - val_loss: 1.1696 - val_acc: 0.7240\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1439 - acc: 0.7415 - val_loss: 1.1685 - val_acc: 0.7270\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1420 - acc: 0.7416 - val_loss: 1.1638 - val_acc: 0.7170\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1393 - acc: 0.7424 - val_loss: 1.1653 - val_acc: 0.7200\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1370 - acc: 0.7427 - val_loss: 1.1583 - val_acc: 0.7230\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.1346 - acc: 0.7435 - val_loss: 1.1607 - val_acc: 0.7230\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1329 - acc: 0.7413 - val_loss: 1.1560 - val_acc: 0.7240\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1308 - acc: 0.7415 - val_loss: 1.1574 - val_acc: 0.7230\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1293 - acc: 0.7413 - val_loss: 1.1544 - val_acc: 0.7240\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1271 - acc: 0.7431 - val_loss: 1.1589 - val_acc: 0.7200\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1259 - acc: 0.7427 - val_loss: 1.1483 - val_acc: 0.7150\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1237 - acc: 0.7441 - val_loss: 1.1463 - val_acc: 0.7220\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1211 - acc: 0.7452 - val_loss: 1.1515 - val_acc: 0.7180\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1199 - acc: 0.7456 - val_loss: 1.1434 - val_acc: 0.7250\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1176 - acc: 0.7441 - val_loss: 1.1457 - val_acc: 0.7180\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1163 - acc: 0.7432 - val_loss: 1.1437 - val_acc: 0.7230\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1141 - acc: 0.7440 - val_loss: 1.1377 - val_acc: 0.7270\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1124 - acc: 0.7445 - val_loss: 1.1367 - val_acc: 0.7230\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1107 - acc: 0.7441 - val_loss: 1.1412 - val_acc: 0.7190\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1097 - acc: 0.7453 - val_loss: 1.1358 - val_acc: 0.7260\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1083 - acc: 0.7456 - val_loss: 1.1323 - val_acc: 0.7230\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1068 - acc: 0.7453 - val_loss: 1.1335 - val_acc: 0.7240\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1045 - acc: 0.7447 - val_loss: 1.1355 - val_acc: 0.7260\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1039 - acc: 0.7461 - val_loss: 1.1331 - val_acc: 0.7210\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1020 - acc: 0.7455 - val_loss: 1.1299 - val_acc: 0.7260\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1011 - acc: 0.7459 - val_loss: 1.1288 - val_acc: 0.7250\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0989 - acc: 0.7472 - val_loss: 1.1259 - val_acc: 0.7210\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0978 - acc: 0.7455 - val_loss: 1.1250 - val_acc: 0.7200\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0961 - acc: 0.7481 - val_loss: 1.1220 - val_acc: 0.7250\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0950 - acc: 0.7483 - val_loss: 1.1221 - val_acc: 0.7210\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0941 - acc: 0.7467 - val_loss: 1.1185 - val_acc: 0.7250\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0925 - acc: 0.7489 - val_loss: 1.1224 - val_acc: 0.7240\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0907 - acc: 0.7475 - val_loss: 1.1166 - val_acc: 0.7270\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0898 - acc: 0.7460 - val_loss: 1.1145 - val_acc: 0.7280\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0876 - acc: 0.7473 - val_loss: 1.1287 - val_acc: 0.7190\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0876 - acc: 0.7484 - val_loss: 1.1187 - val_acc: 0.7240\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0860 - acc: 0.7492 - val_loss: 1.1148 - val_acc: 0.7280\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0848 - acc: 0.7469 - val_loss: 1.1120 - val_acc: 0.7300\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0831 - acc: 0.7496 - val_loss: 1.1093 - val_acc: 0.7240\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0815 - acc: 0.7493 - val_loss: 1.1159 - val_acc: 0.7300\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0813 - acc: 0.7492 - val_loss: 1.1073 - val_acc: 0.7240\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0797 - acc: 0.7477 - val_loss: 1.1096 - val_acc: 0.7270\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0779 - acc: 0.7492 - val_loss: 1.1051 - val_acc: 0.7290\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0766 - acc: 0.7483 - val_loss: 1.1065 - val_acc: 0.7260\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0763 - acc: 0.7492 - val_loss: 1.1043 - val_acc: 0.7260\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0747 - acc: 0.7491 - val_loss: 1.1042 - val_acc: 0.7250\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0734 - acc: 0.7499 - val_loss: 1.1020 - val_acc: 0.7250\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0722 - acc: 0.7493 - val_loss: 1.1012 - val_acc: 0.7260\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0711 - acc: 0.7500 - val_loss: 1.1108 - val_acc: 0.7210\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0704 - acc: 0.7508 - val_loss: 1.0968 - val_acc: 0.7290\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0687 - acc: 0.7524 - val_loss: 1.0967 - val_acc: 0.7250\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0680 - acc: 0.7499 - val_loss: 1.0997 - val_acc: 0.7210\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0665 - acc: 0.7523 - val_loss: 1.1045 - val_acc: 0.7220\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0653 - acc: 0.7508 - val_loss: 1.0946 - val_acc: 0.7330\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0640 - acc: 0.7503 - val_loss: 1.0936 - val_acc: 0.7250\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0634 - acc: 0.7507 - val_loss: 1.0956 - val_acc: 0.7300\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0628 - acc: 0.7513 - val_loss: 1.0904 - val_acc: 0.7290\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0607 - acc: 0.7515 - val_loss: 1.0938 - val_acc: 0.7270\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0598 - acc: 0.7525 - val_loss: 1.0890 - val_acc: 0.7260\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0586 - acc: 0.7512 - val_loss: 1.0898 - val_acc: 0.7260\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0578 - acc: 0.7535 - val_loss: 1.0868 - val_acc: 0.7300\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0565 - acc: 0.7520 - val_loss: 1.0855 - val_acc: 0.7280\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0550 - acc: 0.7527 - val_loss: 1.0857 - val_acc: 0.7310\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0551 - acc: 0.7516 - val_loss: 1.0835 - val_acc: 0.7260\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0535 - acc: 0.7527 - val_loss: 1.0826 - val_acc: 0.7300\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0524 - acc: 0.7516 - val_loss: 1.0876 - val_acc: 0.7270\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0518 - acc: 0.7521 - val_loss: 1.0799 - val_acc: 0.7280\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0505 - acc: 0.7531 - val_loss: 1.0809 - val_acc: 0.7280\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0494 - acc: 0.7525 - val_loss: 1.0794 - val_acc: 0.7300\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0483 - acc: 0.7524 - val_loss: 1.0804 - val_acc: 0.7320\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0475 - acc: 0.7543 - val_loss: 1.0764 - val_acc: 0.7270\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0464 - acc: 0.7543 - val_loss: 1.0774 - val_acc: 0.7300\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0455 - acc: 0.7527 - val_loss: 1.0855 - val_acc: 0.7320\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0456 - acc: 0.7516 - val_loss: 1.0767 - val_acc: 0.7320\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0437 - acc: 0.7511 - val_loss: 1.0759 - val_acc: 0.7310\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0430 - acc: 0.7540 - val_loss: 1.0733 - val_acc: 0.7330\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0417 - acc: 0.7541 - val_loss: 1.0803 - val_acc: 0.7260\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0413 - acc: 0.7544 - val_loss: 1.0726 - val_acc: 0.7300\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0402 - acc: 0.7535 - val_loss: 1.0714 - val_acc: 0.7280\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0386 - acc: 0.7552 - val_loss: 1.0718 - val_acc: 0.7300\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0382 - acc: 0.7543 - val_loss: 1.0705 - val_acc: 0.7300\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0372 - acc: 0.7545 - val_loss: 1.0694 - val_acc: 0.7350\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0356 - acc: 0.7547 - val_loss: 1.0735 - val_acc: 0.7290\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0359 - acc: 0.7556 - val_loss: 1.0686 - val_acc: 0.7290\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0342 - acc: 0.7531 - val_loss: 1.0688 - val_acc: 0.7300\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0332 - acc: 0.7556 - val_loss: 1.0644 - val_acc: 0.7310\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0327 - acc: 0.7552 - val_loss: 1.0656 - val_acc: 0.7320\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0320 - acc: 0.7559 - val_loss: 1.0643 - val_acc: 0.7320\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0309 - acc: 0.7556 - val_loss: 1.0745 - val_acc: 0.7290\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0307 - acc: 0.7555 - val_loss: 1.0637 - val_acc: 0.7280\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0296 - acc: 0.7556 - val_loss: 1.0638 - val_acc: 0.7320\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0289 - acc: 0.7552 - val_loss: 1.0616 - val_acc: 0.7320\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0281 - acc: 0.7565 - val_loss: 1.0616 - val_acc: 0.7340\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0273 - acc: 0.7561 - val_loss: 1.0607 - val_acc: 0.7370\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0257 - acc: 0.7575 - val_loss: 1.0606 - val_acc: 0.7300\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0258 - acc: 0.7548 - val_loss: 1.0563 - val_acc: 0.7320\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0240 - acc: 0.7559 - val_loss: 1.0624 - val_acc: 0.7270\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0243 - acc: 0.7559 - val_loss: 1.0556 - val_acc: 0.7270\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0224 - acc: 0.7569 - val_loss: 1.0562 - val_acc: 0.7310\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0221 - acc: 0.7565 - val_loss: 1.0571 - val_acc: 0.7290\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0215 - acc: 0.7549 - val_loss: 1.0551 - val_acc: 0.7360\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0200 - acc: 0.7559 - val_loss: 1.0541 - val_acc: 0.7320\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0200 - acc: 0.7571 - val_loss: 1.0524 - val_acc: 0.7390\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0201 - acc: 0.7573 - val_loss: 1.0554 - val_acc: 0.7320\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0186 - acc: 0.7563 - val_loss: 1.0547 - val_acc: 0.7310\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0176 - acc: 0.7567 - val_loss: 1.0533 - val_acc: 0.7340\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0164 - acc: 0.7556 - val_loss: 1.0575 - val_acc: 0.7340\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0165 - acc: 0.7571 - val_loss: 1.0550 - val_acc: 0.7320\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0165 - acc: 0.7573 - val_loss: 1.0554 - val_acc: 0.7320\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0151 - acc: 0.7576 - val_loss: 1.0505 - val_acc: 0.7330\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0147 - acc: 0.7581 - val_loss: 1.0478 - val_acc: 0.7370\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0137 - acc: 0.7588 - val_loss: 1.0522 - val_acc: 0.7350\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0132 - acc: 0.7552 - val_loss: 1.0490 - val_acc: 0.7360\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0119 - acc: 0.7580 - val_loss: 1.0531 - val_acc: 0.7300\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0111 - acc: 0.7576 - val_loss: 1.0468 - val_acc: 0.7310\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0101 - acc: 0.7583 - val_loss: 1.0539 - val_acc: 0.7270\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0106 - acc: 0.7608 - val_loss: 1.0499 - val_acc: 0.7310\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0093 - acc: 0.7567 - val_loss: 1.0427 - val_acc: 0.7350\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0085 - acc: 0.7577 - val_loss: 1.0525 - val_acc: 0.7350\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0085 - acc: 0.7599 - val_loss: 1.0452 - val_acc: 0.7400\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0070 - acc: 0.7597 - val_loss: 1.0432 - val_acc: 0.7360\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0071 - acc: 0.7587 - val_loss: 1.0423 - val_acc: 0.7390\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0060 - acc: 0.7587 - val_loss: 1.0417 - val_acc: 0.7320\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0044 - acc: 0.7592 - val_loss: 1.0416 - val_acc: 0.7340\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0050 - acc: 0.7591 - val_loss: 1.0444 - val_acc: 0.7370\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0046 - acc: 0.7596 - val_loss: 1.0439 - val_acc: 0.7320\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0035 - acc: 0.7604 - val_loss: 1.0373 - val_acc: 0.7370\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0027 - acc: 0.7581 - val_loss: 1.0420 - val_acc: 0.7330\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0017 - acc: 0.7583 - val_loss: 1.0400 - val_acc: 0.7350\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0021 - acc: 0.7588 - val_loss: 1.0387 - val_acc: 0.7350\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0005 - acc: 0.7588 - val_loss: 1.0434 - val_acc: 0.7380\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0005 - acc: 0.7581 - val_loss: 1.0400 - val_acc: 0.7340\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9995 - acc: 0.7579 - val_loss: 1.0365 - val_acc: 0.7440\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9989 - acc: 0.7608 - val_loss: 1.0377 - val_acc: 0.7350\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9975 - acc: 0.762 - 0s 48us/step - loss: 0.9977 - acc: 0.7613 - val_loss: 1.0341 - val_acc: 0.7350\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9983 - acc: 0.7579 - val_loss: 1.0378 - val_acc: 0.7350\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9970 - acc: 0.7595 - val_loss: 1.0354 - val_acc: 0.7390\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9959 - acc: 0.7603 - val_loss: 1.0323 - val_acc: 0.7400\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9953 - acc: 0.7581 - val_loss: 1.0350 - val_acc: 0.7360\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9961 - acc: 0.7595 - val_loss: 1.0349 - val_acc: 0.7380\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9944 - acc: 0.7599 - val_loss: 1.0352 - val_acc: 0.7430\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9939 - acc: 0.7591 - val_loss: 1.0298 - val_acc: 0.7410\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9926 - acc: 0.7592 - val_loss: 1.0329 - val_acc: 0.7340\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9929 - acc: 0.7593 - val_loss: 1.0286 - val_acc: 0.7390\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9922 - acc: 0.7588 - val_loss: 1.0293 - val_acc: 0.7350\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9918 - acc: 0.7593 - val_loss: 1.0356 - val_acc: 0.7390\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9916 - acc: 0.7607 - val_loss: 1.0261 - val_acc: 0.7390\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9905 - acc: 0.7613 - val_loss: 1.0345 - val_acc: 0.7340\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9901 - acc: 0.7616 - val_loss: 1.0291 - val_acc: 0.7450\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9899 - acc: 0.7613 - val_loss: 1.0302 - val_acc: 0.7350\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9885 - acc: 0.7593 - val_loss: 1.0269 - val_acc: 0.7350\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9881 - acc: 0.7625 - val_loss: 1.0295 - val_acc: 0.7370\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9877 - acc: 0.7617 - val_loss: 1.0246 - val_acc: 0.7430\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9876 - acc: 0.7609 - val_loss: 1.0325 - val_acc: 0.7410\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9875 - acc: 0.7600 - val_loss: 1.0275 - val_acc: 0.7420\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9859 - acc: 0.7615 - val_loss: 1.0243 - val_acc: 0.7400\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9856 - acc: 0.7611 - val_loss: 1.0230 - val_acc: 0.7440\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9852 - acc: 0.7595 - val_loss: 1.0233 - val_acc: 0.7380\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9837 - acc: 0.7619 - val_loss: 1.0236 - val_acc: 0.7410\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9834 - acc: 0.7617 - val_loss: 1.0245 - val_acc: 0.7370\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9836 - acc: 0.7624 - val_loss: 1.0257 - val_acc: 0.7450\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9827 - acc: 0.7599 - val_loss: 1.0208 - val_acc: 0.7450\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9819 - acc: 0.7597 - val_loss: 1.0228 - val_acc: 0.7420\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9809 - acc: 0.7636 - val_loss: 1.0308 - val_acc: 0.7350\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9819 - acc: 0.7616 - val_loss: 1.0213 - val_acc: 0.7410\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9810 - acc: 0.7607 - val_loss: 1.0196 - val_acc: 0.7390\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9793 - acc: 0.7625 - val_loss: 1.0213 - val_acc: 0.7420\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9796 - acc: 0.7620 - val_loss: 1.0181 - val_acc: 0.7440\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9792 - acc: 0.7628 - val_loss: 1.0241 - val_acc: 0.7380\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9781 - acc: 0.7625 - val_loss: 1.0368 - val_acc: 0.7300\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9792 - acc: 0.7600 - val_loss: 1.0162 - val_acc: 0.7450\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9774 - acc: 0.7637 - val_loss: 1.0186 - val_acc: 0.7370\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9769 - acc: 0.7628 - val_loss: 1.0180 - val_acc: 0.7460\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9760 - acc: 0.7636 - val_loss: 1.0167 - val_acc: 0.7420\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9763 - acc: 0.7621 - val_loss: 1.0205 - val_acc: 0.7360\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9761 - acc: 0.7604 - val_loss: 1.0168 - val_acc: 0.7400\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9751 - acc: 0.7629 - val_loss: 1.0175 - val_acc: 0.7440\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9748 - acc: 0.7637 - val_loss: 1.0149 - val_acc: 0.7390\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9733 - acc: 0.7623 - val_loss: 1.0140 - val_acc: 0.7430\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9739 - acc: 0.7627 - val_loss: 1.0123 - val_acc: 0.7450\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9721 - acc: 0.7632 - val_loss: 1.0130 - val_acc: 0.7400\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9721 - acc: 0.7652 - val_loss: 1.0148 - val_acc: 0.7470\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9718 - acc: 0.7627 - val_loss: 1.0105 - val_acc: 0.7450\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9715 - acc: 0.7664 - val_loss: 1.0127 - val_acc: 0.7430\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9708 - acc: 0.7633 - val_loss: 1.0125 - val_acc: 0.7460\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9709 - acc: 0.7633 - val_loss: 1.0107 - val_acc: 0.7430\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9701 - acc: 0.7631 - val_loss: 1.0168 - val_acc: 0.7450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9702 - acc: 0.7624 - val_loss: 1.0096 - val_acc: 0.7420\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9694 - acc: 0.7636 - val_loss: 1.0093 - val_acc: 0.7410\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9677 - acc: 0.7648 - val_loss: 1.0151 - val_acc: 0.7370\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9678 - acc: 0.7636 - val_loss: 1.0103 - val_acc: 0.7420\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9676 - acc: 0.7640 - val_loss: 1.0112 - val_acc: 0.7400\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9671 - acc: 0.7639 - val_loss: 1.0198 - val_acc: 0.7450\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9673 - acc: 0.7637 - val_loss: 1.0106 - val_acc: 0.7410\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9655 - acc: 0.7649 - val_loss: 1.0141 - val_acc: 0.7390\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9668 - acc: 0.7641 - val_loss: 1.0165 - val_acc: 0.7440\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9657 - acc: 0.7619 - val_loss: 1.0066 - val_acc: 0.7430\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9644 - acc: 0.7648 - val_loss: 1.0152 - val_acc: 0.7420\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9644 - acc: 0.7629 - val_loss: 1.0126 - val_acc: 0.7460\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9636 - acc: 0.7656 - val_loss: 1.0056 - val_acc: 0.7420\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9626 - acc: 0.7660 - val_loss: 1.0060 - val_acc: 0.7400\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9628 - acc: 0.7648 - val_loss: 1.0134 - val_acc: 0.7400\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9621 - acc: 0.7639 - val_loss: 1.0032 - val_acc: 0.7450\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9606 - acc: 0.7635 - val_loss: 1.0071 - val_acc: 0.7440\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9609 - acc: 0.7636 - val_loss: 1.0048 - val_acc: 0.7460\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9598 - acc: 0.7632 - val_loss: 1.0053 - val_acc: 0.7430\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9599 - acc: 0.7649 - val_loss: 1.0062 - val_acc: 0.7430\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9595 - acc: 0.7629 - val_loss: 1.0092 - val_acc: 0.7410\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9589 - acc: 0.7645 - val_loss: 0.9994 - val_acc: 0.7470\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9579 - acc: 0.7651 - val_loss: 1.0067 - val_acc: 0.7390\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9575 - acc: 0.7656 - val_loss: 1.0167 - val_acc: 0.7370\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9579 - acc: 0.7679 - val_loss: 1.0035 - val_acc: 0.7430\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9582 - acc: 0.7647 - val_loss: 1.0002 - val_acc: 0.7430\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9566 - acc: 0.7669 - val_loss: 0.9956 - val_acc: 0.7520\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9570 - acc: 0.7639 - val_loss: 0.9955 - val_acc: 0.7500\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9557 - acc: 0.7652 - val_loss: 0.9999 - val_acc: 0.7440\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9548 - acc: 0.7652 - val_loss: 0.9966 - val_acc: 0.7510\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9545 - acc: 0.7645 - val_loss: 0.9959 - val_acc: 0.7500\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9541 - acc: 0.7672 - val_loss: 0.9949 - val_acc: 0.7500\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9531 - acc: 0.7660 - val_loss: 0.9956 - val_acc: 0.7450\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9537 - acc: 0.7647 - val_loss: 0.9973 - val_acc: 0.7470\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9539 - acc: 0.7644 - val_loss: 0.9953 - val_acc: 0.7470\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9526 - acc: 0.7661 - val_loss: 0.9983 - val_acc: 0.7450\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9532 - acc: 0.7660 - val_loss: 0.9947 - val_acc: 0.7440\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9517 - acc: 0.7676 - val_loss: 1.0005 - val_acc: 0.7470\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9525 - acc: 0.7673 - val_loss: 0.9956 - val_acc: 0.7470\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9517 - acc: 0.766 - 0s 48us/step - loss: 0.9507 - acc: 0.7667 - val_loss: 0.9960 - val_acc: 0.7490\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9507 - acc: 0.7676 - val_loss: 0.9934 - val_acc: 0.7480\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9502 - acc: 0.7671 - val_loss: 0.9934 - val_acc: 0.7450\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9492 - acc: 0.7665 - val_loss: 0.9924 - val_acc: 0.7480\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9495 - acc: 0.7656 - val_loss: 1.0112 - val_acc: 0.7400\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9494 - acc: 0.7664 - val_loss: 0.9925 - val_acc: 0.7500\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9482 - acc: 0.7685 - val_loss: 0.9978 - val_acc: 0.7450\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9472 - acc: 0.7707 - val_loss: 1.0002 - val_acc: 0.7420\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9485 - acc: 0.7673 - val_loss: 0.9982 - val_acc: 0.7440\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9477 - acc: 0.7667 - val_loss: 0.9887 - val_acc: 0.7520\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.9468 - acc: 0.7697 - val_loss: 0.9954 - val_acc: 0.7500\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9473 - acc: 0.7673 - val_loss: 0.9949 - val_acc: 0.7490\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9465 - acc: 0.7665 - val_loss: 0.9927 - val_acc: 0.7510\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9455 - acc: 0.7680 - val_loss: 0.9888 - val_acc: 0.7500\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9462 - acc: 0.7688 - val_loss: 0.9900 - val_acc: 0.7490\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9456 - acc: 0.7673 - val_loss: 1.0027 - val_acc: 0.7480\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9448 - acc: 0.7663 - val_loss: 1.0004 - val_acc: 0.7410\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9437 - acc: 0.7697 - val_loss: 0.9946 - val_acc: 0.7430\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9435 - acc: 0.7684 - val_loss: 0.9906 - val_acc: 0.7480\n",
      "Epoch 413/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9432 - acc: 0.7693 - val_loss: 0.9940 - val_acc: 0.7420\n",
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9426 - acc: 0.7692 - val_loss: 0.9878 - val_acc: 0.7450\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9423 - acc: 0.7703 - val_loss: 0.9863 - val_acc: 0.7510\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9416 - acc: 0.7693 - val_loss: 0.9877 - val_acc: 0.7430\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9419 - acc: 0.7704 - val_loss: 0.9857 - val_acc: 0.7500\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9404 - acc: 0.7699 - val_loss: 0.9994 - val_acc: 0.7390\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9397 - acc: 0.7708 - val_loss: 0.9918 - val_acc: 0.7430\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9401 - acc: 0.7688 - val_loss: 0.9850 - val_acc: 0.7490\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9397 - acc: 0.7685 - val_loss: 0.9855 - val_acc: 0.7530\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9395 - acc: 0.7683 - val_loss: 0.9898 - val_acc: 0.7460\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9390 - acc: 0.7687 - val_loss: 0.9908 - val_acc: 0.7470\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9392 - acc: 0.7701 - val_loss: 0.9980 - val_acc: 0.7420\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9374 - acc: 0.7692 - val_loss: 0.9889 - val_acc: 0.7500\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9376 - acc: 0.7689 - val_loss: 0.9872 - val_acc: 0.7470\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9375 - acc: 0.7713 - val_loss: 0.9800 - val_acc: 0.7540\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9359 - acc: 0.7696 - val_loss: 0.9820 - val_acc: 0.7530\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9369 - acc: 0.7699 - val_loss: 0.9785 - val_acc: 0.7490\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9356 - acc: 0.7676 - val_loss: 0.9952 - val_acc: 0.7440\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9359 - acc: 0.7691 - val_loss: 0.9827 - val_acc: 0.7510\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9353 - acc: 0.7704 - val_loss: 0.9869 - val_acc: 0.7450\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9343 - acc: 0.7701 - val_loss: 0.9915 - val_acc: 0.7440\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9349 - acc: 0.7701 - val_loss: 0.9848 - val_acc: 0.7460\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.9341 - acc: 0.7699 - val_loss: 0.9771 - val_acc: 0.7480\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.9331 - acc: 0.7699 - val_loss: 0.9822 - val_acc: 0.7500\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9336 - acc: 0.7713 - val_loss: 0.9836 - val_acc: 0.7440\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9328 - acc: 0.7712 - val_loss: 0.9850 - val_acc: 0.7460\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9331 - acc: 0.7709 - val_loss: 0.9768 - val_acc: 0.7450\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9311 - acc: 0.7707 - val_loss: 0.9812 - val_acc: 0.7470\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9317 - acc: 0.7696 - val_loss: 0.9783 - val_acc: 0.7530\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9325 - acc: 0.7711 - val_loss: 0.9788 - val_acc: 0.7430\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9304 - acc: 0.7721 - val_loss: 0.9821 - val_acc: 0.7490\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9311 - acc: 0.7692 - val_loss: 0.9929 - val_acc: 0.7450\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9315 - acc: 0.7685 - val_loss: 0.9760 - val_acc: 0.7550\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9290 - acc: 0.7735 - val_loss: 0.9825 - val_acc: 0.7470\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9295 - acc: 0.7701 - val_loss: 0.9795 - val_acc: 0.7470\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9288 - acc: 0.7715 - val_loss: 0.9788 - val_acc: 0.7500\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9297 - acc: 0.7715 - val_loss: 0.9766 - val_acc: 0.7500\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9289 - acc: 0.7711 - val_loss: 0.9830 - val_acc: 0.7460\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9280 - acc: 0.7712 - val_loss: 0.9754 - val_acc: 0.7520\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.9276 - acc: 0.7725 - val_loss: 0.9726 - val_acc: 0.7440\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9260 - acc: 0.7712 - val_loss: 0.9771 - val_acc: 0.7480\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9275 - acc: 0.7709 - val_loss: 0.9747 - val_acc: 0.7600\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9253 - acc: 0.7735 - val_loss: 0.9743 - val_acc: 0.7510\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9250 - acc: 0.7737 - val_loss: 0.9782 - val_acc: 0.7470\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9258 - acc: 0.7715 - val_loss: 0.9739 - val_acc: 0.7530\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9254 - acc: 0.7723 - val_loss: 0.9833 - val_acc: 0.7480\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9246 - acc: 0.7731 - val_loss: 0.9730 - val_acc: 0.7530\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9253 - acc: 0.7725 - val_loss: 0.9748 - val_acc: 0.7510\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9233 - acc: 0.7737 - val_loss: 0.9723 - val_acc: 0.7520\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9241 - acc: 0.7737 - val_loss: 0.9722 - val_acc: 0.7500\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9226 - acc: 0.7743 - val_loss: 0.9699 - val_acc: 0.7570\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9229 - acc: 0.7716 - val_loss: 0.9757 - val_acc: 0.7470\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9228 - acc: 0.7747 - val_loss: 0.9857 - val_acc: 0.7440\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9238 - acc: 0.7723 - val_loss: 0.9715 - val_acc: 0.7500\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9216 - acc: 0.7724 - val_loss: 0.9754 - val_acc: 0.7440\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9214 - acc: 0.7723 - val_loss: 0.9778 - val_acc: 0.7490\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9218 - acc: 0.7749 - val_loss: 0.9865 - val_acc: 0.7400\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9204 - acc: 0.7728 - val_loss: 0.9757 - val_acc: 0.7450\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9198 - acc: 0.7743 - val_loss: 0.9801 - val_acc: 0.7480\n",
      "Epoch 472/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9204 - acc: 0.7748 - val_loss: 0.9769 - val_acc: 0.7460\n",
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9197 - acc: 0.7737 - val_loss: 0.9706 - val_acc: 0.7540\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9184 - acc: 0.7703 - val_loss: 0.9682 - val_acc: 0.7590\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9186 - acc: 0.7745 - val_loss: 0.9733 - val_acc: 0.7490\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9177 - acc: 0.7737 - val_loss: 0.9691 - val_acc: 0.7540\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9186 - acc: 0.7720 - val_loss: 0.9701 - val_acc: 0.7520\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9169 - acc: 0.7748 - val_loss: 0.9691 - val_acc: 0.7470\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9173 - acc: 0.7756 - val_loss: 0.9769 - val_acc: 0.7470\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9178 - acc: 0.7757 - val_loss: 0.9709 - val_acc: 0.7420\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9135 - acc: 0.776 - 0s 48us/step - loss: 0.9161 - acc: 0.7748 - val_loss: 0.9762 - val_acc: 0.7440\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9169 - acc: 0.7728 - val_loss: 0.9674 - val_acc: 0.7530\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9158 - acc: 0.7761 - val_loss: 0.9678 - val_acc: 0.7510\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9155 - acc: 0.7728 - val_loss: 0.9619 - val_acc: 0.7590\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9139 - acc: 0.7759 - val_loss: 0.9698 - val_acc: 0.7490\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9148 - acc: 0.7743 - val_loss: 0.9636 - val_acc: 0.7430\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9149 - acc: 0.7749 - val_loss: 0.9796 - val_acc: 0.7420\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9158 - acc: 0.7721 - val_loss: 0.9635 - val_acc: 0.7510\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9124 - acc: 0.7755 - val_loss: 0.9782 - val_acc: 0.7410\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9136 - acc: 0.7740 - val_loss: 0.9805 - val_acc: 0.7430\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9124 - acc: 0.7768 - val_loss: 0.9677 - val_acc: 0.7580\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9124 - acc: 0.7741 - val_loss: 0.9593 - val_acc: 0.7530\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9112 - acc: 0.7749 - val_loss: 0.9686 - val_acc: 0.7520\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9110 - acc: 0.7769 - val_loss: 0.9675 - val_acc: 0.7490\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9117 - acc: 0.7743 - val_loss: 0.9624 - val_acc: 0.7510\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9121 - acc: 0.7729 - val_loss: 0.9590 - val_acc: 0.7530\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9101 - acc: 0.7744 - val_loss: 0.9706 - val_acc: 0.7540\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9115 - acc: 0.7736 - val_loss: 0.9624 - val_acc: 0.7520\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9097 - acc: 0.7763 - val_loss: 0.9693 - val_acc: 0.7470\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9104 - acc: 0.7757 - val_loss: 0.9688 - val_acc: 0.7500\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9097 - acc: 0.7769 - val_loss: 0.9644 - val_acc: 0.7510\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9095 - acc: 0.7769 - val_loss: 0.9636 - val_acc: 0.7520\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9079 - acc: 0.7761 - val_loss: 0.9598 - val_acc: 0.7540\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9087 - acc: 0.7759 - val_loss: 0.9655 - val_acc: 0.7540\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9072 - acc: 0.7776 - val_loss: 0.9644 - val_acc: 0.7580\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9089 - acc: 0.7783 - val_loss: 0.9642 - val_acc: 0.7600\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9066 - acc: 0.7761 - val_loss: 0.9571 - val_acc: 0.7620\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9064 - acc: 0.7757 - val_loss: 0.9704 - val_acc: 0.7430\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9076 - acc: 0.7755 - val_loss: 0.9661 - val_acc: 0.7530\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9066 - acc: 0.7763 - val_loss: 0.9604 - val_acc: 0.7460\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9073 - acc: 0.7753 - val_loss: 0.9713 - val_acc: 0.7470\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9064 - acc: 0.7791 - val_loss: 0.9587 - val_acc: 0.7500\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9052 - acc: 0.7775 - val_loss: 0.9580 - val_acc: 0.7550\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9044 - acc: 0.7777 - val_loss: 0.9595 - val_acc: 0.7550\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9050 - acc: 0.7781 - val_loss: 0.9622 - val_acc: 0.7470\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9029 - acc: 0.7791 - val_loss: 0.9573 - val_acc: 0.7570\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9031 - acc: 0.7795 - val_loss: 0.9556 - val_acc: 0.7530\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9027 - acc: 0.7771 - val_loss: 0.9607 - val_acc: 0.7480\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9032 - acc: 0.7764 - val_loss: 0.9506 - val_acc: 0.7550\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9021 - acc: 0.7765 - val_loss: 0.9522 - val_acc: 0.7520\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9038 - acc: 0.7771 - val_loss: 0.9521 - val_acc: 0.7620\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9012 - acc: 0.7779 - val_loss: 0.9531 - val_acc: 0.7650\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9009 - acc: 0.7776 - val_loss: 0.9846 - val_acc: 0.7440\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9028 - acc: 0.7805 - val_loss: 0.9533 - val_acc: 0.7580\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8995 - acc: 0.7761 - val_loss: 0.9529 - val_acc: 0.7580\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9003 - acc: 0.7785 - val_loss: 0.9593 - val_acc: 0.7460\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8999 - acc: 0.7781 - val_loss: 0.9534 - val_acc: 0.7610\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9012 - acc: 0.7775 - val_loss: 0.9569 - val_acc: 0.7530\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8997 - acc: 0.7808 - val_loss: 0.9540 - val_acc: 0.7520\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8997 - acc: 0.7797 - val_loss: 0.9561 - val_acc: 0.7520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8988 - acc: 0.7799 - val_loss: 0.9565 - val_acc: 0.7540\n",
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8978 - acc: 0.7793 - val_loss: 0.9566 - val_acc: 0.7520\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8981 - acc: 0.7796 - val_loss: 0.9494 - val_acc: 0.7640\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8989 - acc: 0.7797 - val_loss: 0.9754 - val_acc: 0.7430\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8982 - acc: 0.7809 - val_loss: 0.9488 - val_acc: 0.7580\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8971 - acc: 0.7800 - val_loss: 0.9468 - val_acc: 0.7600\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8970 - acc: 0.7797 - val_loss: 0.9539 - val_acc: 0.7550\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8983 - acc: 0.7787 - val_loss: 0.9824 - val_acc: 0.7490\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8977 - acc: 0.7783 - val_loss: 0.9514 - val_acc: 0.7560\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8965 - acc: 0.7792 - val_loss: 0.9479 - val_acc: 0.7630\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8956 - acc: 0.7800 - val_loss: 0.9623 - val_acc: 0.7430\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8950 - acc: 0.7819 - val_loss: 0.9639 - val_acc: 0.7490\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8958 - acc: 0.7804 - val_loss: 0.9629 - val_acc: 0.7510\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8947 - acc: 0.7793 - val_loss: 0.9604 - val_acc: 0.7520\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8942 - acc: 0.7809 - val_loss: 0.9482 - val_acc: 0.7590\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8947 - acc: 0.7839 - val_loss: 0.9578 - val_acc: 0.7460\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8945 - acc: 0.7821 - val_loss: 0.9502 - val_acc: 0.7510\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8929 - acc: 0.7804 - val_loss: 0.9588 - val_acc: 0.7550\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8937 - acc: 0.7820 - val_loss: 0.9518 - val_acc: 0.7530\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8929 - acc: 0.7792 - val_loss: 0.9483 - val_acc: 0.7570\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8932 - acc: 0.7805 - val_loss: 0.9623 - val_acc: 0.7470\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8946 - acc: 0.7820 - val_loss: 0.9566 - val_acc: 0.7480\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8907 - acc: 0.7811 - val_loss: 0.9511 - val_acc: 0.7570\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8923 - acc: 0.7831 - val_loss: 0.9538 - val_acc: 0.7580\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8914 - acc: 0.7805 - val_loss: 0.9569 - val_acc: 0.7520\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8919 - acc: 0.7799 - val_loss: 0.9466 - val_acc: 0.7600\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8915 - acc: 0.7836 - val_loss: 0.9576 - val_acc: 0.7560\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8935 - acc: 0.7795 - val_loss: 0.9582 - val_acc: 0.7500\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8899 - acc: 0.7839 - val_loss: 0.9538 - val_acc: 0.7480\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8899 - acc: 0.7817 - val_loss: 0.9541 - val_acc: 0.7560\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8913 - acc: 0.7793 - val_loss: 0.9532 - val_acc: 0.7500\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8893 - acc: 0.7836 - val_loss: 0.9457 - val_acc: 0.7600\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8893 - acc: 0.7821 - val_loss: 0.9576 - val_acc: 0.7440\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8899 - acc: 0.7847 - val_loss: 0.9499 - val_acc: 0.7630\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8886 - acc: 0.7817 - val_loss: 0.9653 - val_acc: 0.7440\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8891 - acc: 0.7847 - val_loss: 0.9432 - val_acc: 0.7530\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8888 - acc: 0.7824 - val_loss: 0.9564 - val_acc: 0.7540\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8891 - acc: 0.7828 - val_loss: 0.9420 - val_acc: 0.7580\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8891 - acc: 0.7821 - val_loss: 0.9427 - val_acc: 0.7600\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8859 - acc: 0.7843 - val_loss: 0.9693 - val_acc: 0.7520\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8871 - acc: 0.7845 - val_loss: 0.9420 - val_acc: 0.7560\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8856 - acc: 0.7852 - val_loss: 0.9469 - val_acc: 0.7550\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8863 - acc: 0.7837 - val_loss: 0.9578 - val_acc: 0.7530\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8871 - acc: 0.7831 - val_loss: 0.9395 - val_acc: 0.7570\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8869 - acc: 0.7841 - val_loss: 0.9426 - val_acc: 0.7560\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8851 - acc: 0.7829 - val_loss: 0.9429 - val_acc: 0.7630\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8851 - acc: 0.7837 - val_loss: 0.9369 - val_acc: 0.7620\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8835 - acc: 0.7848 - val_loss: 0.9520 - val_acc: 0.7450\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8837 - acc: 0.7851 - val_loss: 0.9448 - val_acc: 0.7540\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8854 - acc: 0.7855 - val_loss: 0.9405 - val_acc: 0.7630\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8852 - acc: 0.7845 - val_loss: 0.9424 - val_acc: 0.7590\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8860 - acc: 0.7852 - val_loss: 0.9482 - val_acc: 0.7530\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8848 - acc: 0.7847 - val_loss: 0.9481 - val_acc: 0.7580\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8830 - acc: 0.7843 - val_loss: 0.9600 - val_acc: 0.7470\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8834 - acc: 0.7852 - val_loss: 0.9373 - val_acc: 0.7590\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8826 - acc: 0.7865 - val_loss: 0.9383 - val_acc: 0.7660\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8836 - acc: 0.7847 - val_loss: 0.9454 - val_acc: 0.7560\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8824 - acc: 0.7851 - val_loss: 0.9641 - val_acc: 0.7510\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8822 - acc: 0.7871 - val_loss: 0.9459 - val_acc: 0.7530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8835 - acc: 0.7867 - val_loss: 0.9405 - val_acc: 0.7600\n",
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8833 - acc: 0.7844 - val_loss: 0.9473 - val_acc: 0.7530\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8800 - acc: 0.7856 - val_loss: 0.9540 - val_acc: 0.7560\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8825 - acc: 0.7872 - val_loss: 0.9500 - val_acc: 0.7560\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8821 - acc: 0.7857 - val_loss: 0.9446 - val_acc: 0.7580\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8786 - acc: 0.7868 - val_loss: 0.9423 - val_acc: 0.7580\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8803 - acc: 0.7863 - val_loss: 0.9434 - val_acc: 0.7520\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8797 - acc: 0.7879 - val_loss: 0.9467 - val_acc: 0.7560\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8793 - acc: 0.7871 - val_loss: 0.9337 - val_acc: 0.7540\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8789 - acc: 0.7876 - val_loss: 0.9391 - val_acc: 0.7570\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8801 - acc: 0.7864 - val_loss: 0.9413 - val_acc: 0.7580\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8791 - acc: 0.7868 - val_loss: 0.9339 - val_acc: 0.7600\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8786 - acc: 0.7861 - val_loss: 0.9357 - val_acc: 0.7600\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8798 - acc: 0.7859 - val_loss: 0.9394 - val_acc: 0.7580\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8779 - acc: 0.7844 - val_loss: 0.9365 - val_acc: 0.7660\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8781 - acc: 0.7889 - val_loss: 0.9381 - val_acc: 0.7580\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8792 - acc: 0.7879 - val_loss: 0.9384 - val_acc: 0.7560\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8783 - acc: 0.7872 - val_loss: 0.9375 - val_acc: 0.7550\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8758 - acc: 0.7879 - val_loss: 0.9664 - val_acc: 0.7450\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8805 - acc: 0.7867 - val_loss: 0.9394 - val_acc: 0.7580\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8770 - acc: 0.7895 - val_loss: 0.9403 - val_acc: 0.7570\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8760 - acc: 0.7908 - val_loss: 0.9496 - val_acc: 0.7580\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8754 - acc: 0.7883 - val_loss: 0.9348 - val_acc: 0.7590\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8746 - acc: 0.7864 - val_loss: 0.9409 - val_acc: 0.7600\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8763 - acc: 0.7893 - val_loss: 0.9333 - val_acc: 0.7670\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8765 - acc: 0.7879 - val_loss: 0.9316 - val_acc: 0.7620\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8759 - acc: 0.7880 - val_loss: 0.9707 - val_acc: 0.7550\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8750 - acc: 0.7868 - val_loss: 0.9329 - val_acc: 0.7630\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8739 - acc: 0.7889 - val_loss: 0.9377 - val_acc: 0.7630\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8750 - acc: 0.7865 - val_loss: 0.9559 - val_acc: 0.7500\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8730 - acc: 0.7876 - val_loss: 0.9335 - val_acc: 0.7610\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8736 - acc: 0.7883 - val_loss: 0.9367 - val_acc: 0.7620\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8763 - acc: 0.7889 - val_loss: 0.9328 - val_acc: 0.7620\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8728 - acc: 0.7899 - val_loss: 0.9336 - val_acc: 0.7640\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8722 - acc: 0.7892 - val_loss: 0.9537 - val_acc: 0.7530\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8735 - acc: 0.7896 - val_loss: 0.9264 - val_acc: 0.7650\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8718 - acc: 0.7895 - val_loss: 0.9440 - val_acc: 0.7550\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8719 - acc: 0.7899 - val_loss: 0.9444 - val_acc: 0.7550\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8723 - acc: 0.7876 - val_loss: 0.9310 - val_acc: 0.7620\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8744 - acc: 0.7891 - val_loss: 0.9452 - val_acc: 0.7580\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8734 - acc: 0.7881 - val_loss: 0.9373 - val_acc: 0.7530\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8710 - acc: 0.7896 - val_loss: 0.9333 - val_acc: 0.7590\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8709 - acc: 0.7900 - val_loss: 0.9270 - val_acc: 0.7600\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8721 - acc: 0.7903 - val_loss: 0.9575 - val_acc: 0.7610\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8704 - acc: 0.7901 - val_loss: 0.9345 - val_acc: 0.7560\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8717 - acc: 0.7891 - val_loss: 0.9425 - val_acc: 0.7530\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8704 - acc: 0.7919 - val_loss: 0.9353 - val_acc: 0.7600\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8692 - acc: 0.7905 - val_loss: 0.9276 - val_acc: 0.7600\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8685 - acc: 0.7929 - val_loss: 0.9587 - val_acc: 0.7520\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8702 - acc: 0.7904 - val_loss: 0.9336 - val_acc: 0.7560\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8703 - acc: 0.7904 - val_loss: 0.9308 - val_acc: 0.7550\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8675 - acc: 0.7891 - val_loss: 0.9420 - val_acc: 0.7550\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8691 - acc: 0.7904 - val_loss: 0.9284 - val_acc: 0.7590\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8664 - acc: 0.7905 - val_loss: 0.9396 - val_acc: 0.7580\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8673 - acc: 0.7924 - val_loss: 0.9268 - val_acc: 0.7620\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8688 - acc: 0.7912 - val_loss: 0.9539 - val_acc: 0.7620\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8681 - acc: 0.7929 - val_loss: 0.9536 - val_acc: 0.7580\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8690 - acc: 0.7888 - val_loss: 0.9274 - val_acc: 0.7700\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8675 - acc: 0.7905 - val_loss: 0.9325 - val_acc: 0.7600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8684 - acc: 0.7932 - val_loss: 0.9453 - val_acc: 0.7600\n",
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8674 - acc: 0.7933 - val_loss: 0.9375 - val_acc: 0.7590\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8651 - acc: 0.7925 - val_loss: 0.9761 - val_acc: 0.7390\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8666 - acc: 0.7915 - val_loss: 0.9272 - val_acc: 0.7630\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8658 - acc: 0.7945 - val_loss: 0.9362 - val_acc: 0.7600\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8656 - acc: 0.7913 - val_loss: 0.9222 - val_acc: 0.7630\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8646 - acc: 0.7936 - val_loss: 0.9827 - val_acc: 0.7520\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8700 - acc: 0.7893 - val_loss: 0.9286 - val_acc: 0.7640\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8638 - acc: 0.7939 - val_loss: 0.9288 - val_acc: 0.7620\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8658 - acc: 0.7932 - val_loss: 0.9357 - val_acc: 0.7640\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8640 - acc: 0.7941 - val_loss: 0.9388 - val_acc: 0.7620\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8646 - acc: 0.7924 - val_loss: 0.9252 - val_acc: 0.7610\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8642 - acc: 0.7973 - val_loss: 0.9345 - val_acc: 0.7620\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8649 - acc: 0.7923 - val_loss: 0.9329 - val_acc: 0.7640\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8637 - acc: 0.7943 - val_loss: 0.9505 - val_acc: 0.7520\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8647 - acc: 0.7921 - val_loss: 0.9433 - val_acc: 0.7520\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8640 - acc: 0.7964 - val_loss: 0.9450 - val_acc: 0.7630\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8633 - acc: 0.7931 - val_loss: 0.9286 - val_acc: 0.7670\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8635 - acc: 0.7945 - val_loss: 0.9282 - val_acc: 0.7560\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8606 - acc: 0.7952 - val_loss: 1.0082 - val_acc: 0.7440\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8628 - acc: 0.7925 - val_loss: 0.9203 - val_acc: 0.7640\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8630 - acc: 0.7953 - val_loss: 0.9242 - val_acc: 0.7640\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8637 - acc: 0.7952 - val_loss: 0.9294 - val_acc: 0.7590\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8606 - acc: 0.7972 - val_loss: 0.9298 - val_acc: 0.7670\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8608 - acc: 0.7933 - val_loss: 0.9292 - val_acc: 0.7570\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8603 - acc: 0.7929 - val_loss: 0.9485 - val_acc: 0.7560\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8610 - acc: 0.7955 - val_loss: 0.9341 - val_acc: 0.7580\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8600 - acc: 0.7945 - val_loss: 0.9327 - val_acc: 0.7590\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8593 - acc: 0.7963 - val_loss: 0.9257 - val_acc: 0.7630\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8609 - acc: 0.7961 - val_loss: 0.9422 - val_acc: 0.7600\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8598 - acc: 0.7959 - val_loss: 0.9296 - val_acc: 0.7650\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8594 - acc: 0.7925 - val_loss: 0.9263 - val_acc: 0.7680\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8596 - acc: 0.7979 - val_loss: 0.9238 - val_acc: 0.7720\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8600 - acc: 0.7957 - val_loss: 0.9214 - val_acc: 0.7630\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8586 - acc: 0.7957 - val_loss: 0.9208 - val_acc: 0.7660\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8588 - acc: 0.7952 - val_loss: 0.9261 - val_acc: 0.7630\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8568 - acc: 0.7923 - val_loss: 0.9286 - val_acc: 0.7630\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8585 - acc: 0.7963 - val_loss: 0.9297 - val_acc: 0.7600\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8589 - acc: 0.7953 - val_loss: 0.9298 - val_acc: 0.7630\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8590 - acc: 0.7917 - val_loss: 0.9285 - val_acc: 0.7630\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8584 - acc: 0.7964 - val_loss: 0.9405 - val_acc: 0.7560\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8555 - acc: 0.7984 - val_loss: 0.9318 - val_acc: 0.7590\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8572 - acc: 0.7963 - val_loss: 0.9294 - val_acc: 0.7670\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8565 - acc: 0.7967 - val_loss: 0.9261 - val_acc: 0.7580\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8557 - acc: 0.7967 - val_loss: 0.9243 - val_acc: 0.7590\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8548 - acc: 0.7976 - val_loss: 0.9405 - val_acc: 0.7510\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8561 - acc: 0.7975 - val_loss: 0.9253 - val_acc: 0.7660\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8564 - acc: 0.7965 - val_loss: 0.9342 - val_acc: 0.7680\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8563 - acc: 0.7979 - val_loss: 0.9197 - val_acc: 0.7630\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8554 - acc: 0.7953 - val_loss: 0.9193 - val_acc: 0.7710\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8575 - acc: 0.7984 - val_loss: 0.9664 - val_acc: 0.7580\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8576 - acc: 0.7969 - val_loss: 0.9321 - val_acc: 0.7620\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8564 - acc: 0.7948 - val_loss: 0.9347 - val_acc: 0.7550\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8550 - acc: 0.7984 - val_loss: 0.9467 - val_acc: 0.7550\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8562 - acc: 0.7961 - val_loss: 0.9187 - val_acc: 0.7590\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8549 - acc: 0.7973 - val_loss: 0.9262 - val_acc: 0.7660\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8557 - acc: 0.7961 - val_loss: 0.9333 - val_acc: 0.7550\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8552 - acc: 0.7975 - val_loss: 0.9427 - val_acc: 0.7580\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8555 - acc: 0.7980 - val_loss: 0.9254 - val_acc: 0.7700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8527 - acc: 0.8001 - val_loss: 0.9225 - val_acc: 0.7640\n",
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8529 - acc: 0.7983 - val_loss: 0.9239 - val_acc: 0.7670\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8522 - acc: 0.7993 - val_loss: 0.9331 - val_acc: 0.7650\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8547 - acc: 0.7979 - val_loss: 0.9140 - val_acc: 0.7680\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8511 - acc: 0.7995 - val_loss: 0.9271 - val_acc: 0.7690\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8539 - acc: 0.7981 - val_loss: 0.9438 - val_acc: 0.7710\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8554 - acc: 0.7987 - val_loss: 0.9288 - val_acc: 0.7640\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8530 - acc: 0.8017 - val_loss: 0.9670 - val_acc: 0.7560\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8525 - acc: 0.8004 - val_loss: 0.9233 - val_acc: 0.7590\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8521 - acc: 0.7985 - val_loss: 0.9164 - val_acc: 0.7590\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8505 - acc: 0.7995 - val_loss: 0.9270 - val_acc: 0.7680\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8519 - acc: 0.7983 - val_loss: 0.9353 - val_acc: 0.7520\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8510 - acc: 0.8012 - val_loss: 0.9157 - val_acc: 0.7730\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8511 - acc: 0.7983 - val_loss: 0.9265 - val_acc: 0.7640\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8498 - acc: 0.7993 - val_loss: 0.9215 - val_acc: 0.7630\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8524 - acc: 0.7995 - val_loss: 0.9193 - val_acc: 0.7710\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8510 - acc: 0.8000 - val_loss: 0.9511 - val_acc: 0.7500\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8524 - acc: 0.8003 - val_loss: 0.9287 - val_acc: 0.7670\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8493 - acc: 0.7948 - val_loss: 0.9266 - val_acc: 0.7720\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8510 - acc: 0.7996 - val_loss: 0.9193 - val_acc: 0.7760\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8501 - acc: 0.7989 - val_loss: 0.9200 - val_acc: 0.7580\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8507 - acc: 0.7971 - val_loss: 0.9248 - val_acc: 0.7660\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8493 - acc: 0.8009 - val_loss: 0.9194 - val_acc: 0.7600\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8491 - acc: 0.7985 - val_loss: 0.9320 - val_acc: 0.7580\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8505 - acc: 0.7983 - val_loss: 0.9333 - val_acc: 0.7690\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8490 - acc: 0.8015 - val_loss: 0.9134 - val_acc: 0.7770\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8473 - acc: 0.8027 - val_loss: 0.9253 - val_acc: 0.7630\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8490 - acc: 0.7989 - val_loss: 0.9491 - val_acc: 0.7560\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8517 - acc: 0.7997 - val_loss: 0.9168 - val_acc: 0.7690\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8480 - acc: 0.8005 - val_loss: 0.9563 - val_acc: 0.7560\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8485 - acc: 0.8016 - val_loss: 0.9184 - val_acc: 0.7710\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8477 - acc: 0.7997 - val_loss: 0.9829 - val_acc: 0.7400\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8493 - acc: 0.7991 - val_loss: 0.9329 - val_acc: 0.7660\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8490 - acc: 0.7993 - val_loss: 0.9206 - val_acc: 0.7730\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8468 - acc: 0.8037 - val_loss: 0.9562 - val_acc: 0.7590\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8472 - acc: 0.8021 - val_loss: 0.9345 - val_acc: 0.7660\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8479 - acc: 0.8015 - val_loss: 0.9188 - val_acc: 0.7650\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8474 - acc: 0.8023 - val_loss: 0.9368 - val_acc: 0.7550\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8471 - acc: 0.7999 - val_loss: 0.9665 - val_acc: 0.7550\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8468 - acc: 0.8011 - val_loss: 0.9419 - val_acc: 0.7600\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8458 - acc: 0.8007 - val_loss: 0.9214 - val_acc: 0.7640\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8459 - acc: 0.8052 - val_loss: 0.9181 - val_acc: 0.7700\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8459 - acc: 0.8041 - val_loss: 0.9265 - val_acc: 0.7680\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8457 - acc: 0.7999 - val_loss: 0.9399 - val_acc: 0.7600\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8450 - acc: 0.8029 - val_loss: 0.9156 - val_acc: 0.7750\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8476 - acc: 0.8003 - val_loss: 0.9234 - val_acc: 0.7660\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8454 - acc: 0.8015 - val_loss: 0.9133 - val_acc: 0.7760\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8457 - acc: 0.8020 - val_loss: 0.9559 - val_acc: 0.7600\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8458 - acc: 0.8029 - val_loss: 0.9335 - val_acc: 0.7610\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8443 - acc: 0.8020 - val_loss: 0.9158 - val_acc: 0.7700\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8453 - acc: 0.8045 - val_loss: 0.9288 - val_acc: 0.7600\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8448 - acc: 0.8041 - val_loss: 0.9612 - val_acc: 0.7550\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8479 - acc: 0.8000 - val_loss: 0.9325 - val_acc: 0.7640\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8474 - acc: 0.8000 - val_loss: 0.9377 - val_acc: 0.7590\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8452 - acc: 0.8024 - val_loss: 0.9579 - val_acc: 0.7520\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8458 - acc: 0.8005 - val_loss: 0.9094 - val_acc: 0.7740\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8459 - acc: 0.8036 - val_loss: 0.9216 - val_acc: 0.7640\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8429 - acc: 0.8011 - val_loss: 0.9150 - val_acc: 0.7700\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8424 - acc: 0.8016 - val_loss: 0.9373 - val_acc: 0.7590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8425 - acc: 0.8039 - val_loss: 0.9175 - val_acc: 0.7670\n",
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8444 - acc: 0.8043 - val_loss: 0.9889 - val_acc: 0.7560\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8485 - acc: 0.8008 - val_loss: 0.9234 - val_acc: 0.7710\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8426 - acc: 0.8015 - val_loss: 0.9260 - val_acc: 0.7720\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8458 - acc: 0.8028 - val_loss: 0.9481 - val_acc: 0.7570\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8430 - acc: 0.8043 - val_loss: 0.9397 - val_acc: 0.7630\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8435 - acc: 0.7999 - val_loss: 0.9399 - val_acc: 0.7610\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8442 - acc: 0.8020 - val_loss: 0.9413 - val_acc: 0.7550\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8416 - acc: 0.8032 - val_loss: 0.9343 - val_acc: 0.7640\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8435 - acc: 0.8028 - val_loss: 0.9148 - val_acc: 0.7730\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8397 - acc: 0.8017 - val_loss: 0.9135 - val_acc: 0.7670\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8397 - acc: 0.8032 - val_loss: 0.9638 - val_acc: 0.7540\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8439 - acc: 0.7992 - val_loss: 0.9302 - val_acc: 0.7670\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8401 - acc: 0.8053 - val_loss: 0.9291 - val_acc: 0.7570\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8404 - acc: 0.8029 - val_loss: 0.9250 - val_acc: 0.7660\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8396 - acc: 0.8023 - val_loss: 0.9185 - val_acc: 0.7620\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8389 - acc: 0.8045 - val_loss: 0.9270 - val_acc: 0.7650\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8408 - acc: 0.8053 - val_loss: 0.9040 - val_acc: 0.7720\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8414 - acc: 0.8031 - val_loss: 0.9122 - val_acc: 0.7670\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8417 - acc: 0.8049 - val_loss: 0.9197 - val_acc: 0.7610\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8379 - acc: 0.8047 - val_loss: 0.9089 - val_acc: 0.7760\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8374 - acc: 0.8051 - val_loss: 0.9117 - val_acc: 0.7730\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8370 - acc: 0.8044 - val_loss: 0.9170 - val_acc: 0.7580\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8435 - acc: 0.8012 - val_loss: 0.9178 - val_acc: 0.7730\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8377 - acc: 0.8052 - val_loss: 0.9215 - val_acc: 0.7650\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8412 - acc: 0.8032 - val_loss: 0.9180 - val_acc: 0.7660\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8375 - acc: 0.8052 - val_loss: 0.9273 - val_acc: 0.7680\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8399 - acc: 0.8011 - val_loss: 0.9117 - val_acc: 0.7690\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8392 - acc: 0.8076 - val_loss: 0.9238 - val_acc: 0.7660\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8389 - acc: 0.8032 - val_loss: 0.9046 - val_acc: 0.7630\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8373 - acc: 0.8080 - val_loss: 0.9153 - val_acc: 0.7660\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8385 - acc: 0.8044 - val_loss: 0.9135 - val_acc: 0.7690\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8387 - acc: 0.8052 - val_loss: 0.9411 - val_acc: 0.7690\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8403 - acc: 0.8049 - val_loss: 0.9325 - val_acc: 0.7670\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8363 - acc: 0.8035 - val_loss: 0.9104 - val_acc: 0.7760\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8355 - acc: 0.8077 - val_loss: 0.9091 - val_acc: 0.7770\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8368 - acc: 0.8051 - val_loss: 0.9242 - val_acc: 0.7610\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8372 - acc: 0.8073 - val_loss: 0.9132 - val_acc: 0.7750\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8336 - acc: 0.8055 - val_loss: 0.9192 - val_acc: 0.7710\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8373 - acc: 0.8036 - val_loss: 0.9046 - val_acc: 0.7790\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8390 - acc: 0.8067 - val_loss: 0.9176 - val_acc: 0.7720\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8366 - acc: 0.8087 - val_loss: 0.9120 - val_acc: 0.7730\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8358 - acc: 0.8052 - val_loss: 0.9147 - val_acc: 0.7720\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8354 - acc: 0.8085 - val_loss: 0.9367 - val_acc: 0.7640\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8341 - acc: 0.8072 - val_loss: 0.9191 - val_acc: 0.7720\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8344 - acc: 0.8072 - val_loss: 0.9591 - val_acc: 0.7540\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8347 - acc: 0.8067 - val_loss: 0.9063 - val_acc: 0.7660\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8326 - acc: 0.8069 - val_loss: 0.9066 - val_acc: 0.7730\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8351 - acc: 0.8040 - val_loss: 0.9069 - val_acc: 0.7780\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8349 - acc: 0.8052 - val_loss: 0.9099 - val_acc: 0.7850\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8354 - acc: 0.8065 - val_loss: 0.9436 - val_acc: 0.7620\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8341 - acc: 0.8059 - val_loss: 0.9055 - val_acc: 0.7760\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8349 - acc: 0.8068 - val_loss: 0.9179 - val_acc: 0.7770\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8327 - acc: 0.8091 - val_loss: 0.9351 - val_acc: 0.7660\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8324 - acc: 0.8092 - val_loss: 0.9127 - val_acc: 0.7700\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8337 - acc: 0.8077 - val_loss: 0.9110 - val_acc: 0.7750\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8346 - acc: 0.8047 - val_loss: 0.9304 - val_acc: 0.7690\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8316 - acc: 0.8068 - val_loss: 0.9072 - val_acc: 0.7710\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8307 - acc: 0.8109 - val_loss: 0.9231 - val_acc: 0.7730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8340 - acc: 0.8091 - val_loss: 0.9256 - val_acc: 0.7740\n",
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8309 - acc: 0.8108 - val_loss: 0.9085 - val_acc: 0.7690\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8345 - acc: 0.8072 - val_loss: 0.9120 - val_acc: 0.7700\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8315 - acc: 0.8085 - val_loss: 0.9121 - val_acc: 0.7760\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8320 - acc: 0.8067 - val_loss: 0.9146 - val_acc: 0.7600\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8346 - acc: 0.8052 - val_loss: 0.9048 - val_acc: 0.7690\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8292 - acc: 0.8101 - val_loss: 0.9269 - val_acc: 0.7700\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8325 - acc: 0.8056 - val_loss: 0.9143 - val_acc: 0.7760\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8322 - acc: 0.8093 - val_loss: 0.9197 - val_acc: 0.7680\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8333 - acc: 0.8060 - val_loss: 0.9261 - val_acc: 0.7650\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8300 - acc: 0.8065 - val_loss: 0.9048 - val_acc: 0.7660\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8286 - acc: 0.8104 - val_loss: 0.9297 - val_acc: 0.7660\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8308 - acc: 0.8083 - val_loss: 0.9357 - val_acc: 0.7690\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8327 - acc: 0.8083 - val_loss: 0.9092 - val_acc: 0.7740\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8299 - acc: 0.8084 - val_loss: 0.9233 - val_acc: 0.7690\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8323 - acc: 0.8095 - val_loss: 0.9141 - val_acc: 0.7760\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8294 - acc: 0.8089 - val_loss: 0.9427 - val_acc: 0.7620\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8311 - acc: 0.8083 - val_loss: 0.9044 - val_acc: 0.7760\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8326 - acc: 0.8067 - val_loss: 0.9542 - val_acc: 0.7610\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8308 - acc: 0.8076 - val_loss: 0.9079 - val_acc: 0.7790\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8291 - acc: 0.8075 - val_loss: 0.9161 - val_acc: 0.7740\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8299 - acc: 0.8104 - val_loss: 0.9398 - val_acc: 0.7590\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8308 - acc: 0.8083 - val_loss: 0.9116 - val_acc: 0.7730\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8296 - acc: 0.8100 - val_loss: 0.9331 - val_acc: 0.7650\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8288 - acc: 0.8089 - val_loss: 0.9168 - val_acc: 0.7640\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8303 - acc: 0.8117 - val_loss: 0.9401 - val_acc: 0.7610\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8339 - acc: 0.8080 - val_loss: 0.9041 - val_acc: 0.7810\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8276 - acc: 0.8113 - val_loss: 0.9349 - val_acc: 0.7630\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8369 - acc: 0.8059 - val_loss: 0.9050 - val_acc: 0.7760\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8271 - acc: 0.8111 - val_loss: 0.9039 - val_acc: 0.7750\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8268 - acc: 0.8091 - val_loss: 0.9143 - val_acc: 0.7670\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8255 - acc: 0.8113 - val_loss: 0.9101 - val_acc: 0.7730\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8301 - acc: 0.8077 - val_loss: 0.9068 - val_acc: 0.7750\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8263 - acc: 0.8101 - val_loss: 0.9442 - val_acc: 0.7580\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8286 - acc: 0.8084 - val_loss: 0.9307 - val_acc: 0.7700\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8335 - acc: 0.8065 - val_loss: 0.9110 - val_acc: 0.7620\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8280 - acc: 0.8081 - val_loss: 0.9387 - val_acc: 0.7600\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8289 - acc: 0.8088 - val_loss: 0.9338 - val_acc: 0.7600\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8280 - acc: 0.8088 - val_loss: 0.9114 - val_acc: 0.7730\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8263 - acc: 0.8105 - val_loss: 0.9041 - val_acc: 0.7800\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8240 - acc: 0.8091 - val_loss: 0.9151 - val_acc: 0.7610\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8263 - acc: 0.8075 - val_loss: 0.9218 - val_acc: 0.7750\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8290 - acc: 0.8104 - val_loss: 0.9314 - val_acc: 0.7650\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8297 - acc: 0.8069 - val_loss: 0.9199 - val_acc: 0.7690\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8264 - acc: 0.8077 - val_loss: 0.9170 - val_acc: 0.7770\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8292 - acc: 0.8087 - val_loss: 0.9088 - val_acc: 0.7810\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8257 - acc: 0.8115 - val_loss: 0.9049 - val_acc: 0.7760\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8256 - acc: 0.8093 - val_loss: 0.9026 - val_acc: 0.7770\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8322 - acc: 0.8117 - val_loss: 0.9295 - val_acc: 0.7710\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8261 - acc: 0.8131 - val_loss: 0.9145 - val_acc: 0.7770\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8249 - acc: 0.8104 - val_loss: 0.9252 - val_acc: 0.7740\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8301 - acc: 0.8101 - val_loss: 0.9044 - val_acc: 0.7810\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8240 - acc: 0.8093 - val_loss: 0.9048 - val_acc: 0.7800\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8297 - acc: 0.8075 - val_loss: 0.9103 - val_acc: 0.7800\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8281 - acc: 0.8081 - val_loss: 0.9184 - val_acc: 0.7790\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8220 - acc: 0.8101 - val_loss: 0.9064 - val_acc: 0.7720\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8243 - acc: 0.8128 - val_loss: 0.8999 - val_acc: 0.7820\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8272 - acc: 0.8088 - val_loss: 0.9173 - val_acc: 0.7770\n",
      "Epoch 884/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8269 - acc: 0.8099 - val_loss: 0.9278 - val_acc: 0.7660\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8278 - acc: 0.8095 - val_loss: 0.9032 - val_acc: 0.7860\n",
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8233 - acc: 0.8107 - val_loss: 0.9551 - val_acc: 0.7560\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8289 - acc: 0.8099 - val_loss: 0.9437 - val_acc: 0.7570\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8288 - acc: 0.8123 - val_loss: 0.9395 - val_acc: 0.7570\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8233 - acc: 0.8119 - val_loss: 0.9962 - val_acc: 0.7360\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8277 - acc: 0.8096 - val_loss: 0.9029 - val_acc: 0.7810\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8245 - acc: 0.8113 - val_loss: 0.9422 - val_acc: 0.7620\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8226 - acc: 0.8131 - val_loss: 0.9528 - val_acc: 0.7570\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8275 - acc: 0.8089 - val_loss: 0.9102 - val_acc: 0.7780\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8233 - acc: 0.8135 - val_loss: 0.9301 - val_acc: 0.7670\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8242 - acc: 0.8085 - val_loss: 0.9510 - val_acc: 0.7660\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8239 - acc: 0.8112 - val_loss: 0.9062 - val_acc: 0.7810\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8237 - acc: 0.8135 - val_loss: 0.9063 - val_acc: 0.7810\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8240 - acc: 0.8140 - val_loss: 0.8936 - val_acc: 0.7780\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8232 - acc: 0.8132 - val_loss: 0.9384 - val_acc: 0.7510\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8265 - acc: 0.8085 - val_loss: 0.9226 - val_acc: 0.7690\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8209 - acc: 0.8133 - val_loss: 0.8965 - val_acc: 0.7790\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8220 - acc: 0.8133 - val_loss: 0.9151 - val_acc: 0.7720\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8207 - acc: 0.8155 - val_loss: 0.9331 - val_acc: 0.7760\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8249 - acc: 0.8111 - val_loss: 0.9039 - val_acc: 0.7840\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8203 - acc: 0.8123 - val_loss: 0.8988 - val_acc: 0.7800\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8210 - acc: 0.8133 - val_loss: 0.8947 - val_acc: 0.7810\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8213 - acc: 0.8115 - val_loss: 0.9309 - val_acc: 0.7740\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8186 - acc: 0.8145 - val_loss: 0.9314 - val_acc: 0.7590\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8247 - acc: 0.8089 - val_loss: 0.9377 - val_acc: 0.7680\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8232 - acc: 0.8141 - val_loss: 0.9155 - val_acc: 0.7730\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8197 - acc: 0.8153 - val_loss: 0.9049 - val_acc: 0.7840\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8194 - acc: 0.8112 - val_loss: 0.9246 - val_acc: 0.7670\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8207 - acc: 0.8127 - val_loss: 0.9177 - val_acc: 0.7690\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8220 - acc: 0.8116 - val_loss: 0.9018 - val_acc: 0.7730\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8201 - acc: 0.8111 - val_loss: 0.9006 - val_acc: 0.7800\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8191 - acc: 0.8137 - val_loss: 0.9211 - val_acc: 0.7680\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8207 - acc: 0.8109 - val_loss: 0.9051 - val_acc: 0.7710\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8213 - acc: 0.8152 - val_loss: 0.9714 - val_acc: 0.7430\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8236 - acc: 0.8121 - val_loss: 0.9021 - val_acc: 0.7870\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8189 - acc: 0.8111 - val_loss: 0.9390 - val_acc: 0.7590\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8218 - acc: 0.8124 - val_loss: 0.9201 - val_acc: 0.7750\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8189 - acc: 0.8133 - val_loss: 0.9069 - val_acc: 0.7710\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8209 - acc: 0.8132 - val_loss: 0.9186 - val_acc: 0.7790\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8202 - acc: 0.8135 - val_loss: 0.8936 - val_acc: 0.7770\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8179 - acc: 0.8153 - val_loss: 0.9264 - val_acc: 0.7650\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8241 - acc: 0.8119 - val_loss: 0.9465 - val_acc: 0.7710\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8206 - acc: 0.8128 - val_loss: 0.9311 - val_acc: 0.7700\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8182 - acc: 0.8165 - val_loss: 0.9140 - val_acc: 0.7690\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8185 - acc: 0.8132 - val_loss: 0.9140 - val_acc: 0.7720\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8236 - acc: 0.8123 - val_loss: 0.8964 - val_acc: 0.7760\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8189 - acc: 0.8177 - val_loss: 0.9184 - val_acc: 0.7710\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8194 - acc: 0.8169 - val_loss: 0.9308 - val_acc: 0.7740\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8233 - acc: 0.8135 - val_loss: 0.9648 - val_acc: 0.7570\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8289 - acc: 0.8104 - val_loss: 0.9163 - val_acc: 0.7770\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8231 - acc: 0.8137 - val_loss: 0.9138 - val_acc: 0.7840\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8171 - acc: 0.8172 - val_loss: 0.9415 - val_acc: 0.7690\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8190 - acc: 0.8129 - val_loss: 0.9176 - val_acc: 0.7730\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8177 - acc: 0.8173 - val_loss: 0.9206 - val_acc: 0.7760\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8231 - acc: 0.8128 - val_loss: 1.0128 - val_acc: 0.7440\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8215 - acc: 0.8135 - val_loss: 0.9310 - val_acc: 0.7620\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8227 - acc: 0.8113 - val_loss: 0.8967 - val_acc: 0.7780\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8248 - acc: 0.8135 - val_loss: 0.9046 - val_acc: 0.7800\n",
      "Epoch 943/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8149 - acc: 0.8156 - val_loss: 0.9390 - val_acc: 0.7710\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8196 - acc: 0.8161 - val_loss: 0.9012 - val_acc: 0.7780\n",
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8187 - acc: 0.8153 - val_loss: 0.9908 - val_acc: 0.7410\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8171 - acc: 0.8153 - val_loss: 0.8959 - val_acc: 0.7830\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8181 - acc: 0.8177 - val_loss: 0.9009 - val_acc: 0.7890\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8165 - acc: 0.8189 - val_loss: 0.9089 - val_acc: 0.7760\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8175 - acc: 0.8149 - val_loss: 0.9174 - val_acc: 0.7810\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8154 - acc: 0.8181 - val_loss: 0.9426 - val_acc: 0.7690\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8205 - acc: 0.8147 - val_loss: 0.9446 - val_acc: 0.7670\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8201 - acc: 0.8157 - val_loss: 0.9258 - val_acc: 0.7710\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8194 - acc: 0.8141 - val_loss: 0.9489 - val_acc: 0.7590\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8215 - acc: 0.8125 - val_loss: 0.9119 - val_acc: 0.7780\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8176 - acc: 0.8149 - val_loss: 0.9062 - val_acc: 0.7880\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8167 - acc: 0.8176 - val_loss: 0.8965 - val_acc: 0.7810\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8141 - acc: 0.8211 - val_loss: 0.9951 - val_acc: 0.7510\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8184 - acc: 0.8136 - val_loss: 0.9008 - val_acc: 0.7760\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8169 - acc: 0.8175 - val_loss: 0.9145 - val_acc: 0.7790\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8146 - acc: 0.8181 - val_loss: 0.8970 - val_acc: 0.7760\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8140 - acc: 0.8195 - val_loss: 0.9691 - val_acc: 0.7640\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8161 - acc: 0.8208 - val_loss: 0.9218 - val_acc: 0.7710\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8153 - acc: 0.8157 - val_loss: 0.8985 - val_acc: 0.7730\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8165 - acc: 0.8199 - val_loss: 0.9148 - val_acc: 0.7820\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8189 - acc: 0.8123 - val_loss: 0.9230 - val_acc: 0.7690\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8168 - acc: 0.8149 - val_loss: 0.9037 - val_acc: 0.7740\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8148 - acc: 0.8175 - val_loss: 0.9538 - val_acc: 0.7530\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8186 - acc: 0.8153 - val_loss: 0.9456 - val_acc: 0.7610\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8149 - acc: 0.8167 - val_loss: 0.8971 - val_acc: 0.7810\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8130 - acc: 0.8185 - val_loss: 0.9443 - val_acc: 0.7600\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8182 - acc: 0.8205 - val_loss: 0.8914 - val_acc: 0.7780\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8157 - acc: 0.8173 - val_loss: 0.8937 - val_acc: 0.7890\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8165 - acc: 0.8144 - val_loss: 0.9035 - val_acc: 0.7760\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8161 - acc: 0.8172 - val_loss: 0.9271 - val_acc: 0.7740\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8151 - acc: 0.8159 - val_loss: 0.9235 - val_acc: 0.7620\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8181 - acc: 0.8160 - val_loss: 0.8926 - val_acc: 0.7850\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8129 - acc: 0.8207 - val_loss: 0.9030 - val_acc: 0.7800\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8166 - acc: 0.8160 - val_loss: 0.9161 - val_acc: 0.7780\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8165 - acc: 0.8185 - val_loss: 0.8936 - val_acc: 0.7790\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8144 - acc: 0.8160 - val_loss: 0.9084 - val_acc: 0.7720\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8154 - acc: 0.8168 - val_loss: 0.9102 - val_acc: 0.7750\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8123 - acc: 0.8189 - val_loss: 0.8985 - val_acc: 0.7730\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8154 - acc: 0.8196 - val_loss: 0.9041 - val_acc: 0.7760\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8153 - acc: 0.8179 - val_loss: 0.9040 - val_acc: 0.7820\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8142 - acc: 0.8171 - val_loss: 0.9682 - val_acc: 0.7600\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8167 - acc: 0.8151 - val_loss: 0.8979 - val_acc: 0.7830\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8104 - acc: 0.8195 - val_loss: 0.8979 - val_acc: 0.7720\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8143 - acc: 0.8189 - val_loss: 0.9354 - val_acc: 0.7610\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8149 - acc: 0.8165 - val_loss: 0.9506 - val_acc: 0.7580\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8182 - acc: 0.8167 - val_loss: 0.9191 - val_acc: 0.7790\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8122 - acc: 0.8199 - val_loss: 0.9093 - val_acc: 0.7810\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8220 - acc: 0.8141 - val_loss: 0.9601 - val_acc: 0.7650\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8132 - acc: 0.8147 - val_loss: 0.9331 - val_acc: 0.7660\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8142 - acc: 0.8181 - val_loss: 0.9260 - val_acc: 0.7660\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8134 - acc: 0.8193 - val_loss: 0.9657 - val_acc: 0.7490\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8154 - acc: 0.8175 - val_loss: 0.9159 - val_acc: 0.7760\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8111 - acc: 0.8208 - val_loss: 0.9008 - val_acc: 0.7790\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8111 - acc: 0.8203 - val_loss: 0.9150 - val_acc: 0.7750\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8150 - acc: 0.8179 - val_loss: 0.9124 - val_acc: 0.7780\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8128 - acc: 0.8179 - val_loss: 0.9036 - val_acc: 0.7830\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 48us/step\n",
      "1500/1500 [==============================] - 0s 53us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8089194829622904, 0.8201333333651225]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9813235403696696, 0.7619999996821085]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 124us/step - loss: 1.9690 - acc: 0.1505 - val_loss: 1.9406 - val_acc: 0.1650\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.9548 - acc: 0.1567 - val_loss: 1.9328 - val_acc: 0.1810\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.9460 - acc: 0.1647 - val_loss: 1.9264 - val_acc: 0.1880\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.9426 - acc: 0.175 - 1s 84us/step - loss: 1.9428 - acc: 0.1752 - val_loss: 1.9204 - val_acc: 0.2040\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.9301 - acc: 0.1837 - val_loss: 1.9148 - val_acc: 0.2190\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.9236 - acc: 0.1953 - val_loss: 1.9093 - val_acc: 0.2280\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.9165 - acc: 0.2025 - val_loss: 1.9033 - val_acc: 0.2360\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.9135 - acc: 0.2025 - val_loss: 1.8977 - val_acc: 0.2560\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.9062 - acc: 0.2075 - val_loss: 1.8910 - val_acc: 0.2580\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.8973 - acc: 0.2181 - val_loss: 1.8839 - val_acc: 0.2600\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.8862 - acc: 0.2260 - val_loss: 1.8758 - val_acc: 0.2720\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.8807 - acc: 0.2324 - val_loss: 1.8667 - val_acc: 0.2800\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.8747 - acc: 0.2399 - val_loss: 1.8565 - val_acc: 0.2810\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.8625 - acc: 0.2523 - val_loss: 1.8453 - val_acc: 0.2960\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.8581 - acc: 0.2512 - val_loss: 1.8331 - val_acc: 0.3070\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.8470 - acc: 0.2640 - val_loss: 1.8194 - val_acc: 0.3140\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.8292 - acc: 0.2721 - val_loss: 1.8021 - val_acc: 0.3260\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.8203 - acc: 0.2737 - val_loss: 1.7857 - val_acc: 0.3330\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.8080 - acc: 0.2843 - val_loss: 1.7678 - val_acc: 0.3510\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.7868 - acc: 0.2997 - val_loss: 1.7476 - val_acc: 0.3700\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.7740 - acc: 0.3007 - val_loss: 1.7270 - val_acc: 0.3930\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.7568 - acc: 0.3215 - val_loss: 1.7048 - val_acc: 0.3950\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.7446 - acc: 0.3239 - val_loss: 1.6799 - val_acc: 0.4170\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.7220 - acc: 0.3327 - val_loss: 1.6540 - val_acc: 0.4320\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.7195 - acc: 0.3325 - val_loss: 1.6336 - val_acc: 0.4500\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.6912 - acc: 0.3555 - val_loss: 1.6070 - val_acc: 0.4600\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.6707 - acc: 0.3624 - val_loss: 1.5823 - val_acc: 0.4750\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.6516 - acc: 0.3697 - val_loss: 1.5546 - val_acc: 0.4860\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.6408 - acc: 0.3773 - val_loss: 1.5295 - val_acc: 0.5060\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.6149 - acc: 0.3811 - val_loss: 1.5034 - val_acc: 0.5140\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.5996 - acc: 0.3919 - val_loss: 1.4780 - val_acc: 0.5320\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.5810 - acc: 0.3963 - val_loss: 1.4551 - val_acc: 0.5460\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.5618 - acc: 0.4073 - val_loss: 1.4303 - val_acc: 0.5620\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.5531 - acc: 0.4095 - val_loss: 1.4067 - val_acc: 0.5710\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 1.5257 - acc: 0.4288 - val_loss: 1.3822 - val_acc: 0.5740\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 1s 97us/step - loss: 1.5038 - acc: 0.4320 - val_loss: 1.3594 - val_acc: 0.5890\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 1.4867 - acc: 0.4443 - val_loss: 1.3356 - val_acc: 0.5970\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.4847 - acc: 0.4391 - val_loss: 1.3191 - val_acc: 0.6100\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.4586 - acc: 0.4572 - val_loss: 1.2963 - val_acc: 0.6120\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.4424 - acc: 0.4616 - val_loss: 1.2747 - val_acc: 0.6200\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.4239 - acc: 0.4681 - val_loss: 1.2560 - val_acc: 0.6340\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.4129 - acc: 0.4663 - val_loss: 1.2348 - val_acc: 0.6390\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.3972 - acc: 0.4705 - val_loss: 1.2186 - val_acc: 0.6410\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.3815 - acc: 0.4893 - val_loss: 1.2007 - val_acc: 0.6440\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.3713 - acc: 0.4855 - val_loss: 1.1832 - val_acc: 0.6550\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.3535 - acc: 0.4941 - val_loss: 1.1654 - val_acc: 0.6560\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.3513 - acc: 0.4980 - val_loss: 1.1522 - val_acc: 0.6580\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.3296 - acc: 0.5000 - val_loss: 1.1357 - val_acc: 0.6680\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.3263 - acc: 0.5011 - val_loss: 1.1234 - val_acc: 0.6630\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.3044 - acc: 0.5089 - val_loss: 1.1096 - val_acc: 0.6760\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.3003 - acc: 0.5059 - val_loss: 1.0968 - val_acc: 0.6740\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.2771 - acc: 0.5252 - val_loss: 1.0806 - val_acc: 0.6740\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.2800 - acc: 0.5185 - val_loss: 1.0686 - val_acc: 0.6780\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.2763 - acc: 0.5203 - val_loss: 1.0579 - val_acc: 0.6820\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.2490 - acc: 0.5321 - val_loss: 1.0456 - val_acc: 0.6830\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.2394 - acc: 0.5347 - val_loss: 1.0349 - val_acc: 0.6830\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.2340 - acc: 0.5376 - val_loss: 1.0236 - val_acc: 0.6870\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.2255 - acc: 0.5308 - val_loss: 1.0124 - val_acc: 0.6880\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.2152 - acc: 0.5441 - val_loss: 1.0023 - val_acc: 0.6950\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.2088 - acc: 0.5512 - val_loss: 0.9930 - val_acc: 0.6900\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.1969 - acc: 0.5477 - val_loss: 0.9830 - val_acc: 0.6940\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.1977 - acc: 0.5516 - val_loss: 0.9738 - val_acc: 0.6950\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.1886 - acc: 0.5625 - val_loss: 0.9660 - val_acc: 0.6970\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.1855 - acc: 0.5579 - val_loss: 0.9577 - val_acc: 0.7000\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.1570 - acc: 0.5639 - val_loss: 0.9489 - val_acc: 0.6980\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.1589 - acc: 0.5687 - val_loss: 0.9414 - val_acc: 0.6960\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.1435 - acc: 0.5765 - val_loss: 0.9307 - val_acc: 0.6950\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.1488 - acc: 0.5656 - val_loss: 0.9261 - val_acc: 0.6960\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.1387 - acc: 0.5753 - val_loss: 0.9174 - val_acc: 0.6950\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.1243 - acc: 0.5797 - val_loss: 0.9101 - val_acc: 0.6970\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.1195 - acc: 0.5847 - val_loss: 0.9027 - val_acc: 0.6980\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.0988 - acc: 0.5931 - val_loss: 0.8943 - val_acc: 0.7030\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.1030 - acc: 0.5868 - val_loss: 0.8881 - val_acc: 0.7050\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.1044 - acc: 0.5872 - val_loss: 0.8839 - val_acc: 0.7060\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.0792 - acc: 0.5911 - val_loss: 0.8745 - val_acc: 0.7080\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0889 - acc: 0.5893 - val_loss: 0.8682 - val_acc: 0.7080\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0818 - acc: 0.5955 - val_loss: 0.8637 - val_acc: 0.7100\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0724 - acc: 0.6013 - val_loss: 0.8572 - val_acc: 0.7070\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.0526 - acc: 0.6104 - val_loss: 0.8512 - val_acc: 0.7090\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0842 - acc: 0.5920 - val_loss: 0.8486 - val_acc: 0.7080\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.0721 - acc: 0.5964 - val_loss: 0.8455 - val_acc: 0.7140\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0553 - acc: 0.6037 - val_loss: 0.8381 - val_acc: 0.7100\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0379 - acc: 0.6101 - val_loss: 0.8329 - val_acc: 0.7110\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.0503 - acc: 0.6064 - val_loss: 0.8299 - val_acc: 0.7160\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0360 - acc: 0.6125 - val_loss: 0.8225 - val_acc: 0.7190\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.0307 - acc: 0.6071 - val_loss: 0.8174 - val_acc: 0.7180\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.0419 - acc: 0.6141 - val_loss: 0.8166 - val_acc: 0.7160\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 1s 117us/step - loss: 1.0287 - acc: 0.6111 - val_loss: 0.8125 - val_acc: 0.7110\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 1s 121us/step - loss: 1.0234 - acc: 0.6265 - val_loss: 0.8082 - val_acc: 0.7130\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 1s 101us/step - loss: 1.0107 - acc: 0.6191 - val_loss: 0.8013 - val_acc: 0.7190\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 1.0086 - acc: 0.6213 - val_loss: 0.7971 - val_acc: 0.7170\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 0.9952 - acc: 0.6313 - val_loss: 0.7928 - val_acc: 0.7210\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0094 - acc: 0.6207 - val_loss: 0.7909 - val_acc: 0.7220\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.9923 - acc: 0.6297 - val_loss: 0.7878 - val_acc: 0.7220\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.9822 - acc: 0.6301 - val_loss: 0.7827 - val_acc: 0.7230\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.9896 - acc: 0.6323 - val_loss: 0.7799 - val_acc: 0.7190\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9779 - acc: 0.6251 - val_loss: 0.7755 - val_acc: 0.7200\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9853 - acc: 0.6304 - val_loss: 0.7726 - val_acc: 0.7260\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9766 - acc: 0.6323 - val_loss: 0.7693 - val_acc: 0.7220\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.9765 - acc: 0.6391 - val_loss: 0.7671 - val_acc: 0.7190\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.9609 - acc: 0.6411 - val_loss: 0.7617 - val_acc: 0.7200\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.9754 - acc: 0.6363 - val_loss: 0.7618 - val_acc: 0.7220\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9624 - acc: 0.6392 - val_loss: 0.7574 - val_acc: 0.7230\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9619 - acc: 0.6393 - val_loss: 0.7545 - val_acc: 0.7230\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9534 - acc: 0.6411 - val_loss: 0.7512 - val_acc: 0.7220\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9654 - acc: 0.6385 - val_loss: 0.7500 - val_acc: 0.7270\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9394 - acc: 0.6436 - val_loss: 0.7462 - val_acc: 0.7250\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.9548 - acc: 0.6439 - val_loss: 0.7483 - val_acc: 0.7160\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9387 - acc: 0.6463 - val_loss: 0.7417 - val_acc: 0.7200\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9451 - acc: 0.6431 - val_loss: 0.7413 - val_acc: 0.7230\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9404 - acc: 0.6473 - val_loss: 0.7387 - val_acc: 0.7240\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9304 - acc: 0.6563 - val_loss: 0.7347 - val_acc: 0.7280\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9268 - acc: 0.6512 - val_loss: 0.7328 - val_acc: 0.7250\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.9388 - acc: 0.6528 - val_loss: 0.7311 - val_acc: 0.7250\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.9261 - acc: 0.6560 - val_loss: 0.7282 - val_acc: 0.7240\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9261 - acc: 0.6493 - val_loss: 0.7271 - val_acc: 0.7270\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.9249 - acc: 0.6445 - val_loss: 0.7243 - val_acc: 0.7250\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9068 - acc: 0.6617 - val_loss: 0.7190 - val_acc: 0.7270\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.9110 - acc: 0.6569 - val_loss: 0.7191 - val_acc: 0.7260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.9080 - acc: 0.6587 - val_loss: 0.7148 - val_acc: 0.7290\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.9052 - acc: 0.6627 - val_loss: 0.7139 - val_acc: 0.7300\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.9200 - acc: 0.6521 - val_loss: 0.7164 - val_acc: 0.7270\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.9056 - acc: 0.6625 - val_loss: 0.7125 - val_acc: 0.7250\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8857 - acc: 0.6747 - val_loss: 0.7078 - val_acc: 0.7280\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9093 - acc: 0.6584 - val_loss: 0.7079 - val_acc: 0.7250\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8947 - acc: 0.6661 - val_loss: 0.7056 - val_acc: 0.7300\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.8984 - acc: 0.6673 - val_loss: 0.7026 - val_acc: 0.7330\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8897 - acc: 0.6633 - val_loss: 0.7001 - val_acc: 0.7320\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.8788 - acc: 0.6732 - val_loss: 0.6968 - val_acc: 0.7310\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8795 - acc: 0.6705 - val_loss: 0.6930 - val_acc: 0.7360\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8784 - acc: 0.6720 - val_loss: 0.6920 - val_acc: 0.7390\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8843 - acc: 0.6713 - val_loss: 0.6921 - val_acc: 0.7320\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8693 - acc: 0.6747 - val_loss: 0.6900 - val_acc: 0.7340\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8752 - acc: 0.6723 - val_loss: 0.6900 - val_acc: 0.7350\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8789 - acc: 0.6729 - val_loss: 0.6880 - val_acc: 0.7350\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8584 - acc: 0.6773 - val_loss: 0.6854 - val_acc: 0.7390\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8664 - acc: 0.6741 - val_loss: 0.6848 - val_acc: 0.7360\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.8739 - acc: 0.6753 - val_loss: 0.6850 - val_acc: 0.7370\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.8549 - acc: 0.6815 - val_loss: 0.6840 - val_acc: 0.7400\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8542 - acc: 0.6785 - val_loss: 0.6816 - val_acc: 0.7400\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8628 - acc: 0.6764 - val_loss: 0.6801 - val_acc: 0.7380\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8469 - acc: 0.6893 - val_loss: 0.6791 - val_acc: 0.7400\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8484 - acc: 0.6797 - val_loss: 0.6781 - val_acc: 0.7410\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8528 - acc: 0.6773 - val_loss: 0.6750 - val_acc: 0.7390\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8544 - acc: 0.6785 - val_loss: 0.6732 - val_acc: 0.7390\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8520 - acc: 0.6831 - val_loss: 0.6726 - val_acc: 0.7410\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8455 - acc: 0.6877 - val_loss: 0.6726 - val_acc: 0.7380\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8421 - acc: 0.6825 - val_loss: 0.6711 - val_acc: 0.7390\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8383 - acc: 0.6871 - val_loss: 0.6697 - val_acc: 0.7480\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8396 - acc: 0.6888 - val_loss: 0.6678 - val_acc: 0.7430\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8406 - acc: 0.6897 - val_loss: 0.6667 - val_acc: 0.7420\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.8289 - acc: 0.6967 - val_loss: 0.6654 - val_acc: 0.7440\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8265 - acc: 0.6905 - val_loss: 0.6617 - val_acc: 0.7490\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8240 - acc: 0.6943 - val_loss: 0.6635 - val_acc: 0.7480\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8281 - acc: 0.6893 - val_loss: 0.6620 - val_acc: 0.7420\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8240 - acc: 0.6965 - val_loss: 0.6606 - val_acc: 0.7430\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8209 - acc: 0.6913 - val_loss: 0.6590 - val_acc: 0.7470\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8244 - acc: 0.6912 - val_loss: 0.6582 - val_acc: 0.7490\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8218 - acc: 0.6932 - val_loss: 0.6561 - val_acc: 0.7520\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.8235 - acc: 0.6888 - val_loss: 0.6569 - val_acc: 0.7480\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8110 - acc: 0.6975 - val_loss: 0.6539 - val_acc: 0.7490\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8143 - acc: 0.6979 - val_loss: 0.6539 - val_acc: 0.7460\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8011 - acc: 0.7016 - val_loss: 0.6508 - val_acc: 0.7490\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 0.8023 - acc: 0.7017 - val_loss: 0.6526 - val_acc: 0.7480\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 0.8035 - acc: 0.6940 - val_loss: 0.6494 - val_acc: 0.7520\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.8142 - acc: 0.6925 - val_loss: 0.6517 - val_acc: 0.7510\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 0.7963 - acc: 0.7040 - val_loss: 0.6471 - val_acc: 0.7510\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 0.8042 - acc: 0.6981 - val_loss: 0.6469 - val_acc: 0.7510\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.7993 - acc: 0.7007 - val_loss: 0.6459 - val_acc: 0.7470\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.8061 - acc: 0.6973 - val_loss: 0.6445 - val_acc: 0.7530\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.7934 - acc: 0.7023 - val_loss: 0.6475 - val_acc: 0.7480\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 1s 100us/step - loss: 0.7996 - acc: 0.6973 - val_loss: 0.6438 - val_acc: 0.7470\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 0.8045 - acc: 0.6935 - val_loss: 0.6464 - val_acc: 0.7440\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 1s 110us/step - loss: 0.7758 - acc: 0.7131 - val_loss: 0.6402 - val_acc: 0.7570\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.7848 - acc: 0.7099 - val_loss: 0.6402 - val_acc: 0.7520\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.7847 - acc: 0.7052 - val_loss: 0.6403 - val_acc: 0.7500\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.7878 - acc: 0.7075 - val_loss: 0.6376 - val_acc: 0.7490\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.7925 - acc: 0.7097 - val_loss: 0.6382 - val_acc: 0.7470\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.7847 - acc: 0.7088 - val_loss: 0.6369 - val_acc: 0.7460\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.7758 - acc: 0.7125 - val_loss: 0.6371 - val_acc: 0.7480\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.7842 - acc: 0.7063 - val_loss: 0.6360 - val_acc: 0.7490\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7806 - acc: 0.7067 - val_loss: 0.6369 - val_acc: 0.7520\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.7867 - acc: 0.7108 - val_loss: 0.6366 - val_acc: 0.7500\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.7752 - acc: 0.7085 - val_loss: 0.6335 - val_acc: 0.7520\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.7723 - acc: 0.7160 - val_loss: 0.6328 - val_acc: 0.7490\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7794 - acc: 0.7088 - val_loss: 0.6330 - val_acc: 0.7520\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7556 - acc: 0.7225 - val_loss: 0.6305 - val_acc: 0.7480\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.7644 - acc: 0.7149 - val_loss: 0.6267 - val_acc: 0.7530\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.7593 - acc: 0.7136 - val_loss: 0.6263 - val_acc: 0.7550\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.7646 - acc: 0.7167 - val_loss: 0.6273 - val_acc: 0.7570\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.7770 - acc: 0.7099 - val_loss: 0.6271 - val_acc: 0.7560\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.7635 - acc: 0.7153 - val_loss: 0.6252 - val_acc: 0.7570\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.7606 - acc: 0.7105 - val_loss: 0.6255 - val_acc: 0.7550\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.7567 - acc: 0.7200 - val_loss: 0.6246 - val_acc: 0.7510\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7559 - acc: 0.7157 - val_loss: 0.6227 - val_acc: 0.7510\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 1s 105us/step - loss: 0.7522 - acc: 0.7145 - val_loss: 0.6244 - val_acc: 0.7520\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 0.7553 - acc: 0.7215 - val_loss: 0.6225 - val_acc: 0.7550\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7603 - acc: 0.7136 - val_loss: 0.6228 - val_acc: 0.7550\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.7464 - acc: 0.7153 - val_loss: 0.6229 - val_acc: 0.7560\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7474 - acc: 0.7252 - val_loss: 0.6187 - val_acc: 0.7580\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 57us/step\n",
      "1500/1500 [==============================] - 0s 59us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4714420175631841, 0.8345333333333333]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6543982555071512, 0.7473333333333333]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 1.8902 - acc: 0.2218 - val_loss: 1.8295 - val_acc: 0.2730\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 1.7618 - acc: 0.3201 - val_loss: 1.6768 - val_acc: 0.3770\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 1.5970 - acc: 0.4154 - val_loss: 1.5088 - val_acc: 0.4540\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 1.4269 - acc: 0.5002 - val_loss: 1.3465 - val_acc: 0.5357\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 1.2700 - acc: 0.5835 - val_loss: 1.2045 - val_acc: 0.6137\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 1.1342 - acc: 0.6425 - val_loss: 1.0822 - val_acc: 0.6483\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 1.0188 - acc: 0.6765 - val_loss: 0.9813 - val_acc: 0.6790\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.9242 - acc: 0.6995 - val_loss: 0.8981 - val_acc: 0.6953\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.8512 - acc: 0.7150 - val_loss: 0.8388 - val_acc: 0.7110\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.7963 - acc: 0.7263 - val_loss: 0.7924 - val_acc: 0.7217\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.7545 - acc: 0.7356 - val_loss: 0.7561 - val_acc: 0.7283\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.7218 - acc: 0.7425 - val_loss: 0.7280 - val_acc: 0.7390\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.6952 - acc: 0.7527 - val_loss: 0.7073 - val_acc: 0.7460\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.6735 - acc: 0.7591 - val_loss: 0.6883 - val_acc: 0.7450\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.6547 - acc: 0.7630 - val_loss: 0.6760 - val_acc: 0.7567\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.6388 - acc: 0.7696 - val_loss: 0.6605 - val_acc: 0.7543\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.6245 - acc: 0.7742 - val_loss: 0.6490 - val_acc: 0.7587\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.6122 - acc: 0.7777 - val_loss: 0.6397 - val_acc: 0.7670\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.6002 - acc: 0.7830 - val_loss: 0.6292 - val_acc: 0.7693\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5897 - acc: 0.7864 - val_loss: 0.6226 - val_acc: 0.7713\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5801 - acc: 0.7904 - val_loss: 0.6143 - val_acc: 0.7743\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5710 - acc: 0.7940 - val_loss: 0.6084 - val_acc: 0.7760\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.5628 - acc: 0.7962 - val_loss: 0.6018 - val_acc: 0.7830\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5547 - acc: 0.8005 - val_loss: 0.5982 - val_acc: 0.7833\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5473 - acc: 0.8036 - val_loss: 0.5930 - val_acc: 0.7870\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5403 - acc: 0.8062 - val_loss: 0.5884 - val_acc: 0.7887\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5337 - acc: 0.8091 - val_loss: 0.5836 - val_acc: 0.7883\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5270 - acc: 0.8114 - val_loss: 0.5802 - val_acc: 0.7923\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.5214 - acc: 0.8144 - val_loss: 0.5761 - val_acc: 0.7893\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.5157 - acc: 0.8163 - val_loss: 0.5715 - val_acc: 0.7930\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.5101 - acc: 0.8181 - val_loss: 0.5707 - val_acc: 0.7930\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.5051 - acc: 0.8196 - val_loss: 0.5659 - val_acc: 0.7970\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.4999 - acc: 0.8221 - val_loss: 0.5633 - val_acc: 0.7963\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.4951 - acc: 0.8231 - val_loss: 0.5605 - val_acc: 0.7957\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.4906 - acc: 0.8262 - val_loss: 0.5587 - val_acc: 0.7980\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.4860 - acc: 0.8268 - val_loss: 0.5569 - val_acc: 0.7987\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.4816 - acc: 0.8288 - val_loss: 0.5568 - val_acc: 0.7963\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4776 - acc: 0.8294 - val_loss: 0.5501 - val_acc: 0.7973\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.4737 - acc: 0.8315 - val_loss: 0.5486 - val_acc: 0.7983\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.4696 - acc: 0.8329 - val_loss: 0.5497 - val_acc: 0.8010\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.4657 - acc: 0.8338 - val_loss: 0.5467 - val_acc: 0.8010\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4623 - acc: 0.8360 - val_loss: 0.5466 - val_acc: 0.8020\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4587 - acc: 0.8372 - val_loss: 0.5441 - val_acc: 0.8020\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4550 - acc: 0.8389 - val_loss: 0.5448 - val_acc: 0.8013\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4522 - acc: 0.8400 - val_loss: 0.5410 - val_acc: 0.8020\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4483 - acc: 0.8405 - val_loss: 0.5386 - val_acc: 0.8033\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4454 - acc: 0.8427 - val_loss: 0.5381 - val_acc: 0.8050\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4424 - acc: 0.8431 - val_loss: 0.5368 - val_acc: 0.8017\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4393 - acc: 0.8455 - val_loss: 0.5347 - val_acc: 0.8067\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4363 - acc: 0.8458 - val_loss: 0.5341 - val_acc: 0.8043\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4334 - acc: 0.8463 - val_loss: 0.5352 - val_acc: 0.8080\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4304 - acc: 0.8470 - val_loss: 0.5331 - val_acc: 0.8080\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4276 - acc: 0.8497 - val_loss: 0.5330 - val_acc: 0.8080\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4248 - acc: 0.8512 - val_loss: 0.5346 - val_acc: 0.8083\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.4224 - acc: 0.8505 - val_loss: 0.5313 - val_acc: 0.8103\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4197 - acc: 0.8522 - val_loss: 0.5320 - val_acc: 0.8097\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4173 - acc: 0.8531 - val_loss: 0.5300 - val_acc: 0.8097\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4147 - acc: 0.8541 - val_loss: 0.5305 - val_acc: 0.8103\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4120 - acc: 0.8548 - val_loss: 0.5302 - val_acc: 0.8127\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4100 - acc: 0.8555 - val_loss: 0.5293 - val_acc: 0.8137\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4077 - acc: 0.8558 - val_loss: 0.5278 - val_acc: 0.8113\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.4051 - acc: 0.8565 - val_loss: 0.5293 - val_acc: 0.8093\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4032 - acc: 0.8573 - val_loss: 0.5268 - val_acc: 0.8120\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4005 - acc: 0.8595 - val_loss: 0.5281 - val_acc: 0.8120\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3982 - acc: 0.8603 - val_loss: 0.5315 - val_acc: 0.8163\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3966 - acc: 0.8607 - val_loss: 0.5274 - val_acc: 0.8137\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3943 - acc: 0.8615 - val_loss: 0.5263 - val_acc: 0.8130\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3920 - acc: 0.8625 - val_loss: 0.5284 - val_acc: 0.8110\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.3900 - acc: 0.8625 - val_loss: 0.5280 - val_acc: 0.8123\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3878 - acc: 0.8639 - val_loss: 0.5267 - val_acc: 0.8143\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3859 - acc: 0.8643 - val_loss: 0.5268 - val_acc: 0.8133\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3839 - acc: 0.8663 - val_loss: 0.5307 - val_acc: 0.8163\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3820 - acc: 0.8659 - val_loss: 0.5315 - val_acc: 0.8137\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3802 - acc: 0.8662 - val_loss: 0.5280 - val_acc: 0.8140\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3783 - acc: 0.8672 - val_loss: 0.5270 - val_acc: 0.8150\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3759 - acc: 0.8688 - val_loss: 0.5294 - val_acc: 0.8147\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3747 - acc: 0.8688 - val_loss: 0.5272 - val_acc: 0.8143\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3723 - acc: 0.8694 - val_loss: 0.5277 - val_acc: 0.8133\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3710 - acc: 0.8696 - val_loss: 0.5282 - val_acc: 0.8157\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3688 - acc: 0.8706 - val_loss: 0.5287 - val_acc: 0.8147\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3673 - acc: 0.8723 - val_loss: 0.5276 - val_acc: 0.8140\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3656 - acc: 0.8722 - val_loss: 0.5289 - val_acc: 0.8150\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3640 - acc: 0.8722 - val_loss: 0.5313 - val_acc: 0.8127\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3621 - acc: 0.8737 - val_loss: 0.5319 - val_acc: 0.8133\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3607 - acc: 0.8744 - val_loss: 0.5299 - val_acc: 0.8140\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3589 - acc: 0.8743 - val_loss: 0.5315 - val_acc: 0.8160\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3573 - acc: 0.8757 - val_loss: 0.5309 - val_acc: 0.8160\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3559 - acc: 0.8759 - val_loss: 0.5324 - val_acc: 0.8113\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3541 - acc: 0.8762 - val_loss: 0.5316 - val_acc: 0.8133\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3525 - acc: 0.8778 - val_loss: 0.5337 - val_acc: 0.8150\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3513 - acc: 0.8778 - val_loss: 0.5352 - val_acc: 0.8163\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3494 - acc: 0.8786 - val_loss: 0.5335 - val_acc: 0.8140\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3479 - acc: 0.8788 - val_loss: 0.5336 - val_acc: 0.8137\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3468 - acc: 0.8782 - val_loss: 0.5351 - val_acc: 0.8137\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3454 - acc: 0.8801 - val_loss: 0.5360 - val_acc: 0.8127\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3436 - acc: 0.8804 - val_loss: 0.5374 - val_acc: 0.8130\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3421 - acc: 0.8809 - val_loss: 0.5362 - val_acc: 0.8107\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3406 - acc: 0.8821 - val_loss: 0.5392 - val_acc: 0.8123\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3393 - acc: 0.8827 - val_loss: 0.5375 - val_acc: 0.8137\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3382 - acc: 0.8825 - val_loss: 0.5409 - val_acc: 0.8100\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3366 - acc: 0.8831 - val_loss: 0.5407 - val_acc: 0.8110\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3354 - acc: 0.8832 - val_loss: 0.5395 - val_acc: 0.8090\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3339 - acc: 0.8830 - val_loss: 0.5408 - val_acc: 0.8097\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3329 - acc: 0.8839 - val_loss: 0.5439 - val_acc: 0.8130\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3313 - acc: 0.8850 - val_loss: 0.5415 - val_acc: 0.8110\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3299 - acc: 0.8860 - val_loss: 0.5445 - val_acc: 0.8097\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3284 - acc: 0.8862 - val_loss: 0.5475 - val_acc: 0.8113\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3272 - acc: 0.8872 - val_loss: 0.5449 - val_acc: 0.8103\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3260 - acc: 0.8870 - val_loss: 0.5439 - val_acc: 0.8120\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3250 - acc: 0.8879 - val_loss: 0.5463 - val_acc: 0.8083\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3233 - acc: 0.8883 - val_loss: 0.5460 - val_acc: 0.8097\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3226 - acc: 0.8893 - val_loss: 0.5476 - val_acc: 0.8113\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3212 - acc: 0.8891 - val_loss: 0.5474 - val_acc: 0.8107\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3199 - acc: 0.8893 - val_loss: 0.5509 - val_acc: 0.8120\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3185 - acc: 0.8899 - val_loss: 0.5509 - val_acc: 0.8113\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3172 - acc: 0.8906 - val_loss: 0.5507 - val_acc: 0.8103\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3159 - acc: 0.8909 - val_loss: 0.5534 - val_acc: 0.8103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3148 - acc: 0.8914 - val_loss: 0.5524 - val_acc: 0.8117\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3134 - acc: 0.8917 - val_loss: 0.5533 - val_acc: 0.8120\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3124 - acc: 0.8921 - val_loss: 0.5560 - val_acc: 0.8097\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 45us/step\n",
      "4000/4000 [==============================] - 0s 48us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3092427464687463, 0.8932727272727272]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5509318904876709, 0.80475]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
